{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c666001-f359-458f-85e8-86098eb6c6bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m----> 2\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:503\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    500\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m--> 503\u001b[0m         \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSparkSession$\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m     )\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(session\u001b[38;5;241m.\u001b[39m_jsparkSession, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"records\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dbd9ab9-0855-4930-913b-ba42ea557419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in table: 10004500\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter update percentage (e.g., 1 for 1%):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 100045 rows (~1.0%) in 201 batches...\n",
      "Processing batch 1/201...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/04 09:17:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:17:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:17:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:17:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:17:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:17:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:17:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:17:11 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/201 updated.\n",
      "Processing batch 2/201...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/04 09:20:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:20:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:20:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:20:37 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:20:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:20:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:20:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:20:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2/201 updated.\n",
      "Processing batch 3/201...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/04 09:23:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:23:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:23:39 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:23:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:23:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:23:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:23:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:23:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3/201 updated.\n",
      "Processing batch 4/201...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/04 09:26:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:26:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:26:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:26:45 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:26:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:26:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:26:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:26:46 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4/201 updated.\n",
      "Processing batch 5/201...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/04 09:30:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:30:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:30:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:30:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:30:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:30:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:30:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:30:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5/201 updated.\n",
      "Processing batch 6/201...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/04 09:32:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:32:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:32:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:32:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:32:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:32:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6/201 updated.\n",
      "Processing batch 7/201...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/04 09:36:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:36:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:36:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:36:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:36:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:36:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:36:04 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:36:05 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7/201 updated.\n",
      "Processing batch 8/201...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/04 09:39:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:39:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:39:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:39:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:39:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:39:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:39:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:39:09 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8/201 updated.\n",
      "Processing batch 9/201...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/04 09:42:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:42:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:42:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:42:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:42:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:42:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:42:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 09:42:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9/201 updated.\n",
      "Processing batch 10/201...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 305:===========> (227 + 4) / 267][Stage 306:>              (0 + 0) / 267]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6058.891s][warning][gc,alloc] Executor task launch worker for task 231.0 in stage 305.0 (TID 10072): Retried waiting for GCLocker too often allocating 1048578 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/04 09:45:04 ERROR Executor: Exception in task 231.0 in stage 305.0 (TID 10072)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/03/04 09:45:04 WARN TaskSetManager: Lost task 231.0 in stage 305.0 (TID 10072) (5e0ed98abed9 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "25/03/04 09:45:04 ERROR TaskSetManager: Task 231 in stage 305.0 failed 1 times; aborting job\n",
      "25/03/04 09:45:04 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 231.0 in stage 305.0 (TID 10072),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/15/temp_shuffle_ed1e3a84-2ee4-4891-badc-0569d2bb0869\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3f/temp_shuffle_6327d217-3c86-4b4b-9eeb-a525b91abbb9\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1f/temp_shuffle_771b330b-fdc3-415f-80d0-bf6cbf95dacf\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/23/temp_shuffle_2f08e80b-019a-496b-9b16-1fcc366acc4b\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/20/temp_shuffle_263f73c4-7eec-4f86-88df-6136bc359a99\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/16/temp_shuffle_be699563-2577-4cef-a285-3d017a1ac2dd\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/27/temp_shuffle_e80094ec-36ed-4eb8-8baf-332473cc56d9\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/07/temp_shuffle_9a286f32-abaa-420b-9fce-45450c9e1a60\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/22/temp_shuffle_efa5b9f5-c46f-425d-a80a-0ec98b7d99a1\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3c/temp_shuffle_ac27055f-989a-417a-91b4-88ca627d478f\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/14/temp_shuffle_2cc24c9c-3f8c-4b9c-b1e9-fa8b9ede7cc0\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/28/temp_shuffle_1d2e00e4-c1bc-4cd0-a84a-7dd8f6510c7a\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/2d/temp_shuffle_6c0ba128-3e3a-4acc-b0e9-aa030e96645d\n",
      "25/03/04 09:45:04 WARN TaskSetManager: Lost task 225.0 in stage 305.0 (TID 10066) (5e0ed98abed9 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 231 in stage 305.0 failed 1 times, most recent failure: Lost task 231.0 in stage 305.0 (TID 10072) (5e0ed98abed9 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0c/temp_shuffle_6f8752e5-79e3-4a7c-8dc8-aa3a608e4f3c\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3f/temp_shuffle_3ee1c335-79a9-4ce7-a86f-53a1da17305e\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3b/temp_shuffle_34775e6c-8456-40c8-8177-d6297e7831f1\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/36/temp_shuffle_4ef8d292-df00-4bce-8dd4-0e06a03ba5f6\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3e/temp_shuffle_0a4ea703-33ac-471a-80f6-1cfc84bf7ba0\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/17/temp_shuffle_d3f519d6-77a8-481b-9eaf-5b79133ed2e5\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/23/temp_shuffle_a5454fd2-8a7f-49e1-8aff-d915ae218fcf\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1f/temp_shuffle_ce2e36bf-b8c6-445b-b43f-5653b8962ff3\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3d/temp_shuffle_d83ae1ab-b646-46e1-9635-522044e42145\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/10/temp_shuffle_5835a3e3-bfa4-4eee-8c3f-1460971aa147\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0d/temp_shuffle_568c26f7-6cd9-4388-90f6-8284783e3a1e\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0a/temp_shuffle_d40327a9-6d90-4d78-b3ce-db22d146e862\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/08/temp_shuffle_73e672b7-b125-484a-804f-99186d94363e\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0a/temp_shuffle_9b8e33da-314d-43ff-b5a4-7742004ddb4c\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/11/temp_shuffle_53434f4e-8311-47bd-b762-9aecca69cb01\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/39/temp_shuffle_d7daa0a7-f215-4d12-88e7-7048a50e831b\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1e/temp_shuffle_9db02a47-b592-4cf4-9ea8-cc0a307aedfc\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/23/temp_shuffle_743232ba-a58e-4d99-9bc3-3368f8afe5c6\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0e/temp_shuffle_b7185def-5776-4429-a9ff-75bdb5b3be44\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1f/temp_shuffle_831acfe6-31fb-4ae8-8e5e-1adfa1651679\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/29/temp_shuffle_9308745b-4080-4a1f-b4bb-cf96960d367a\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1b/temp_shuffle_6cc9a9d5-3676-49cd-817d-f687cff7439a\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/11/temp_shuffle_d5c8315d-82c9-4aee-a9dc-0d6a45a6dd5e\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/28/temp_shuffle_5637fa5c-70b7-461c-9aaa-173bd63fe4a3\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1d/temp_shuffle_612abb49-094a-4e6a-a2b8-6bc84edce5cf\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/34/temp_shuffle_4eb16d8e-fd3f-4a10-bf42-7751555e5de9\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/31/temp_shuffle_f2103015-4716-4b65-aa9e-dc390389112b\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1b/temp_shuffle_87b64ea1-a794-47c4-8422-22fc6f4c3e8c\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/33/temp_shuffle_00c1f608-6c95-48bc-b685-68b450f656d5\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/32/temp_shuffle_5ee11694-e777-4387-979e-810d29fc50a4\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/00/temp_shuffle_492e3c87-5ad9-4125-951d-e0113110ea1e\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1b/temp_shuffle_d472695a-a9d5-4edf-a76e-471d30b8318f\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0c/temp_shuffle_ead2480a-0b74-4b06-b96a-c12b3f0d195a\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/2e/temp_shuffle_6fce47cb-3584-489b-8b3e-f9f281690d18\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/19/temp_shuffle_ca9d3398-9a71-4218-8efd-184a062fe818\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3a/temp_shuffle_27c8c174-02d5-417c-b8f2-4c4d464a028d\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/38/temp_shuffle_11c839b2-feb5-4bb7-8fa7-fd30cffdc54e\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/35/temp_shuffle_42feb952-f5f5-4c13-bad9-d9106777d625\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/00/temp_shuffle_4b561cdb-c3df-491a-b2ac-dd56f7d450bd\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0b/temp_shuffle_8fdf85da-e579-46d0-af83-f4fbc2dc5803\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/35/temp_shuffle_234a66e8-220d-49b9-a05a-5856f6d140e4\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/19/temp_shuffle_6524e643-4ccf-40d0-8377-ff2ddcb00b81\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/38/temp_shuffle_324bb5f9-21f8-4418-8abb-5ca364f80fde\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/2b/temp_shuffle_8237f6a6-f9d8-4d60-9170-278c0027c685\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3d/temp_shuffle_a88c236f-7b33-4132-8ae2-3c70313975f9\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/26/temp_shuffle_519566a4-18ce-490f-adc9-6455f06c0f28\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/26/temp_shuffle_b74c3851-f4c3-48e6-b6b0-e828d8b3a2cc\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3c/temp_shuffle_16f9666b-2539-429f-9941-4c8172c95d46\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/22/temp_shuffle_7100afb2-098b-49d1-a0cf-deb0e768001c\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1c/temp_shuffle_19806686-d53b-44f8-a1a2-ae678e32425b\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/06/temp_shuffle_31af6263-e918-48fd-8f72-30cb2d6212bf\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3a/temp_shuffle_f852e01a-5d74-40c0-8a22-2627baa124eb\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/2a/temp_shuffle_5a4a784a-514b-4680-920e-1f6c6de8dea9\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0b/temp_shuffle_02e18c7d-3dcb-4f7f-bafb-985188d267d8\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/20/temp_shuffle_4cd6a0f1-3e1b-4672-9da0-fa0a919428b1\n",
      "25/03/04 09:45:04 WARN TaskSetManager: Lost task 224.0 in stage 305.0 (TID 10065) (5e0ed98abed9 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 231 in stage 305.0 failed 1 times, most recent failure: Lost task 231.0 in stage 305.0 (TID 10072) (5e0ed98abed9 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/2f/temp_shuffle_5d7704a7-29b4-4b93-bb39-f2721557fc48\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1f/temp_shuffle_7b436cb5-b5ad-44b2-8f19-f2f37c5c96e7\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/2a/temp_shuffle_313b1f53-5375-4d20-8a1f-1cb955a71359\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/06/temp_shuffle_a2b6f5f0-2e06-45b7-810d-3ab6b05710c9\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/11/temp_shuffle_b3fb645d-6001-403c-9c6d-0849460d944a\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/14/temp_shuffle_b20eca14-5699-4f22-acf2-65ddd3905b44\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/2f/temp_shuffle_7a8b1b28-25b1-41eb-9d09-b93c3efe9abc\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3d/temp_shuffle_c73d835d-ae82-4cfe-9f48-b221329617f3\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/2c/temp_shuffle_98c51071-65bc-427b-b721-fbc6dd0eba56\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/16/temp_shuffle_7a7df6ea-4024-4a29-8935-efe6ff365c55\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3e/temp_shuffle_693817b8-8a7a-4888-8973-6fc882d41407\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/28/temp_shuffle_a1047a4a-d663-4aa5-8f20-071e5fca6b99\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/26/temp_shuffle_d3b38583-dde5-4bb1-98c1-fdd6577147f4\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3c/temp_shuffle_b83f80a8-9dad-4529-b4c2-759bda4fb7a8\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1f/temp_shuffle_70b44409-3fd2-46cc-9ca1-5bf18399ac97\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0e/temp_shuffle_fa2c89f0-870c-4b2e-9555-5068b5b4bb3d\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/06/temp_shuffle_c143f1f8-8dde-44c8-9419-654705c3d674\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/11/temp_shuffle_88047589-1e66-4686-8ed4-9c1bf3be4cb4\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0d/temp_shuffle_51787102-d31d-4202-9bce-b87acf3c9611\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/09/temp_shuffle_841d0ae3-aab8-46fb-9077-45b7a4fd0d75\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1a/temp_shuffle_5bd65838-5fc2-4c27-a88d-78a0810ed7aa\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/07/temp_shuffle_66171f1c-087f-431a-b09e-eb40edf0c1e2\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/35/temp_shuffle_4a4ce7aa-13a0-4104-8950-e8836dc8aad5\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3f/temp_shuffle_672ba11d-d7aa-4320-8874-e67be92ca483\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/04/temp_shuffle_60180497-fedc-462d-b50b-9f7a9d7d632e\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/2b/temp_shuffle_dc18973d-a686-4692-af9e-4ed79335dfa7\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/19/temp_shuffle_74dfb126-ee58-4078-8061-d9562bc7fe15\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/01/temp_shuffle_747a63b7-b16c-4013-913a-8031eb8905bb\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/00/temp_shuffle_65ee513f-ecac-4952-ab77-113ff7fdc6e0\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/21/temp_shuffle_30b568db-4dfd-43c4-a1f7-cf21ebeb4577\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/06/temp_shuffle_41305713-4263-4296-a90d-c3fcae55e777\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3e/temp_shuffle_503b7849-731e-4866-90b8-fb756a0d3d5a\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3f/temp_shuffle_94c02bd5-dbac-42ae-a07a-688441f8fe2b\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/22/temp_shuffle_a6a08752-9173-413c-9d2c-a9ae241046e6\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/37/temp_shuffle_55dfcb6a-11dd-417a-8408-0c031ad8d245\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/22/temp_shuffle_bd4ec9b7-6ee3-4160-9601-b40ecc9500a1\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0b/temp_shuffle_08e07879-8635-407f-81b9-38adb0c84a72\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/38/temp_shuffle_94cc1437-5c0f-4d34-8e3d-e04de6c929eb\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/2d/temp_shuffle_59cbc01d-3587-4248-895c-432b3f4140d0\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/30/temp_shuffle_0dc8276e-dc08-4c40-ab0b-8165d1380502\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/13/temp_shuffle_9d2b0e47-f641-404b-85c7-155f021ebb4f\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/15/temp_shuffle_cc90a35e-03ce-4691-b0cc-d5ba91ec44a5\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/14/temp_shuffle_747c348c-7b60-4a01-b857-2f1bfde6ddcd\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/03/temp_shuffle_954c4349-76f4-4335-93e0-4fe508113469\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1d/temp_shuffle_78de607d-b6fa-403b-ae3d-b413b144bc31\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1b/temp_shuffle_85a16f48-5cd1-4d20-b996-50c44b784d8a\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/11/temp_shuffle_f17a24eb-66ae-416d-a1af-c86b59110dcd\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/07/temp_shuffle_848b770a-fedc-4e01-8e9f-88b4ee5a2dd9\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/26/temp_shuffle_76d79ee3-98dd-4375-95b7-9b9956ddf09a\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0a/temp_shuffle_c9694fa1-c20e-44ec-8a83-f46b409d375d\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/2b/temp_shuffle_176df4e4-a3cc-4627-8048-055590c244f5\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/04/temp_shuffle_02c180f5-f691-4fd2-b80a-f6ea0637339f\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1a/temp_shuffle_e4077b36-ef21-4a62-a91f-7fada47f5d28\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/13/temp_shuffle_70edaac9-752f-4c67-8371-bd305517b4b6\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/10/temp_shuffle_922a2ce2-f9d2-48da-bb54-308abcdc35a1\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0d/temp_shuffle_4776cdcd-22b5-4677-b414-58291d73db7d\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/23/temp_shuffle_d2999cf6-b99d-4994-be51-d3103e32a9e9\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/03/temp_shuffle_8faad498-2aec-4ad8-bcec-855ff45c4c45\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3f/temp_shuffle_4ca280d0-5d49-4fad-8432-5c198d4e1bcc\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0f/temp_shuffle_8f94b3ea-677b-4025-9e2e-e76c9e66616f\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/2e/temp_shuffle_d1ab5ffe-71a2-400d-ba27-385638982f77\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3c/temp_shuffle_bf414f08-349f-4ea9-a982-a9c0f2b3dd38\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/02/temp_shuffle_f0494473-5122-4143-ac3c-fdf512191553\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3d/temp_shuffle_cec36be4-1828-42a8-b7b6-87c93e3eab39\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/19/temp_shuffle_a1f4f65e-d446-46b7-ac72-62471f501efe\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/06/temp_shuffle_51d47d2d-8365-4ee1-aa45-c485ecba98a3\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/14/temp_shuffle_c4baa99c-da93-4aac-91fd-e0f2f132f3ab\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/14/temp_shuffle_0a0e1fbd-c430-42b0-965c-f4a43e0104fa\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/2c/temp_shuffle_8907bc5c-ecaa-40f0-a00c-f8aa78c00248\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1e/temp_shuffle_978b7479-6c9d-4a00-aaee-91d715137825\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/31/temp_shuffle_7f46706d-e4e1-47c8-8365-5c907d109a9c\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3b/temp_shuffle_5746d022-ad2c-472d-8c51-37d48916bc2a\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1a/temp_shuffle_0e1ca720-b757-41a5-928e-104d57b7edd9\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/30/temp_shuffle_16d9322e-0001-43b4-9a8b-cb5d7ba7b9f5\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/07/temp_shuffle_607fddb8-c9b4-4498-b592-249d6938834b\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1a/temp_shuffle_42c9bb25-6c7e-43f3-803b-5aacc62d264e\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0b/temp_shuffle_59cd0884-985d-4b7a-a350-a2e0595ee201\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/35/temp_shuffle_19617efc-1272-410f-a82f-b1e9d194d933\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3d/temp_shuffle_72722534-3dbd-44a8-bef9-debaf4216492\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/32/temp_shuffle_1f4313ac-755f-4a63-9170-06778bf0cccd\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/30/temp_shuffle_57ed16a2-ab40-4484-aad8-0efe221217ce\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3a/temp_shuffle_60362240-6695-446a-8486-03bb12134df7\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/00/temp_shuffle_be100443-9a01-435e-87c6-f879e1428187\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/31/temp_shuffle_3c6cbf7a-c900-4a72-9fa8-04ed6db3af88\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/31/temp_shuffle_69105ba5-af3b-419e-a837-f64abcdc18fe\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/05/temp_shuffle_8719578f-d3bb-430f-96e8-f7a5b465b3cc\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3a/temp_shuffle_51fdebf0-76fc-4b2f-85e3-8248ff7dd574\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/25/temp_shuffle_5264f848-8f96-40de-8a92-f05d1ad034da\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/31/temp_shuffle_b0233eba-890f-4841-bef6-38d8b4cd07b8\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0c/temp_shuffle_c69e524b-8f75-49d3-80e8-7b4f4dba837d\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/10/temp_shuffle_0dca37cd-2231-4ce5-87ac-275b3916f342\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/04/temp_shuffle_bed43df6-45eb-4f15-845e-32dcf24be767\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/30/temp_shuffle_cd80c163-c34b-482d-a066-e137dbc07471\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/08/temp_shuffle_34c01911-3ade-4383-95d3-dac6996e1300\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/14/temp_shuffle_be873e78-eb7f-4dec-a03f-e934c11090d1\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/17/temp_shuffle_bff67be8-7f7e-4c7f-89f1-9b7e84e55b67\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/21/temp_shuffle_73f8332e-b1e3-48ab-be81-d0c965dcd64e\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/30/temp_shuffle_916ae547-4711-40d2-b5d2-4a30b0021868\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0a/temp_shuffle_2011d9a3-403c-4882-bd12-bb026aeb4869\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/2d/temp_shuffle_0c389e23-492b-47dc-a081-632d01293a3f\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1e/temp_shuffle_705c5ca6-0ce6-4834-966b-0274df34e277\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/04/temp_shuffle_886aa3fa-a37c-40ee-b474-a194f8e00367\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/33/temp_shuffle_80cd775a-a059-445f-a25a-091d625cfa83\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/31/temp_shuffle_758a7824-23a7-4714-a02f-66c81995dd0a\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/32/temp_shuffle_8791253a-fd13-4859-91ba-8f510d6fb943\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3f/temp_shuffle_2a739474-92f3-4027-a73c-fbf7d34afcf6\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3e/temp_shuffle_a097a6ca-cce5-45c1-8190-33b3786019f4\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/21/temp_shuffle_fcb68f29-dfab-4862-bc51-3eadf472ba8c\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/06/temp_shuffle_29942c0b-4dd8-4593-8822-d81d0a747968\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/03/temp_shuffle_31b941ac-8425-4e01-9089-668ee510f9d2\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/20/temp_shuffle_b6a45b4a-6274-4ab1-91f4-4d89ef82a42d\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/04/temp_shuffle_3358848d-c514-4140-8c7c-c31a8fc5a49a\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/02/temp_shuffle_d68cd2d9-0b07-41cf-83f6-c0b36fd9ee3e\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/3d/temp_shuffle_a0b97c63-aede-46b3-9fa4-6bc56a385eb9\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/2b/temp_shuffle_4b1ecd1c-7e62-41bc-a757-6178f20f7af1\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/1b/temp_shuffle_1804bda0-b86f-488a-86ff-7094d162970d\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/2b/temp_shuffle_a55ef4d8-acbf-4dbe-9264-bc4c4926d564\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/33/temp_shuffle_d721af87-82cc-4270-bc56-c46fc3650faf\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0d/temp_shuffle_8c94cc91-33e9-4fba-bcb7-3ee2a0fd4c95\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/17/temp_shuffle_e5b9f54a-9fca-4749-a4d4-0a21d2b43e29\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/09/temp_shuffle_9865bf5b-c58f-46a8-90e2-6108f8786fd2\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0c/temp_shuffle_4764b08d-b528-4b98-9e1c-f4ae553ca94d\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/0f/temp_shuffle_8104f412-7048-495e-9b8d-05f06c4d0311\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/06/temp_shuffle_4f8bc775-1f24-453a-ad46-16be3f448e44\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/28/temp_shuffle_ef6e17b7-1e8f-460d-a47e-6aca65341111\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/2f/temp_shuffle_5b722561-21dc-4ef1-b2d3-015e4df2645e\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/03/temp_shuffle_b8cf9ac3-265a-40a2-a41d-2062d57bcdc0\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/2a/temp_shuffle_9ff58545-4cd2-467c-8ace-1d0156a39953\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/29/temp_shuffle_38236452-e106-4cc8-8d8f-6cc6770e8c76\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/00/temp_shuffle_7a987f05-b1ed-45fd-bf03-99fb90cc111f\n",
      "25/03/04 09:45:04 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-244b373c-a8e3-4302-95fa-49829b358285/09/temp_shuffle_131335b7-308a-47b0-a4e2-e107b25bec59\n",
      "25/03/04 09:45:04 WARN TaskSetManager: Lost task 232.0 in stage 305.0 (TID 10073) (5e0ed98abed9 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 231 in stage 305.0 failed 1 times, most recent failure: Lost task 231.0 in stage 305.0 (TID 10072) (5e0ed98abed9 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/04 09:45:04 WARN TaskSetManager: Lost task 230.0 in stage 305.0 (TID 10071) (5e0ed98abed9 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 231 in stage 305.0 failed 1 times, most recent failure: Lost task 231.0 in stage 305.0 (TID 10072) (5e0ed98abed9 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o497.append.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 231 in stage 305.0 failed 1 times, most recent failure: Lost task 231.0 in stage 305.0 (TID 10072) (5e0ed98abed9 executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m     updated_batch \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mjoin(sampled_df\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_col_0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_col_1\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_col_1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# Optimize writing using append mode or merge\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     \u001b[43mupdated_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdemo.nyc.taxis_10M_50COLUMNS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_num\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m updated.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m st\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:2107\u001b[0m, in \u001b[0;36mDataFrameWriterV2.append\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2102\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m3.1\u001b[39m)\n\u001b[1;32m   2103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappend\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;124;03m    Append the contents of the data frame to the output table.\u001b[39;00m\n\u001b[1;32m   2106\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2107\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o497.append.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 231 in stage 305.0 failed 1 times, most recent failure: Lost task 231.0 in stage 305.0 (TID 10072) (5e0ed98abed9 executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import col, when, rand, expr\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with memory optimizations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Optimized Iceberg Update\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the Iceberg table\n",
    "df = spark.table(\"demo.nyc.taxis_10M_50COLUMNS\")\n",
    "\n",
    "# Debugging: Print total row count\n",
    "total_rows = df.count()\n",
    "print(f\"Total rows in table: {total_rows}\")\n",
    "\n",
    "# Get update percentage from user\n",
    "update_percentage = float(input(\"Enter update percentage (e.g., 1 for 1%): \").strip()) / 100\n",
    "num_rows = max(1, int(total_rows * update_percentage))  # Ensure at least 1 row is updated\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 500  # Reduced batch size to optimize memory usage\n",
    "num_batches = max(1, (num_rows // batch_size) + (1 if num_rows % batch_size else 0))  # Ensure at least 1 batch\n",
    "\n",
    "print(f\"Updating {num_rows} rows (~{update_percentage*100}%) in {num_batches} batches...\")\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "for batch_num in range(num_batches):\n",
    "    print(f\"Processing batch {batch_num + 1}/{num_batches}...\")\n",
    "\n",
    "    # Select a batch of unique row IDs without collecting to the driver\n",
    "    sampled_df = df.select(\"extra_col_0\").distinct().orderBy(rand()).limit(min(batch_size, num_rows))\n",
    "\n",
    "    # Merge updates instead of overwriting partitions\n",
    "    updated_batch = df.alias(\"source\").join(sampled_df.alias(\"updates\"), \"extra_col_0\", \"inner\") \\\n",
    "        .withColumn(\"extra_col_1\", col(\"extra_col_1\") + 10)\n",
    "\n",
    "    # Optimize writing using append mode or merge\n",
    "    updated_batch.repartition(100).writeTo(\"demo.nyc.taxis_10M_50COLUMNS\").append()\n",
    "\n",
    "    print(f\"Batch {batch_num + 1}/{num_batches} updated.\")\n",
    "\n",
    "end = time.time() - st\n",
    "print(f\"\\nTotal update time for {num_batches} batches: {end:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61463e1a-30d9-4954-be73-f4cf0908c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/04 09:56:19 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in table: 10009002\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter update percentage (e.g., 1 for 1%):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 100090 rows (~1.0%) in 201 batches...\n",
      "Processing batch 1/201...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/04 09:56:30 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `sampled_df` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 3 pos 39;\n'MergeIntoTable ('source.extra_col_0 = 'updates.extra_col_0), [updateaction(None, assignment('source.extra_col_1, ('source.extra_col_1 + 10)))]\n:- SubqueryAlias source\n:  +- SubqueryAlias demo.nyc.taxis_10M_50COLUMNS\n:     +- RelationV2[extra_col_0#165, extra_col_1#166, extra_col_2#167, extra_col_3#168, extra_col_4#169, extra_col_5#170, extra_col_6#171, extra_col_7#172, extra_col_8#173, extra_col_9#174, extra_col_10#175, extra_col_11#176, extra_col_12#177, extra_col_13#178, extra_col_14#179, extra_col_15#180, extra_col_16#181, extra_col_17#182, extra_col_18#183, extra_col_19#184, extra_col_20#185, extra_col_21#186, extra_col_22#187, extra_col_23#188, ... 26 more fields] demo.nyc.taxis_10M_50COLUMNS demo.nyc.taxis_10M_50COLUMNS\n+- 'SubqueryAlias updates\n   +- 'Project ['extra_col_0]\n      +- 'UnresolvedRelation [sampled_df], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m     sampled_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_col_0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdistinct()\u001b[38;5;241m.\u001b[39morderBy(rand())\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;28mmin\u001b[39m(batch_size, num_rows))\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Use MERGE INTO for efficient updates\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'''\u001b[39;49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;43m        MERGE INTO demo.nyc.taxis_10M_50COLUMNS AS source\u001b[39;49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;43m        USING (SELECT extra_col_0 FROM sampled_df) AS updates\u001b[39;49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;43m        ON source.extra_col_0 = updates.extra_col_0\u001b[39;49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;43m        WHEN MATCHED THEN\u001b[39;49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;43m        UPDATE SET source.extra_col_1 = source.extra_col_1 + 10\u001b[39;49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_num\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m updated.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m st\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `sampled_df` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 3 pos 39;\n'MergeIntoTable ('source.extra_col_0 = 'updates.extra_col_0), [updateaction(None, assignment('source.extra_col_1, ('source.extra_col_1 + 10)))]\n:- SubqueryAlias source\n:  +- SubqueryAlias demo.nyc.taxis_10M_50COLUMNS\n:     +- RelationV2[extra_col_0#165, extra_col_1#166, extra_col_2#167, extra_col_3#168, extra_col_4#169, extra_col_5#170, extra_col_6#171, extra_col_7#172, extra_col_8#173, extra_col_9#174, extra_col_10#175, extra_col_11#176, extra_col_12#177, extra_col_13#178, extra_col_14#179, extra_col_15#180, extra_col_16#181, extra_col_17#182, extra_col_18#183, extra_col_19#184, extra_col_20#185, extra_col_21#186, extra_col_22#187, extra_col_23#188, ... 26 more fields] demo.nyc.taxis_10M_50COLUMNS demo.nyc.taxis_10M_50COLUMNS\n+- 'SubqueryAlias updates\n   +- 'Project ['extra_col_0]\n      +- 'UnresolvedRelation [sampled_df], [], false\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import expr, rand\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with memory optimizations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Optimized Iceberg Update\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.7\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"50MB\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the Iceberg table\n",
    "df = spark.table(\"demo.nyc.taxis_10M_50COLUMNS\")\n",
    "\n",
    "# Debugging: Print total row count\n",
    "total_rows = df.count()\n",
    "print(f\"Total rows in table: {total_rows}\")\n",
    "\n",
    "# Get update percentage from user\n",
    "update_percentage = float(input(\"Enter update percentage (e.g., 1 for 1%): \").strip()) / 100\n",
    "num_rows = max(1, int(total_rows * update_percentage))  # Ensure at least 1 row is updated\n",
    "\n",
    "# Define batch size dynamically\n",
    "batch_size = min(500, num_rows // 10)  # Adjust batch size based on total updates\n",
    "num_batches = max(1, (num_rows // batch_size) + (1 if num_rows % batch_size else 0))  # Ensure at least 1 batch\n",
    "\n",
    "print(f\"Updating {num_rows} rows (~{update_percentage*100}%) in {num_batches} batches...\")\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "for batch_num in range(num_batches):\n",
    "    print(f\"Processing batch {batch_num + 1}/{num_batches}...\")\n",
    "    \n",
    "    # Select a batch of unique row IDs without collecting to the driver\n",
    "    sampled_df = df.select(\"extra_col_0\").distinct().orderBy(rand()).limit(min(batch_size, num_rows))\n",
    "    \n",
    "    # Use MERGE INTO for efficient updates\n",
    "    spark.sql(f'''\n",
    "        MERGE INTO demo.nyc.taxis_10M_50COLUMNS AS source\n",
    "        USING (SELECT extra_col_0 FROM sampled_df) AS updates\n",
    "        ON source.extra_col_0 = updates.extra_col_0\n",
    "        WHEN MATCHED THEN\n",
    "        UPDATE SET source.extra_col_1 = source.extra_col_1 + 10\n",
    "    ''')\n",
    "\n",
    "    print(f\"Batch {batch_num + 1}/{num_batches} updated.\")\n",
    "\n",
    "end = time.time() - st\n",
    "print(f\"\\nTotal update time for {num_batches} batches: {end:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c4b1b84-b4ac-4815-97d5-5bda320f5157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in table: 10009002\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter update percentage (e.g., 1 for 1%):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 100090 rows (~1.0%) in 201 batches...\n",
      "Processing batch 1/201...\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[INVALID_NON_DETERMINISTIC_EXPRESSIONS] The operator expects a deterministic expression, but the actual expression is \"(extra_col_0 = extra_col_0)\", \"exists(extra_col_0)\".; line 2 pos 8;\nReplaceData\n:  +- Filter (outer(extra_col_0#380) = extra_col_0#490)\n:     +- SubqueryAlias updates\n:        +- SubqueryAlias sampled_temp_view\n:           +- View (`sampled_temp_view`, [extra_col_0#490])\n:              +- GlobalLimit 500\n:                 +- LocalLimit 500\n:                    +- Project [extra_col_0#490]\n:                       +- Sort [_nondeterministic#379 ASC NULLS FIRST], true\n:                          +- Project [extra_col_0#490, rand(-1231214445441557296) AS _nondeterministic#379]\n:                             +- Deduplicate [extra_col_0#490]\n:                                +- Project [extra_col_0#490]\n:                                   +- SubqueryAlias demo.nyc.taxis_10M_50COLUMNS\n:                                      +- RelationV2[extra_col_0#490, extra_col_1#491, extra_col_2#492, extra_col_3#493, extra_col_4#494, extra_col_5#495, extra_col_6#496, extra_col_7#497, extra_col_8#498, extra_col_9#499, extra_col_10#500, extra_col_11#501, extra_col_12#502, extra_col_13#503, extra_col_14#504, extra_col_15#505, extra_col_16#506, extra_col_17#507, extra_col_18#508, extra_col_19#509, extra_col_20#510, extra_col_21#511, extra_col_22#512, extra_col_23#513, ... 26 more fields] demo.nyc.taxis_10M_50COLUMNS demo.nyc.taxis_10M_50COLUMNS\n+- MergeRows[extra_col_0#438, extra_col_1#439, extra_col_2#440, extra_col_3#441, extra_col_4#442, extra_col_5#443, extra_col_6#444, extra_col_7#445, extra_col_8#446, extra_col_9#447, extra_col_10#448, extra_col_11#449, extra_col_12#450, extra_col_13#451, extra_col_14#452, extra_col_15#453, extra_col_16#454, extra_col_17#455, extra_col_18#456, extra_col_19#457, extra_col_20#458, extra_col_21#459, extra_col_22#460, extra_col_23#461, ... 27 more fields]\n   +- Join LeftOuter, (extra_col_0#380 = extra_col_0#215), leftHint=(strategy=no_broadcast_and_replication)\n      :- Project [extra_col_0#380, extra_col_1#381, extra_col_2#382, extra_col_3#383, extra_col_4#384, extra_col_5#385, extra_col_6#386, extra_col_7#387, extra_col_8#388, extra_col_9#389, extra_col_10#390, extra_col_11#391, extra_col_12#392, extra_col_13#393, extra_col_14#394, extra_col_15#395, extra_col_16#396, extra_col_17#397, extra_col_18#398, extra_col_19#399, extra_col_20#400, extra_col_21#401, extra_col_22#402, extra_col_23#403, ... 29 more fields]\n      :  +- RelationV2[extra_col_0#380, extra_col_1#381, extra_col_2#382, extra_col_3#383, extra_col_4#384, extra_col_5#385, extra_col_6#386, extra_col_7#387, extra_col_8#388, extra_col_9#389, extra_col_10#390, extra_col_11#391, extra_col_12#392, extra_col_13#393, extra_col_14#394, extra_col_15#395, extra_col_16#396, extra_col_17#397, extra_col_18#398, extra_col_19#399, extra_col_20#400, extra_col_21#401, extra_col_22#402, extra_col_23#403, ... 27 more fields] demo.nyc.taxis_10M_50COLUMNS demo.nyc.taxis_10M_50COLUMNS\n      +- Project [extra_col_0#215, true AS __row_from_source#437]\n         +- SubqueryAlias updates\n            +- SubqueryAlias sampled_temp_view\n               +- View (`sampled_temp_view`, [extra_col_0#215])\n                  +- GlobalLimit 500\n                     +- LocalLimit 500\n                        +- Project [extra_col_0#215]\n                           +- Sort [_nondeterministic#379 ASC NULLS FIRST], true\n                              +- Project [extra_col_0#215, rand(-1231214445441557296) AS _nondeterministic#379]\n                                 +- Deduplicate [extra_col_0#215]\n                                    +- Project [extra_col_0#215]\n                                       +- SubqueryAlias demo.nyc.taxis_10M_50COLUMNS\n                                          +- RelationV2[extra_col_0#215, extra_col_1#216, extra_col_2#217, extra_col_3#218, extra_col_4#219, extra_col_5#220, extra_col_6#221, extra_col_7#222, extra_col_8#223, extra_col_9#224, extra_col_10#225, extra_col_11#226, extra_col_12#227, extra_col_13#228, extra_col_14#229, extra_col_15#230, extra_col_16#231, extra_col_17#232, extra_col_18#233, extra_col_19#234, extra_col_20#235, extra_col_21#236, extra_col_22#237, extra_col_23#238, ... 26 more fields] demo.nyc.taxis_10M_50COLUMNS demo.nyc.taxis_10M_50COLUMNS\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m     sampled_df\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msampled_temp_view\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Use MERGE INTO for efficient updates\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'''\u001b[39;49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;43m        MERGE INTO demo.nyc.taxis_10M_50COLUMNS AS source\u001b[39;49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;43m        USING sampled_temp_view AS updates\u001b[39;49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;43m        ON source.extra_col_0 = updates.extra_col_0\u001b[39;49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;43m        WHEN MATCHED THEN\u001b[39;49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;43m        UPDATE SET source.extra_col_1 = source.extra_col_1 + 10\u001b[39;49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_num\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m updated.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m st\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [INVALID_NON_DETERMINISTIC_EXPRESSIONS] The operator expects a deterministic expression, but the actual expression is \"(extra_col_0 = extra_col_0)\", \"exists(extra_col_0)\".; line 2 pos 8;\nReplaceData\n:  +- Filter (outer(extra_col_0#380) = extra_col_0#490)\n:     +- SubqueryAlias updates\n:        +- SubqueryAlias sampled_temp_view\n:           +- View (`sampled_temp_view`, [extra_col_0#490])\n:              +- GlobalLimit 500\n:                 +- LocalLimit 500\n:                    +- Project [extra_col_0#490]\n:                       +- Sort [_nondeterministic#379 ASC NULLS FIRST], true\n:                          +- Project [extra_col_0#490, rand(-1231214445441557296) AS _nondeterministic#379]\n:                             +- Deduplicate [extra_col_0#490]\n:                                +- Project [extra_col_0#490]\n:                                   +- SubqueryAlias demo.nyc.taxis_10M_50COLUMNS\n:                                      +- RelationV2[extra_col_0#490, extra_col_1#491, extra_col_2#492, extra_col_3#493, extra_col_4#494, extra_col_5#495, extra_col_6#496, extra_col_7#497, extra_col_8#498, extra_col_9#499, extra_col_10#500, extra_col_11#501, extra_col_12#502, extra_col_13#503, extra_col_14#504, extra_col_15#505, extra_col_16#506, extra_col_17#507, extra_col_18#508, extra_col_19#509, extra_col_20#510, extra_col_21#511, extra_col_22#512, extra_col_23#513, ... 26 more fields] demo.nyc.taxis_10M_50COLUMNS demo.nyc.taxis_10M_50COLUMNS\n+- MergeRows[extra_col_0#438, extra_col_1#439, extra_col_2#440, extra_col_3#441, extra_col_4#442, extra_col_5#443, extra_col_6#444, extra_col_7#445, extra_col_8#446, extra_col_9#447, extra_col_10#448, extra_col_11#449, extra_col_12#450, extra_col_13#451, extra_col_14#452, extra_col_15#453, extra_col_16#454, extra_col_17#455, extra_col_18#456, extra_col_19#457, extra_col_20#458, extra_col_21#459, extra_col_22#460, extra_col_23#461, ... 27 more fields]\n   +- Join LeftOuter, (extra_col_0#380 = extra_col_0#215), leftHint=(strategy=no_broadcast_and_replication)\n      :- Project [extra_col_0#380, extra_col_1#381, extra_col_2#382, extra_col_3#383, extra_col_4#384, extra_col_5#385, extra_col_6#386, extra_col_7#387, extra_col_8#388, extra_col_9#389, extra_col_10#390, extra_col_11#391, extra_col_12#392, extra_col_13#393, extra_col_14#394, extra_col_15#395, extra_col_16#396, extra_col_17#397, extra_col_18#398, extra_col_19#399, extra_col_20#400, extra_col_21#401, extra_col_22#402, extra_col_23#403, ... 29 more fields]\n      :  +- RelationV2[extra_col_0#380, extra_col_1#381, extra_col_2#382, extra_col_3#383, extra_col_4#384, extra_col_5#385, extra_col_6#386, extra_col_7#387, extra_col_8#388, extra_col_9#389, extra_col_10#390, extra_col_11#391, extra_col_12#392, extra_col_13#393, extra_col_14#394, extra_col_15#395, extra_col_16#396, extra_col_17#397, extra_col_18#398, extra_col_19#399, extra_col_20#400, extra_col_21#401, extra_col_22#402, extra_col_23#403, ... 27 more fields] demo.nyc.taxis_10M_50COLUMNS demo.nyc.taxis_10M_50COLUMNS\n      +- Project [extra_col_0#215, true AS __row_from_source#437]\n         +- SubqueryAlias updates\n            +- SubqueryAlias sampled_temp_view\n               +- View (`sampled_temp_view`, [extra_col_0#215])\n                  +- GlobalLimit 500\n                     +- LocalLimit 500\n                        +- Project [extra_col_0#215]\n                           +- Sort [_nondeterministic#379 ASC NULLS FIRST], true\n                              +- Project [extra_col_0#215, rand(-1231214445441557296) AS _nondeterministic#379]\n                                 +- Deduplicate [extra_col_0#215]\n                                    +- Project [extra_col_0#215]\n                                       +- SubqueryAlias demo.nyc.taxis_10M_50COLUMNS\n                                          +- RelationV2[extra_col_0#215, extra_col_1#216, extra_col_2#217, extra_col_3#218, extra_col_4#219, extra_col_5#220, extra_col_6#221, extra_col_7#222, extra_col_8#223, extra_col_9#224, extra_col_10#225, extra_col_11#226, extra_col_12#227, extra_col_13#228, extra_col_14#229, extra_col_15#230, extra_col_16#231, extra_col_17#232, extra_col_18#233, extra_col_19#234, extra_col_20#235, extra_col_21#236, extra_col_22#237, extra_col_23#238, ... 26 more fields] demo.nyc.taxis_10M_50COLUMNS demo.nyc.taxis_10M_50COLUMNS\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import expr, rand\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with memory optimizations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Optimized Iceberg Update\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.7\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"50MB\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the Iceberg table\n",
    "df = spark.table(\"demo.nyc.taxis_10M_50COLUMNS\")\n",
    "\n",
    "# Debugging: Print total row count\n",
    "total_rows = df.count()\n",
    "print(f\"Total rows in table: {total_rows}\")\n",
    "\n",
    "# Get update percentage from user\n",
    "update_percentage = float(input(\"Enter update percentage (e.g., 1 for 1%): \").strip()) / 100\n",
    "num_rows = max(1, int(total_rows * update_percentage))  # Ensure at least 1 row is updated\n",
    "\n",
    "# Define batch size dynamically\n",
    "batch_size = min(500, num_rows // 10)  # Adjust batch size based on total updates\n",
    "num_batches = max(1, (num_rows // batch_size) + (1 if num_rows % batch_size else 0))  # Ensure at least 1 batch\n",
    "\n",
    "print(f\"Updating {num_rows} rows (~{update_percentage*100}%) in {num_batches} batches...\")\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "for batch_num in range(num_batches):\n",
    "    print(f\"Processing batch {batch_num + 1}/{num_batches}...\")\n",
    "\n",
    "    # Select a batch of unique row IDs without collecting to the driver\n",
    "    sampled_df = df.select(\"extra_col_0\").distinct().orderBy(rand()).limit(min(batch_size, num_rows))\n",
    "\n",
    "    # Register the sampled DataFrame as a temporary view\n",
    "    sampled_df.createOrReplaceTempView(\"sampled_temp_view\")\n",
    "\n",
    "    # Use MERGE INTO for efficient updates\n",
    "    spark.sql(f'''\n",
    "        MERGE INTO demo.nyc.taxis_10M_50COLUMNS AS source\n",
    "        USING sampled_temp_view AS updates\n",
    "        ON source.extra_col_0 = updates.extra_col_0\n",
    "        WHEN MATCHED THEN\n",
    "        UPDATE SET source.extra_col_1 = source.extra_col_1 + 10\n",
    "    ''')\n",
    "\n",
    "    print(f\"Batch {batch_num + 1}/{num_batches} updated.\")\n",
    "\n",
    "end = time.time() - st\n",
    "print(f\"\\nTotal update time for {num_batches} batches: {end:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33190992-a8fc-4f14-ae07-5e978a7ce9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the percentage of records to update (0-100):  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/04 10:14:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:14:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:14:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:14:38 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:14:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:14:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:14:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:14:40 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:16:43 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/03/04 10:16:44 WARN TaskSetManager: Stage 36 contains a task of very large size (1873 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/04 10:19:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:19:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:19:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:19:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:19:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:19:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:19:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:19:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:19:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:19:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:19:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:19:55 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:19:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:19:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:19:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/04 10:19:57 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 49:>                                                        (0 + 4) / 35]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1663.798s][warning][gc,alloc] refresh progress: Retried waiting for GCLocker too often allocating 330 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/04 10:20:42 ERROR Utils: uncaught error in thread Spark Context Cleaner, stopping SparkContext\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$865/0x00000008406ee840.get$Lambda(Unknown Source)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.invokeStatic(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$781/0x00000008406a8c40.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1356)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n",
      "25/03/04 10:20:43 ERROR Utils: throw uncaught fatal error in thread Spark Context Cleaner\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$865/0x00000008406ee840.get$Lambda(Unknown Source)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.invokeStatic(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$781/0x00000008406a8c40.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1356)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n",
      "Exception in thread \"Spark Context Cleaner\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$865/0x00000008406ee840.get$Lambda(Unknown Source)\n",
      "\tat java.base/java.lang.invoke.DirectMethodHandle$Holder.invokeStatic(DirectMethodHandle$Holder)\n",
      "\tat java.base/java.lang.invoke.Invokers$Holder.linkToTargetMethod(Invokers$Holder)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$781/0x00000008406a8c40.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1356)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n",
      "25/03/04 10:20:44 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.bytes.HeapByteBufferAllocator.allocate(HeapByteBufferAllocator.java:32)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.bytes.CapacityByteArrayOutputStream.addSlab(CapacityByteArrayOutputStream.java:193)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.bytes.CapacityByteArrayOutputStream.write(CapacityByteArrayOutputStream.java:223)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.bytes.LittleEndianDataOutputStream.write(LittleEndianDataOutputStream.java:73)\n",
      "\tat java.base/java.io.OutputStream.write(OutputStream.java:122)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.io.api.Binary$ByteArrayBackedBinary.writeTo(Binary.java:306)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.column.values.plain.PlainValuesWriter.writeBytes(PlainValuesWriter.java:55)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.column.values.dictionary.DictionaryValuesWriter$PlainBinaryDictionaryValuesWriter.fallBackDictionaryEncodedData(DictionaryValuesWriter.java:295)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.column.values.dictionary.DictionaryValuesWriter.fallBackAllValuesTo(DictionaryValuesWriter.java:130)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.column.values.fallback.FallbackValuesWriter.fallBack(FallbackValuesWriter.java:155)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.column.values.fallback.FallbackValuesWriter.getBytes(FallbackValuesWriter.java:76)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.column.impl.ColumnWriterV1.writePage(ColumnWriterV1.java:60)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.column.impl.ColumnWriterBase.writePage(ColumnWriterBase.java:389)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.column.impl.ColumnWriteStoreBase.sizeCheck(ColumnWriteStoreBase.java:235)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.column.impl.ColumnWriteStoreBase.endRecord(ColumnWriteStoreBase.java:222)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.column.impl.ColumnWriteStoreV1.endRecord(ColumnWriteStoreV1.java:29)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.add(ParquetWriter.java:136)\n",
      "\tat org.apache.iceberg.io.DataWriter.write(DataWriter.java:71)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.write(RollingFileWriter.java:90)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.write(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.write(SparkWrite.java:724)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.write(SparkWrite.java:707)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:493)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:448)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask$$Lambda$4550/0x0000000841877c40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "25/03/04 10:20:45 ERROR DataWritingSparkTask: Aborting commit for partition 3 (task 3212, attempt 0, stage 49.0)\n",
      "Exception in thread \"refresh progress\" java.lang.OutOfMemoryError: Java heap space\n",
      "25/03/04 10:20:52 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/03/04 10:20:52 ERROR DataWritingSparkTask: Aborting commit for partition 1 (task 3210, attempt 0, stage 49.0)\n",
      "25/03/04 10:20:54 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1219)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)\n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:48)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 12 more\n",
      "25/03/04 10:20:55 ERROR Utils: Aborting task\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/03/04 10:20:55 ERROR DataWritingSparkTask: Aborting commit for partition 2 (task 3211, attempt 0, stage 49.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1680.296s][warning][gc,alloc] Executor task launch worker for task 3.0 in stage 49.0 (TID 3212): Retried waiting for GCLocker too often allocating 32291 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"RemoteBlock-temp-file-clean-thread\" java.lang.OutOfMemoryError: Java heap space\n",
      "25/03/04 10:21:08 WARN Utils: Suppressing exception in finally: Cannot return unfinished footer.\n",
      "java.lang.IllegalStateException: Cannot return unfinished footer.\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.Preconditions.checkState(Preconditions.java:156)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.getFooter(ParquetFileWriter.java:1386)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.metrics(ParquetWriter.java:144)\n",
      "\tat org.apache.iceberg.io.DataWriter.close(DataWriter.java:90)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:122)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:747)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$9(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1419)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/04 10:21:08 WARN Utils: Suppressing exception in finally: Cannot return unfinished footer.\n",
      "java.lang.IllegalStateException: Cannot return unfinished footer.\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.Preconditions.checkState(Preconditions.java:156)\n",
      "\tat org.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.getFooter(ParquetFileWriter.java:1386)\n",
      "\tat org.apache.iceberg.parquet.ParquetWriter.metrics(ParquetWriter.java:144)\n",
      "\tat org.apache.iceberg.io.DataWriter.close(DataWriter.java:90)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:122)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:747)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$9(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1419)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/04 10:21:08 ERROR Executor: Exception in task 3.0 in stage 49.0 (TID 3212)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/03/04 10:21:08 ERROR Executor: Exception in task 2.0 in stage 49.0 (TID 3211)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/03/04 10:21:08 WARN S3OutputStream: Unclosed output stream created by:\n",
      "\torg.apache.iceberg.aws.s3.S3OutputStream.<init>(S3OutputStream.java:132)\n",
      "\torg.apache.iceberg.aws.s3.S3OutputFile.createOrOverwrite(S3OutputFile.java:70)\n",
      "\torg.apache.iceberg.parquet.ParquetIO$ParquetOutputFile.createOrOverwrite(ParquetIO.java:151)\n",
      "\torg.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:345)\n",
      "\torg.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:323)\n",
      "\torg.apache.iceberg.parquet.ParquetWriter.ensureWriterInitialized(ParquetWriter.java:111)\n",
      "\torg.apache.iceberg.parquet.ParquetWriter.flushRowGroup(ParquetWriter.java:210)\n",
      "\torg.apache.iceberg.parquet.ParquetWriter.close(ParquetWriter.java:254)\n",
      "\torg.apache.iceberg.io.DataWriter.close(DataWriter.java:82)\n",
      "\torg.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:122)\n",
      "\torg.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n",
      "\torg.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n",
      "\torg.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:747)\n",
      "\torg.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.abort(SparkWrite.java:739)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$6(WriteToDataSourceV2Exec.scala:482)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1408)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\torg.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\torg.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\torg.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\torg.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tjava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tjava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tjava.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/04 10:21:08 ERROR Executor: Exception in task 1.0 in stage 49.0 (TID 3210)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/03/04 10:21:08 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 3.0 in stage 49.0 (TID 3212),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/03/04 10:21:08 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 49.0 (TID 3211),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/03/04 10:21:08 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 49.0 (TID 3210),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/03/04 10:21:08 WARN TaskSetManager: Lost task 1.0 in stage 49.0 (TID 3210) (5e0ed98abed9 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "25/03/04 10:21:08 ERROR TaskSetManager: Task 1 in stage 49.0 failed 1 times; aborting job\n",
      "25/03/04 10:21:08 ERROR ReplaceDataExec: Data source write support IcebergBatchWrite(table=demo.nyc.taxis_10M_50COLUMNS, format=PARQUET) is aborting.\n",
      "25/03/04 10:21:08 ERROR ReplaceDataExec: Data source write support IcebergBatchWrite(table=demo.nyc.taxis_10M_50COLUMNS, format=PARQUET) aborted.\n",
      "25/03/04 10:21:08 WARN S3OutputStream: Unclosed output stream created by:\n",
      "\torg.apache.iceberg.aws.s3.S3OutputStream.<init>(S3OutputStream.java:132)\n",
      "\torg.apache.iceberg.aws.s3.S3OutputFile.createOrOverwrite(S3OutputFile.java:70)\n",
      "\torg.apache.iceberg.parquet.ParquetIO$ParquetOutputFile.createOrOverwrite(ParquetIO.java:151)\n",
      "\torg.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:345)\n",
      "\torg.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:323)\n",
      "\torg.apache.iceberg.parquet.ParquetWriter.ensureWriterInitialized(ParquetWriter.java:111)\n",
      "\torg.apache.iceberg.parquet.ParquetWriter.flushRowGroup(ParquetWriter.java:210)\n",
      "\torg.apache.iceberg.parquet.ParquetWriter.close(ParquetWriter.java:254)\n",
      "\torg.apache.iceberg.io.DataWriter.close(DataWriter.java:82)\n",
      "\torg.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:122)\n",
      "\torg.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n",
      "\torg.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n",
      "\torg.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:747)\n",
      "\torg.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.abort(SparkWrite.java:739)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$6(WriteToDataSourceV2Exec.scala:482)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1408)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\torg.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\torg.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\torg.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\torg.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tjava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tjava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tjava.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/04 10:21:08 WARN S3OutputStream: Unclosed output stream created by:\n",
      "\torg.apache.iceberg.aws.s3.S3OutputStream.<init>(S3OutputStream.java:132)\n",
      "\torg.apache.iceberg.aws.s3.S3OutputFile.createOrOverwrite(S3OutputFile.java:70)\n",
      "\torg.apache.iceberg.parquet.ParquetIO$ParquetOutputFile.createOrOverwrite(ParquetIO.java:151)\n",
      "\torg.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:345)\n",
      "\torg.apache.iceberg.shaded.org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:323)\n",
      "\torg.apache.iceberg.parquet.ParquetWriter.ensureWriterInitialized(ParquetWriter.java:111)\n",
      "\torg.apache.iceberg.parquet.ParquetWriter.flushRowGroup(ParquetWriter.java:210)\n",
      "\torg.apache.iceberg.parquet.ParquetWriter.close(ParquetWriter.java:254)\n",
      "\torg.apache.iceberg.io.DataWriter.close(DataWriter.java:82)\n",
      "\torg.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:122)\n",
      "\torg.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n",
      "\torg.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n",
      "\torg.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:747)\n",
      "\torg.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.abort(SparkWrite.java:739)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$6(WriteToDataSourceV2Exec.scala:482)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1408)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\torg.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\torg.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\torg.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\torg.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\torg.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\torg.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\torg.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tjava.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tjava.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tjava.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/04 10:21:08 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:267)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.MergeRowsExec$MergeRowIterator.hasNext(MergeRowsExec.scala:188)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "25/03/04 10:21:08 ERROR DataWritingSparkTask: Aborting commit for partition 4 (task 3213, attempt 0, stage 49.0)\n",
      "25/03/04 10:21:08 ERROR DataWritingSparkTask: Aborted commit for partition 4 (task 3213, attempt 0, stage 49.0)\n",
      "25/03/04 10:21:08 WARN TaskSetManager: Lost task 4.0 in stage 49.0 (TID 3213) (5e0ed98abed9 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 49.0 failed 1 times, most recent failure: Lost task 1.0 in stage 49.0 (TID 3210) (5e0ed98abed9 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o39.sql.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 49.0 failed 1 times, most recent failure: Lost task 1.0 in stage 49.0 (TID 3210) (5e0ed98abed9 executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:385)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:359)\n\tat org.apache.spark.sql.execution.datasources.v2.ReplaceDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:271)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:337)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:336)\n\tat org.apache.spark.sql.execution.datasources.v2.ReplaceDataExec.run(WriteToDataSourceV2Exec.scala:271)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m pre_update_count \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfilter(df\u001b[38;5;241m.\u001b[39mextra_col_0\u001b[38;5;241m.\u001b[39misin([row\u001b[38;5;241m.\u001b[39mextra_col_0 \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m sampled_df\u001b[38;5;241m.\u001b[39mcollect()]))\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Perform MERGE INTO operation\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'''\u001b[39;49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;43m    MERGE INTO demo.nyc.taxis_10M_50COLUMNS AS source\u001b[39;49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;43m    USING sampled_temp_view AS updates\u001b[39;49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;43m    ON source.extra_col_0 = updates.extra_col_0\u001b[39;49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;43m    WHEN MATCHED THEN\u001b[39;49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;43m    UPDATE SET source.extra_col_1 = source.extra_col_1 + 10\u001b[39;49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;43m'''\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Count records after update\u001b[39;00m\n\u001b[1;32m     42\u001b[0m post_update_count \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfilter(df\u001b[38;5;241m.\u001b[39mextra_col_0\u001b[38;5;241m.\u001b[39misin([row\u001b[38;5;241m.\u001b[39mextra_col_0 \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m sampled_df\u001b[38;5;241m.\u001b[39mcollect()]))\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o39.sql.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 49.0 failed 1 times, most recent failure: Lost task 1.0 in stage 49.0 (TID 3210) (5e0ed98abed9 executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:385)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:359)\n\tat org.apache.spark.sql.execution.datasources.v2.ReplaceDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:271)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run(WriteToDataSourceV2Exec.scala:337)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ExistingTableWriteExec.run$(WriteToDataSourceV2Exec.scala:336)\n\tat org.apache.spark.sql.execution.datasources.v2.ReplaceDataExec.run(WriteToDataSourceV2Exec.scala:271)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/04 10:21:09 ERROR Executor: Exception in task 0.0 in stage 49.0 (TID 3209)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/03/04 10:21:09 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 0.0 in stage 49.0 (TID 3209),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Initialize Spark session with Iceberg support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Merge\") \\\n",
    "    .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.demo.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.demo.warehouse\", \"s3://your-bucket/warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the Iceberg table\n",
    "df = spark.read.format(\"iceberg\").load(\"demo.nyc.taxis_10M_50COLUMNS\")\n",
    "\n",
    "# Get user input for update percentage\n",
    "update_percentage = float(input(\"Enter the percentage of records to update (0-100): \")) / 100\n",
    "\n",
    "# Calculate batch size\n",
    "num_rows = df.count()\n",
    "batch_size = int(num_rows * update_percentage)\n",
    "st = time.time()\n",
    "\n",
    "# Select distinct values of extra_col_0 for sampling\n",
    "sampled_df = df.select(\"extra_col_0\").distinct().orderBy(\"extra_col_0\").limit(batch_size)\n",
    "\n",
    "# Register as a temporary view\n",
    "sampled_df.createOrReplaceTempView(\"sampled_temp_view\")\n",
    "\n",
    "# Count records before update for verification\n",
    "pre_update_count = df.filter(df.extra_col_0.isin([row.extra_col_0 for row in sampled_df.collect()])).count()\n",
    "\n",
    "# Perform MERGE INTO operation\n",
    "spark.sql('''\n",
    "    MERGE INTO demo.nyc.taxis_10M_50COLUMNS AS source\n",
    "    USING sampled_temp_view AS updates\n",
    "    ON source.extra_col_0 = updates.extra_col_0\n",
    "    WHEN MATCHED THEN\n",
    "    UPDATE SET source.extra_col_1 = source.extra_col_1 + 10\n",
    "''')\n",
    "\n",
    "# Count records after update\n",
    "post_update_count = df.filter(df.extra_col_0.isin([row.extra_col_0 for row in sampled_df.collect()])).count()\n",
    "\n",
    "# Calculate the number of records updated\n",
    "updated_count = post_update_count - pre_update_count\n",
    "\n",
    "print(f\"Number of records updated: {updated_count}\")\n",
    "print(\"Update completed successfully.\")\n",
    "print(f\"Time taken: {time.time() - st} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ea74254-5b7e-4784-8c10-fa225afa8ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "SparkSession$ does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize Spark session with Iceberg support\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIceberg Merge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.demo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.iceberg.spark.SparkCatalog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.demo.type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhadoop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.catalog.demo.warehouse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://your-bucket/warehouse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Load the Iceberg table\u001b[39;00m\n\u001b[1;32m     13\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miceberg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdemo.nyc.taxis_10M_50COLUMNS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:503\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    500\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m--> 503\u001b[0m         \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSparkSession$\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m     )\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(session\u001b[38;5;241m.\u001b[39m_jsparkSession, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1725\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1722\u001b[0m _, error_message \u001b[38;5;241m=\u001b[39m get_error_message(answer)\n\u001b[1;32m   1723\u001b[0m message \u001b[38;5;241m=\u001b[39m compute_exception_message(\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name), error_message)\n\u001b[0;32m-> 1725\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(message)\n",
      "\u001b[0;31mPy4JError\u001b[0m: SparkSession$ does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Initialize Spark session with Iceberg support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Iceberg Merge\") \\\n",
    "    .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.demo.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.demo.warehouse\", \"s3://your-bucket/warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the Iceberg table\n",
    "df = spark.read.format(\"iceberg\").load(\"demo.nyc.taxis_10M_50COLUMNS\")\n",
    "\n",
    "# Get user input for update percentage\n",
    "update_percentage = float(input(\"Enter the percentage of records to update (0-100): \")) / 100\n",
    "\n",
    "# Calculate batch size\n",
    "num_rows = df.count()\n",
    "batch_size = int(num_rows * update_percentage)\n",
    "st = time.time()\n",
    "\n",
    "# Select distinct values of extra_col_0 for sampling\n",
    "sampled_df = df.select(\"extra_col_0\").distinct().orderBy(\"extra_col_0\").limit(batch_size)\n",
    "\n",
    "# Register as a temporary view\n",
    "sampled_df.createOrReplaceTempView(\"sampled_temp_view\")\n",
    "\n",
    "# Perform MERGE INTO operation\n",
    "spark.sql('''\n",
    "    MERGE INTO demo.nyc.taxis_10M_50COLUMNS AS source\n",
    "    USING sampled_temp_view AS updates\n",
    "    ON source.extra_col_0 = updates.extra_col_0\n",
    "    WHEN MATCHED THEN\n",
    "    UPDATE SET source.extra_col_1 = source.extra_col_1 + 10\n",
    "''')\n",
    "\n",
    "print(\"Update completed successfully.\")\n",
    "print(f\"Time taken: {time.time() - st} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be24a55e-097f-4715-b9a3-2f36b4279f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|        snapshot_id|     made_current_at|\n",
      "+-------------------+--------------------+\n",
      "| 105668051919482591|2025-03-04 10:00:...|\n",
      "|6814201661817395182|2025-03-04 09:42:...|\n",
      "|6521404266517949867|2025-03-04 09:39:...|\n",
      "|7604542288563918334|2025-03-04 09:36:...|\n",
      "| 552092269728169030|2025-03-04 09:33:...|\n",
      "|2209669498964280618|2025-03-04 09:30:...|\n",
      "|7674319906980595297|2025-03-04 09:27:...|\n",
      "|5468151910761078557|2025-03-04 09:23:...|\n",
      "|7947044984117739091|2025-03-04 09:20:...|\n",
      "|5673072808774441768|2025-03-04 09:17:...|\n",
      "|1470626163957892397|2025-03-04 09:09:...|\n",
      "|7160248633578306637|2025-03-04 09:06:...|\n",
      "|5174434347034670474|2025-03-04 08:47:...|\n",
      "|1794110359935204866|2025-03-04 08:44:...|\n",
      "|6937317397142763324|2025-03-04 08:41:...|\n",
      "|3089841895765010104|2025-03-04 08:38:...|\n",
      "|7127818708494625786|2025-03-04 08:34:...|\n",
      "|2707916599315632908|2025-03-04 08:30:...|\n",
      "| 497229154627925394|2025-03-04 08:25:...|\n",
      "|7776288292023736999|2025-03-04 08:13:...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT snapshot_id, made_current_at FROM demo.nyc.taxis_10M_50COLUMNS.history ORDER BY made_current_at DESC\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a304160-e742-42fb-8143-b8d82ef9ab12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2025-03-04 10:00:...| 105668051919482591|6814201661817395182|overwrite|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "|2025-03-04 09:42:...|6814201661817395182|6521404266517949867|   append|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "|2025-03-04 09:39:...|6521404266517949867|7604542288563918334|   append|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "|2025-03-04 09:36:...|7604542288563918334| 552092269728169030|   append|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "|2025-03-04 09:33:...| 552092269728169030|2209669498964280618|   append|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "|2025-03-04 09:30:...|2209669498964280618|7674319906980595297|   append|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "|2025-03-04 09:27:...|7674319906980595297|5468151910761078557|   append|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "|2025-03-04 09:23:...|5468151910761078557|7947044984117739091|   append|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "|2025-03-04 09:20:...|7947044984117739091|5673072808774441768|   append|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "|2025-03-04 09:17:...|5673072808774441768|1470626163957892397|   append|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "|2025-03-04 09:09:...|1470626163957892397|7160248633578306637|   append|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "|2025-03-04 09:06:...|7160248633578306637|5174434347034670474|   append|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "|2025-03-04 08:47:...|5174434347034670474|1794110359935204866|   append|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "|2025-03-04 08:44:...|1794110359935204866|6937317397142763324|   append|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "|2025-03-04 08:41:...|6937317397142763324|3089841895765010104|   append|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "|2025-03-04 08:38:...|3089841895765010104|7127818708494625786|   append|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "|2025-03-04 08:34:...|7127818708494625786|2707916599315632908|   append|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "|2025-03-04 08:30:...|2707916599315632908| 497229154627925394|   append|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "|2025-03-04 08:25:...| 497229154627925394|7776288292023736999|   append|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "|2025-03-04 08:13:...|7776288292023736999|5251605072415225303|   append|s3://warehouse/ny...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.sql(\"SELECT * FROM demo.nyc.taxis_10M_50COLUMNS.snapshots ORDER BY committed_at desc\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92039cfb-871c-44a6-ad5c-8e8fae348e26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
