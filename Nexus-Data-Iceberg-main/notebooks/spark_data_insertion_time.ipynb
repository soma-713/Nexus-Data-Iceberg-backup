{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1544bc6e-fb7c-47c3-9961-7099c4258610",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (2.2.1)\n",
      "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.9/site-packages (3.2.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.9/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "608420a0-392e-4d76-9a36-a1e788ebb7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"records\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b443206d-6ebe-46fd-bd8a-2934dd468585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS demo.nyc.taxis_10M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9595ebac-0106-4941-9239-b1f7e1a513cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "iceberg_table_dir = \"../warehouse/nyc/taxis_10M\"\n",
    "metadata_dir = f\"{iceberg_table_dir}/metadata\"\n",
    "data_dir = f\"{iceberg_table_dir}/data\"\n",
    "input_data_dir = f\"../input_data\"\n",
    "analysis_info = []\n",
    "records_before_op = 0\n",
    "\n",
    "def append_to_file(file_path, msg):\n",
    "    open_mode = \"a\"\n",
    "    if not os.path.exists(file_path):\n",
    "        open_mode = \"w\"\n",
    "\n",
    "    # Open the CSV file in write mode\n",
    "    with open(file_path, open_mode) as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        if open_mode==\"w\":\n",
    "            #writing header of the columns\n",
    "            writer.writerows([list(msg.keys())])    \n",
    "\n",
    "        row_values = [list(msg.values())]\n",
    "        # Write the data to the CSV file\n",
    "        writer.writerows(row_values)\n",
    "\n",
    "def get_size():\n",
    "    # List the metadata files\n",
    "    manifest_pattern = re.compile(r\".*-m\\d+\\.avro$\")\n",
    "    metadata_files = os.listdir(metadata_dir)\n",
    "    \n",
    "    # Initialize variables to store the sizes of different types of metadata files\n",
    "    snap_avro_size = 0\n",
    "    metadata_json_size = 0\n",
    "    m_avro_size = 0\n",
    "\n",
    "    data_dir_size = 0\n",
    "    # get data dir size\n",
    "    data_dir_files = os.listdir(data_dir)\n",
    "    # print(data_dir_files)\n",
    "    for filename in data_dir_files:\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        data_dir_size += os.path.getsize(file_path) / 1024  # Convert size to KB\n",
    "    \n",
    "    # Iterate through the metadata files and calculate their sizes\n",
    "    for file in metadata_files:\n",
    "        file_path = os.path.join(metadata_dir, file)\n",
    "        file_size_kb = os.path.getsize(file_path) / 1024  # Convert size to KB\n",
    "        \n",
    "        if file.startswith(\"snap-\") and file.endswith(\".avro\"):\n",
    "            snap_avro_size += file_size_kb\n",
    "        elif file.endswith(\".metadata.json\"):\n",
    "            metadata_json_size += file_size_kb\n",
    "        elif manifest_pattern.match(file):\n",
    "            m_avro_size += file_size_kb\n",
    "    \n",
    "    # Print the time taken and the sizes of the metadata files\n",
    "    # print(f\"Time taken to read Parquet files: {time_taken:.2f} seconds\")\n",
    "    # print(f\"Size of snap-*.avro files: {snap_avro_size:.2f} KB\")\n",
    "    # print(f\"Size of *.metadata.json files: {metadata_json_size:.2f} KB\")\n",
    "    # print(f\"Size of *m{0-9}{1,}.avro files: {m_avro_size:.2f} KB\")\n",
    "\n",
    "    return {\"data_dir_size\": data_dir_size,\"metadata_size\": metadata_json_size,\"snapshot_size\": snap_avro_size,\"manifest_size\": m_avro_size}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7587c21-bddb-4b48-80e6-32f2d827d5c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, FloatType, LongType, StructType,StructField, StringType\n",
    "schema = StructType([\n",
    "  StructField(\"vendor_id\", LongType(), True),\n",
    "  StructField(\"trip_id\", LongType(), True),\n",
    "  StructField(\"trip_distance\", FloatType(), True),\n",
    "  StructField(\"fare_amount\", DoubleType(), True),\n",
    "  StructField(\"store_and_fwd_flag\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame([], schema)\n",
    "df.writeTo(\"demo.nyc.taxis_10M\").create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e629758c-b748-4a02-bd27-662baad3ff03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------+-----------+------------------+\n",
      "|vendor_id|trip_id|trip_distance|fare_amount|store_and_fwd_flag|\n",
      "+---------+-------+-------------+-----------+------------------+\n",
      "+---------+-------+-------------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"demo.nyc.taxis_10M\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97079507-6b27-4991-aed7-757bfd1ac48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter input file type csv or parquet? :  parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started with file=records_1000000_part_10_1740473176.1294348.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started with file=records_1000000_part_1_1740473116.5725977.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_2_1740473123.6205893.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_3_1740473130.7518847.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_4_1740473137.6493435.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_5_1740473144.4021842.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_6_1740473151.2823489.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_7_1740473157.6888306.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_8_1740473163.7258036.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_9_1740473169.8142922.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total insertion time: 19.61 sec\n"
     ]
    }
   ],
   "source": [
    "import time, csv\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "input_data_dir = f\"../input_data\"\n",
    "output_dir = f\"../output\"\n",
    "analysis_info = []\n",
    "records_before_op = 0\n",
    "total_insertion_time = 0  # Track total insertion time\n",
    "\n",
    "file_type = input(\"Enter input file type csv or parquet? : \")\n",
    "file_type = file_type.lower().strip()\n",
    "input_data_dir = os.path.join(input_data_dir, file_type)\n",
    "input_files = os.listdir(input_data_dir)\n",
    "\n",
    "analysis_file = os.path.join(output_dir, f\"analysis_info_{file_type}.csv\")\n",
    "if os.path.exists(analysis_file):\n",
    "    os.remove(analysis_file)\n",
    "\n",
    "df = spark.table(\"demo.nyc.taxis_10M\")\n",
    "records_before_op = df.count()\n",
    "digits = len(str(records_before_op))\n",
    "\n",
    "for file in input_files:\n",
    "    print(f\"Started with file={file}\")\n",
    "    file_path = os.path.join(input_data_dir, file)\n",
    "\n",
    "    st = time.time()\n",
    "    if file_type == \"parquet\":\n",
    "        df = spark.read.parquet(file_path)\n",
    "    else:\n",
    "        df = spark.read.csv(file_path, header=True)\n",
    "        df = df.select(\n",
    "            F.col(\"vendor_id\").cast(\"long\").alias(\"vendor_id\"),\n",
    "            F.col(\"trip_id\").cast(\"long\").alias(\"trip_id\"),\n",
    "            F.col(\"trip_distance\").cast(\"float\").alias(\"trip_distance\"),\n",
    "            F.col(\"fare_amount\").cast(\"double\").alias(\"fare_amount\"),\n",
    "            F.col(\"store_and_fwd_flag\").cast(\"string\").alias(\"store_and_fwd_flag\")\n",
    "        )\n",
    "    \n",
    "    rows = df.count()\n",
    "    \n",
    "    df.writeTo(\"demo.nyc.taxis_10M\").append()\n",
    "    end = time.time() - st\n",
    "    total_insertion_time += end  # Accumulate insertion time\n",
    "\n",
    "    details = get_size()\n",
    "    details[\"time_taken\"] = f\"{end:.2f} sec\"\n",
    "    details[\"Operation\"] = f\"Inserted {rows} records\"\n",
    "    details[\"records_after_op\"] = records_before_op + rows\n",
    "\n",
    "    records_before_op += rows\n",
    "    del st, end, df\n",
    "\n",
    "    append_to_file(analysis_file, details)\n",
    "    analysis_info.append(details)\n",
    "    \n",
    "    print(f\"Inserted {rows} records..\")\n",
    "\n",
    "    current_digit = len(str(records_before_op))\n",
    "\n",
    "    if current_digit <= digits:\n",
    "        continue\n",
    "    else:\n",
    "        digits = current_digit\n",
    "        df = spark.table(\"demo.nyc.taxis_10M\")\n",
    "    \n",
    "    for vendor_id in df.select('vendor_id').distinct().collect()[:10]:\n",
    "        vendor_id = vendor_id[0]\n",
    "        df = spark.table(\"demo.nyc.taxis_10M\")\n",
    "\n",
    "        st = time.time()\n",
    "        updated_df = df.withColumn(\"fare_amount\", \n",
    "                                  when(col(\"vendor_id\") == vendor_id, col(\"fare_amount\") + 40)\n",
    "                                  .otherwise(col(\"fare_amount\")))\n",
    "        \n",
    "        updated_df.writeTo(\"demo.nyc.taxis_10M\").overwritePartitions()\n",
    "        \n",
    "        end = time.time() - st\n",
    "        rows = updated_df.filter(updated_df['vendor_id'] == vendor_id).count()\n",
    "\n",
    "        details = get_size()\n",
    "        details[\"time_taken\"] = f\"{end:.2f} sec\"\n",
    "        details[\"Operation\"] = f\"Updated {rows} records\"\n",
    "        details[\"records_after_op\"] = records_before_op\n",
    "\n",
    "        append_to_file(analysis_file, details)\n",
    "\n",
    "        del df, st, end\n",
    "        analysis_info.append(details)\n",
    "\n",
    "# Print total insertion time at the end\n",
    "print(f\"\\nTotal insertion time: {total_insertion_time:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c390ba-cc7b-401a-897a-a71e2dfca2a1",
   "metadata": {},
   "source": [
    "## Perform operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca727b2e-ad22-41af-bca3-809ad57f02f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter input file type csv or parquet? :  parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started with file=records_1000000_part_10_1740473176.1294348.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started with file=records_1000000_part_1_1740473116.5725977.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n",
      "Started with file=records_1000000_part_2_1740473123.6205893.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n",
      "Started with file=records_1000000_part_3_1740473130.7518847.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n",
      "Started with file=records_1000000_part_4_1740473137.6493435.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n",
      "Started with file=records_1000000_part_5_1740473144.4021842.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n",
      "Started with file=records_1000000_part_6_1740473151.2823489.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n",
      "Started with file=records_1000000_part_7_1740473157.6888306.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n",
      "Started with file=records_1000000_part_8_1740473163.7258036.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n",
      "Started with file=records_1000000_part_9_1740473169.8142922.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c40c8c0-6c85-4e72-acd9-bf7bf2cb1f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw analysis text file created: ../output/analysis_info_parquet.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"raw analysis text file created: {analysis_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d27ee99-b613-4087-b7a0-eddc7823a3de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'data_dir_size': 0.5,\n",
       "  'metadata_size': 1.0,\n",
       "  'snapshot_size': 1.0,\n",
       "  'manifest_size': 0.5,\n",
       "  'time_taken': '5.029073238372803 sec',\n",
       "  'Operation': 'Inserted 1000000 records',\n",
       "  'records_after_op': 1000000},\n",
       " {'data_dir_size': 1.0,\n",
       "  'metadata_size': 1.5,\n",
       "  'snapshot_size': 1.5,\n",
       "  'manifest_size': 1.5,\n",
       "  'time_taken': '2.3605992794036865 sec',\n",
       "  'Operation': 'Updated 9941 records',\n",
       "  'records_after_op': 1000000},\n",
       " {'data_dir_size': 1.5,\n",
       "  'metadata_size': 2.0,\n",
       "  'snapshot_size': 2.0,\n",
       "  'manifest_size': 2.5,\n",
       "  'time_taken': '2.579869508743286 sec',\n",
       "  'Operation': 'Updated 9886 records',\n",
       "  'records_after_op': 1000000},\n",
       " {'data_dir_size': 2.0,\n",
       "  'metadata_size': 2.5,\n",
       "  'snapshot_size': 2.5,\n",
       "  'manifest_size': 3.5,\n",
       "  'time_taken': '2.046767234802246 sec',\n",
       "  'Operation': 'Updated 10106 records',\n",
       "  'records_after_op': 1000000},\n",
       " {'data_dir_size': 2.5,\n",
       "  'metadata_size': 3.0,\n",
       "  'snapshot_size': 3.0,\n",
       "  'manifest_size': 4.5,\n",
       "  'time_taken': '2.135364532470703 sec',\n",
       "  'Operation': 'Updated 10161 records',\n",
       "  'records_after_op': 1000000},\n",
       " {'data_dir_size': 3.0,\n",
       "  'metadata_size': 3.5,\n",
       "  'snapshot_size': 3.5,\n",
       "  'manifest_size': 5.5,\n",
       "  'time_taken': '2.0601184368133545 sec',\n",
       "  'Operation': 'Updated 9951 records',\n",
       "  'records_after_op': 1000000},\n",
       " {'data_dir_size': 3.5,\n",
       "  'metadata_size': 4.0,\n",
       "  'snapshot_size': 4.0,\n",
       "  'manifest_size': 6.5,\n",
       "  'time_taken': '1.9497051239013672 sec',\n",
       "  'Operation': 'Updated 10042 records',\n",
       "  'records_after_op': 1000000},\n",
       " {'data_dir_size': 4.0,\n",
       "  'metadata_size': 4.5,\n",
       "  'snapshot_size': 4.5,\n",
       "  'manifest_size': 7.5,\n",
       "  'time_taken': '1.952420949935913 sec',\n",
       "  'Operation': 'Updated 10181 records',\n",
       "  'records_after_op': 1000000},\n",
       " {'data_dir_size': 4.5,\n",
       "  'metadata_size': 5.0,\n",
       "  'snapshot_size': 5.0,\n",
       "  'manifest_size': 8.5,\n",
       "  'time_taken': '2.31351375579834 sec',\n",
       "  'Operation': 'Updated 9964 records',\n",
       "  'records_after_op': 1000000},\n",
       " {'data_dir_size': 5.0,\n",
       "  'metadata_size': 5.5,\n",
       "  'snapshot_size': 5.5,\n",
       "  'manifest_size': 9.5,\n",
       "  'time_taken': '2.1709489822387695 sec',\n",
       "  'Operation': 'Updated 9892 records',\n",
       "  'records_after_op': 1000000},\n",
       " {'data_dir_size': 5.5,\n",
       "  'metadata_size': 6.0,\n",
       "  'snapshot_size': 6.0,\n",
       "  'manifest_size': 10.5,\n",
       "  'time_taken': '1.9839367866516113 sec',\n",
       "  'Operation': 'Updated 9960 records',\n",
       "  'records_after_op': 1000000},\n",
       " {'data_dir_size': 6.0,\n",
       "  'metadata_size': 6.5,\n",
       "  'snapshot_size': 6.5,\n",
       "  'manifest_size': 11.0,\n",
       "  'time_taken': '2.1394755840301514 sec',\n",
       "  'Operation': 'Inserted 1000000 records',\n",
       "  'records_after_op': 2000000},\n",
       " {'data_dir_size': 6.5,\n",
       "  'metadata_size': 7.0,\n",
       "  'snapshot_size': 7.0,\n",
       "  'manifest_size': 11.5,\n",
       "  'time_taken': '2.0915589332580566 sec',\n",
       "  'Operation': 'Inserted 1000000 records',\n",
       "  'records_after_op': 3000000},\n",
       " {'data_dir_size': 7.0,\n",
       "  'metadata_size': 7.5,\n",
       "  'snapshot_size': 7.5,\n",
       "  'manifest_size': 12.0,\n",
       "  'time_taken': '2.4676318168640137 sec',\n",
       "  'Operation': 'Inserted 1000000 records',\n",
       "  'records_after_op': 4000000},\n",
       " {'data_dir_size': 7.5,\n",
       "  'metadata_size': 8.0,\n",
       "  'snapshot_size': 8.0,\n",
       "  'manifest_size': 12.5,\n",
       "  'time_taken': '2.1026580333709717 sec',\n",
       "  'Operation': 'Inserted 1000000 records',\n",
       "  'records_after_op': 5000000},\n",
       " {'data_dir_size': 8.0,\n",
       "  'metadata_size': 8.5,\n",
       "  'snapshot_size': 8.5,\n",
       "  'manifest_size': 13.0,\n",
       "  'time_taken': '1.9342899322509766 sec',\n",
       "  'Operation': 'Inserted 1000000 records',\n",
       "  'records_after_op': 6000000},\n",
       " {'data_dir_size': 8.5,\n",
       "  'metadata_size': 9.0,\n",
       "  'snapshot_size': 9.0,\n",
       "  'manifest_size': 13.5,\n",
       "  'time_taken': '1.895963191986084 sec',\n",
       "  'Operation': 'Inserted 1000000 records',\n",
       "  'records_after_op': 7000000},\n",
       " {'data_dir_size': 9.0,\n",
       "  'metadata_size': 9.5,\n",
       "  'snapshot_size': 9.5,\n",
       "  'manifest_size': 14.0,\n",
       "  'time_taken': '1.9418895244598389 sec',\n",
       "  'Operation': 'Inserted 1000000 records',\n",
       "  'records_after_op': 8000000},\n",
       " {'data_dir_size': 9.5,\n",
       "  'metadata_size': 10.0,\n",
       "  'snapshot_size': 10.0,\n",
       "  'manifest_size': 14.5,\n",
       "  'time_taken': '2.0640904903411865 sec',\n",
       "  'Operation': 'Inserted 1000000 records',\n",
       "  'records_after_op': 9000000},\n",
       " {'data_dir_size': 10.0,\n",
       "  'metadata_size': 10.5,\n",
       "  'snapshot_size': 10.5,\n",
       "  'manifest_size': 15.0,\n",
       "  'time_taken': '2.214691162109375 sec',\n",
       "  'Operation': 'Inserted 1000000 records',\n",
       "  'records_after_op': 10000000},\n",
       " {'data_dir_size': 15.0,\n",
       "  'metadata_size': 11.0,\n",
       "  'snapshot_size': 11.0,\n",
       "  'manifest_size': 20.5,\n",
       "  'time_taken': '10.252361059188843 sec',\n",
       "  'Operation': 'Updated 99908 records',\n",
       "  'records_after_op': 10000000},\n",
       " {'data_dir_size': 20.0,\n",
       "  'metadata_size': 11.5,\n",
       "  'snapshot_size': 11.5,\n",
       "  'manifest_size': 21.5,\n",
       "  'time_taken': '13.243946552276611 sec',\n",
       "  'Operation': 'Updated 99961 records',\n",
       "  'records_after_op': 10000000},\n",
       " {'data_dir_size': 25.0,\n",
       "  'metadata_size': 12.0,\n",
       "  'snapshot_size': 12.0,\n",
       "  'manifest_size': 22.5,\n",
       "  'time_taken': '10.61682939529419 sec',\n",
       "  'Operation': 'Updated 100403 records',\n",
       "  'records_after_op': 10000000},\n",
       " {'data_dir_size': 30.0,\n",
       "  'metadata_size': 12.5,\n",
       "  'snapshot_size': 12.5,\n",
       "  'manifest_size': 23.5,\n",
       "  'time_taken': '9.719685554504395 sec',\n",
       "  'Operation': 'Updated 100407 records',\n",
       "  'records_after_op': 10000000},\n",
       " {'data_dir_size': 35.0,\n",
       "  'metadata_size': 13.0,\n",
       "  'snapshot_size': 13.0,\n",
       "  'manifest_size': 24.5,\n",
       "  'time_taken': '9.596880674362183 sec',\n",
       "  'Operation': 'Updated 99839 records',\n",
       "  'records_after_op': 10000000},\n",
       " {'data_dir_size': 40.0,\n",
       "  'metadata_size': 13.5,\n",
       "  'snapshot_size': 13.5,\n",
       "  'manifest_size': 25.5,\n",
       "  'time_taken': '8.439291715621948 sec',\n",
       "  'Operation': 'Updated 99593 records',\n",
       "  'records_after_op': 10000000},\n",
       " {'data_dir_size': 45.0,\n",
       "  'metadata_size': 14.0,\n",
       "  'snapshot_size': 14.0,\n",
       "  'manifest_size': 26.5,\n",
       "  'time_taken': '8.788921117782593 sec',\n",
       "  'Operation': 'Updated 99560 records',\n",
       "  'records_after_op': 10000000},\n",
       " {'data_dir_size': 50.0,\n",
       "  'metadata_size': 14.5,\n",
       "  'snapshot_size': 14.5,\n",
       "  'manifest_size': 27.5,\n",
       "  'time_taken': '9.136718034744263 sec',\n",
       "  'Operation': 'Updated 100094 records',\n",
       "  'records_after_op': 10000000},\n",
       " {'data_dir_size': 55.0,\n",
       "  'metadata_size': 15.0,\n",
       "  'snapshot_size': 15.0,\n",
       "  'manifest_size': 28.5,\n",
       "  'time_taken': '9.510155200958252 sec',\n",
       "  'Operation': 'Updated 99887 records',\n",
       "  'records_after_op': 10000000},\n",
       " {'data_dir_size': 60.0,\n",
       "  'metadata_size': 15.5,\n",
       "  'snapshot_size': 15.5,\n",
       "  'manifest_size': 29.5,\n",
       "  'time_taken': '10.86997103691101 sec',\n",
       "  'Operation': 'Updated 100222 records',\n",
       "  'records_after_op': 10000000}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb0b44e1-dafd-403f-a8c1-36dba013ff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b43edc2a-7797-41ef-905b-afec61f42868",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(analysis_file) and analysis_info:\n",
    "    pd.DataFrame(analysis_info).to_csv(analysis_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c82451f-af8b-4967-a82b-bd9b2510fed6",
   "metadata": {},
   "source": [
    "## Analysis file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "db238b82-a2dd-4bd0-aac4-c31cc39039b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to ../output/_analysis_details_parquet_1000000.xlsx\n",
      "Digitalwise DataFrame saved to ../output/_digitwise_analysis_details_parquet_1000000.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_type = \"parquet\"\n",
    "df = pd.read_csv(analysis_file)\n",
    "\n",
    "df = df.rename(columns={\"Unnamed: 0\":\"sr_no\",\"data_dir_size\": \"data_dir_size_kb\",\\\n",
    "                        \"metadata_size\":\"metadata_size_kb\", \"snapshot_size\": \"snapshot_size_kb\",\\\n",
    "                        \"manifest_size\":\"manifest_size_kb\", \"time_taken\": \"time_taken_sec\"})\n",
    "\n",
    "if \"sr_no\" not in df.columns:\n",
    "    df[\"sr_no\"] = df.index\n",
    "\n",
    "df[\"sr_no\"]+=1\n",
    "df['total_size_kb'] = df['data_dir_size_kb']+df['metadata_size_kb']+df['snapshot_size_kb']+df['manifest_size_kb']\n",
    "\n",
    "df['total_size_mb'] = df['total_size_kb'].apply(lambda x: round(x/1024,3))\n",
    "df['time_taken_sec'] = df['time_taken_sec'].apply(lambda x: round(float(x.split()[0]), 3))\n",
    "\n",
    "columns = ['sr_no','records_after_op', 'Operation', 'time_taken_sec', 'data_dir_size_kb', 'metadata_size_kb', \\\n",
    "           'snapshot_size_kb', 'manifest_size_kb', 'total_size_kb', 'total_size_mb']\n",
    "\n",
    "df = df[columns]\n",
    "\n",
    "# dump to excel file\n",
    "\n",
    "def save_df_to_excel(df, output_file, record_digitwise_output_file):\n",
    "    \"\"\"\n",
    "    Save DataFrame to an Excel file with different sheets based on the 'records_after_op' column.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to save.\n",
    "        output_file (str): The path to the output Excel file.\n",
    "    \"\"\"\n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine\n",
    "    with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "        # Get unique values in the 'records_after_op' column\n",
    "        unique_values = df['records_after_op'].unique()\n",
    "\n",
    "        # Iterate through unique values and save each subset to a different sheet\n",
    "        for value in unique_values:\n",
    "            subset_df = df[df['records_after_op'] == value]\n",
    "            sheet_name = f\"{value}_records\"\n",
    "            subset_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    \n",
    "    print(f\"DataFrame saved to {output_file}\")\n",
    "    # another excel file for multiply of 10 records in each sheet based on digits of records_after_op\n",
    "    # eg. val = 20000 -> 5 digits\n",
    "    df['digits'] = df['records_after_op'].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    with pd.ExcelWriter(record_digitwise_output_file, engine='xlsxwriter') as writer:\n",
    "        # Get unique values in the 'records_after_op' column\n",
    "        unique_values = df['digits'].unique()\n",
    "\n",
    "        # Iterate through unique values and save each subset to a different sheet\n",
    "        for value in unique_values:\n",
    "            subset_df = df[df['digits'] == value]\n",
    "            records = subset_df.iloc[df.shape[0]-1, 1]\n",
    "            sheet_name = f\"{value}_digits_{records}_records\"\n",
    "            subset_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    print(f\"Digitalwise DataFrame saved to {record_digitwise_output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "# Define the output Excel file path\n",
    "output_file = f'../output/_analysis_details_{file_type}_{df.loc[df.shape[0]-1, \"records_after_op\"]}.xlsx'\n",
    "record_digitwise_output_file = f'../output/_digitwise_analysis_details_{file_type}_{df.loc[df.shape[0]-1, \"records_after_op\"]}.xlsx'\n",
    "\n",
    "# Save the DataFrame to the Excel file\n",
    "save_df_to_excel(df, output_file, record_digitwise_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e93147e-754e-4a23-92dc-3cf4de808594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
