{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35e052-86b4-42f5-83cf-9efbddb22c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dcd5394c-0ce8-4b3d-91de-f9839ff09bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/28 13:25:39 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"records\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c0eb2fbb-97ed-4810-9303-bde3d14b628c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " spark.sql(\"DROP TABLE IF EXISTS demo.nyc.taxis_1000_50_product\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6bda4a8b-f73f-4b25-bc17-f95cb555aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "iceberg_table_dir = \"../warehouse/nyc/taxis_1000_50_product\"\n",
    "metadata_dir = f\"{iceberg_table_dir}/metadata\"\n",
    "data_dir = f\"{iceberg_table_dir}/data\"\n",
    "input_data_dir = f\"../input_data\"\n",
    "analysis_info = []\n",
    "records_before_op = 0\n",
    "\n",
    "def append_to_file(file_path, msg):\n",
    "    open_mode = \"a\"\n",
    "    if not os.path.exists(file_path):\n",
    "        open_mode = \"w\"\n",
    "\n",
    "    # Open the CSV file in write mode\n",
    "    with open(file_path, open_mode) as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        if open_mode==\"w\":\n",
    "            #writing header of the columns\n",
    "            writer.writerows([list(msg.keys())])    \n",
    "\n",
    "        row_values = [list(msg.values())]\n",
    "        # Write the data to the CSV file\n",
    "        writer.writerows(row_values)\n",
    "\n",
    "def get_size():\n",
    "    # List the metadata files\n",
    "    manifest_pattern = re.compile(r\".*-m\\d+\\.avro$\")\n",
    "    metadata_files = os.listdir(metadata_dir)\n",
    "    \n",
    "    # Initialize variables to store the sizes of different types of metadata files\n",
    "    snap_avro_size = 0\n",
    "    metadata_json_size = 0\n",
    "    m_avro_size = 0\n",
    "\n",
    "    data_dir_size = 0\n",
    "    # get data dir size\n",
    "    data_dir_files = os.listdir(data_dir)\n",
    "    # print(data_dir_files)\n",
    "    for filename in data_dir_files:\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        data_dir_size += os.path.getsize(file_path) / 1024  # Convert size to KB\n",
    "    \n",
    "    # Iterate through the metadata files and calculate their sizes\n",
    "    for file in metadata_files:\n",
    "        file_path = os.path.join(metadata_dir, file)\n",
    "        file_size_kb = os.path.getsize(file_path) / 1024  # Convert size to KB\n",
    "        \n",
    "        if file.startswith(\"snap-\") and file.endswith(\".avro\"):\n",
    "            snap_avro_size += file_size_kb\n",
    "        elif file.endswith(\".metadata.json\"):\n",
    "            metadata_json_size += file_size_kb\n",
    "        elif manifest_pattern.match(file):\n",
    "            m_avro_size += file_size_kb\n",
    "    \n",
    "    # Print the time taken and the sizes of the metadata files\n",
    "    # print(f\"Time taken to read Parquet files: {time_taken:.2f} seconds\")\n",
    "    # print(f\"Size of snap-*.avro files: {snap_avro_size:.2f} KB\")\n",
    "    # print(f\"Size of *.metadata.json files: {metadata_json_size:.2f} KB\")\n",
    "    # print(f\"Size of *m{0-9}{1,}.avro files: {m_avro_size:.2f} KB\")\n",
    "\n",
    "    return {\"data_dir_size\": data_dir_size,\"metadata_size\": metadata_json_size,\"snapshot_size\": snap_avro_size,\"manifest_size\": m_avro_size}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fc52770e-bf50-4028-9d58-916f919225c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/28 13:27:35 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|extra_col_0|extra_col_1|extra_col_2|extra_col_3|extra_col_4|extra_col_5|extra_col_6|extra_col_7|extra_col_8|extra_col_9|extra_col_10|extra_col_11|extra_col_12|extra_col_13|extra_col_14|extra_col_15|extra_col_16|extra_col_17|extra_col_18|extra_col_19|extra_col_20|extra_col_21|extra_col_22|extra_col_23|extra_col_24|extra_col_25|extra_col_26|extra_col_27|extra_col_28|extra_col_29|extra_col_30|extra_col_31|extra_col_32|extra_col_33|extra_col_34|extra_col_35|extra_col_36|extra_col_37|extra_col_38|extra_col_39|extra_col_40|extra_col_41|extra_col_42|extra_col_43|extra_col_44|extra_col_45|extra_col_46|extra_col_47|extra_col_48|extra_col_49|\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "| JAGYKOHUFK|       2415|        CAR| 2024-05-01| JTBZZUDDRO|       1247|         CV| 2022-06-01| BDLPVXJGNB|       5666|         CAR|  2021-04-01|  JQMBJAHAYP|        7155|         CAR|  2024-05-01|  ALAQAASHUW|         749|         CAR|  2022-12-01|  SZIDKBCWPT|        6117|          CV|  2020-09-01|  JOXAZMKAZU|        4949|          TW|  2020-03-01|  ELADLLICPQ|        5811|         CAR|  2022-10-01|  PEEZFEEHKY|        4042|          CV|  2021-09-01|  AOXFYSXWDO|        2682|         CAR|  2023-12-01|  DEIVSPRIUX|        4293|          TW|  2022-03-01|  TVETIEPQDQ|        6226|         CAR|  2022-04-01|  ZQKPUTZKRB|        9330|\n",
      "| CJJRXSDLLN|       2372|        CAR| 2023-10-01| JTDKDSXVPG|       2408|        CAR| 2024-01-01| YIXYEQVTEU|       3944|         CAR|  2022-05-01|  DUTIMBJCOJ|        4819|          TW|  2021-12-01|  IKPKBXUOMX|        1328|         CAR|  2023-06-01|  ILIWRCIRTQ|        2560|          CV|  2022-12-01|  GCCPCCVODP|          99|          CV|  2021-10-01|  XKIROPXGIB|        6087|          CV|  2021-05-01|  YCZRJBOPIE|        9875|         CAR|  2024-04-01|  UCOPXJMMGW|        8754|         CAR|  2022-04-01|  AMUCAFJEJB|        8130|         CAR|  2023-01-01|  XJURWKYSPE|         834|          CV|  2021-07-01|  ATKRQOJQLX|        5788|\n",
      "| AZRAGCRSUZ|       6793|        CAR| 2022-03-01| QNVNQVMVPR|       9635|        CAR| 2024-09-01| QCADRNGRMO|       3800|          TW|  2022-11-01|  GHFTDBSKFH|        9370|         CAR|  2024-09-01|  ZJYZGYVVOY|        8167|          TW|  2023-06-01|  MXCXWDGRIB|        9144|          TW|  2020-08-01|  XACULTXLUW|        9619|          TW|  2024-12-01|  XRMTZDQNOQ|        1060|          TW|  2020-01-01|  MTGZVBUYXY|        6221|          CV|  2020-09-01|  SRKPYQFBNN|        5909|          CV|  2020-07-01|  OYUATFNNMM|        8377|          CV|  2021-01-01|  CYYAXENRGD|        8436|          CV|  2022-10-01|  SULMGODFBB|        9271|\n",
      "| FGMTHJIMSP|       5390|         TW| 2021-09-01| ZDTYUJFJDY|       6555|         TW| 2020-11-01| IZKCLTPLMI|       9453|          TW|  2025-02-01|  NAGHSQDHRH|        8910|          TW|  2021-08-01|  CXZFCLDKLN|        1399|          CV|  2020-08-01|  IWPURUTWAT|        2765|          TW|  2021-03-01|  BFGZTBPFFO|        1057|         CAR|  2024-10-01|  ICITCUOPWT|        8108|          TW|  2022-03-01|  JFGZKNBFOX|        4749|         CAR|  2024-07-01|  CDHBXPJHKK|        4023|          CV|  2022-10-01|  JFHQNSNDFC|        9493|         CAR|  2023-11-01|  UPBGZQPUEF|         452|          TW|  2024-02-01|  OYVLIZLDQE|        6295|\n",
      "| EJOOFCOTHD|         53|         TW| 2023-07-01| RBTDMLVSXZ|       6511|        CAR| 2020-02-01| WOFGXYYXVQ|         75|          TW|  2024-07-01|  APEASUUNBG|         968|          TW|  2022-07-01|  RDRFUMVVPQ|         256|          TW|  2024-08-01|  EDLALVFDEF|        3026|          CV|  2022-05-01|  QRKRVLRUUE|        6747|         CAR|  2021-12-01|  ETXNPWRBFG|         883|          CV|  2021-08-01|  MGKMUNFSUY|        8245|          TW|  2022-06-01|  YMSVMPFPBP|         402|          CV|  2020-06-01|  DCCPGYARVO|        2083|          CV|  2024-10-01|  UUQRQCMHAS|        4809|         CAR|  2023-03-01|  OIJKIUZIES|        7925|\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, DateType\n",
    ")\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Generate Taxis Data\").getOrCreate()\n",
    "\n",
    "def generate_random_month_date():\n",
    "    \"\"\"Generate a random date with the first day of a month.\"\"\"\n",
    "    start_date = datetime(2020, 1, 1)\n",
    "    end_date = datetime.today()\n",
    "    delta = end_date - start_date\n",
    "    random_days = random.randint(0, delta.days)\n",
    "    random_date = start_date + timedelta(days=random_days)\n",
    "    \n",
    "    return random_date.replace(day=1)  # Ensuring DATE format\n",
    "\n",
    "# Define schema ensuring extra_col_3 is DateType\n",
    "schema = StructType([\n",
    "    StructField(f\"extra_col_{i}\", StringType(), True) if i % 4 == 0 else  \n",
    "    StructField(f\"extra_col_{i}\", IntegerType(), True) if i % 4 == 1 else  \n",
    "    StructField(f\"extra_col_{i}\", StringType(), True) if i % 4 == 2 else  \n",
    "    StructField(f\"extra_col_{i}\", DateType(), True)  # Ensure DATE type for extra_col_3\n",
    "    for i in range(50)\n",
    "])\n",
    "\n",
    "def generate_records(n):\n",
    "    \"\"\"Generate records based on schema.\"\"\"\n",
    "    data = []\n",
    "    for _ in range(n):\n",
    "        row = [\n",
    "            ''.join(random.choice(string.ascii_uppercase) for _ in range(10)) if i % 4 == 0 else\n",
    "            random.randint(1, 10000) if i % 4 == 1 else\n",
    "            random.choice([\"CAR\", \"TW\", \"CV\"]) if i % 4 == 2 else\n",
    "            generate_random_month_date()  # Ensure DATE type\n",
    "            for i in range(50)\n",
    "        ]\n",
    "        data.append(row)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate Data\n",
    "records = generate_records(1000)  # Adjust number of records\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(records, schema)\n",
    "\n",
    "# Ensure extra_col_3 is stored as DateType\n",
    "df = df.withColumn(\"extra_col_3\", F.to_date(F.col(\"extra_col_3\")))\n",
    "\n",
    "# Show sample records\n",
    "df.show(5)\n",
    "\n",
    "# Save to Iceberg Table\n",
    "# df.writeTo(\"demo.nyc.taxis_1000_50_product\").create()\n",
    "df.writeTo(\"demo.nyc.taxis_1000_50_product\") \\\n",
    "    .partitionedBy(\"extra_col_2\") \\\n",
    "    .create()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "72f44de1-3b71-4e66-9ebc-6dcdc99ccc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.table(\"demo.nyc.taxis_1000_50_product\")\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a8c952b9-cac6-407d-82c9-afcdf1f52a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter input file type (csv or parquet):  parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: records_1000_1_1740744671.parquet\n",
      "Inserted 1000 records in 1.40 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "input_data_dir = \"../input_data\"\n",
    "output_dir = \"../output\"\n",
    "\n",
    "file_type = input(\"Enter input file type (csv or parquet): \").lower().strip()\n",
    "input_data_dir = os.path.join(input_data_dir, file_type)\n",
    "input_files = os.listdir(input_data_dir)\n",
    "\n",
    "df = spark.table(\"demo.nyc.taxis_1000_50_product\")\n",
    "records_before_op = df.count()\n",
    "\n",
    "for file in input_files:\n",
    "    print(f\"Processing file: {file}\")\n",
    "    file_path = os.path.join(input_data_dir, file)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Read Data\n",
    "    if file_type == \"parquet\":\n",
    "        df = spark.read.parquet(file_path)\n",
    "    else:\n",
    "        df = spark.read.csv(file_path, header=True)\n",
    "        df = df.select(\n",
    "            *[F.col(f\"extra_col_{i}\").cast(\"string\" if i % 4 == 0 or i % 4 == 2 else \"int\" if i % 4 == 1 else \"date\") for i in range(50)]\n",
    "        )\n",
    "    \n",
    "    # Ensure extra_col_3 is in DATE format\n",
    "    df = df.withColumn(\"extra_col_3\", F.to_date(F.col(\"extra_col_3\")))\n",
    "\n",
    "    # Insert into Iceberg\n",
    "    df.writeTo(\"demo.nyc.taxis_1000_50_product\").append()\n",
    "\n",
    "    print(f\"Inserted {df.count()} records in {time.time() - start_time:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f5a28e02-7647-4b3f-bb71-14e722054298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|extra_col_2|extra_col_1|\n",
      "+-----------+-----------+\n",
      "|CAR        |2415       |\n",
      "|CAR        |2372       |\n",
      "|CAR        |6793       |\n",
      "|CAR        |4729       |\n",
      "|CAR        |1181       |\n",
      "|CAR        |1615       |\n",
      "|CAR        |866        |\n",
      "|CAR        |4326       |\n",
      "|CAR        |6330       |\n",
      "|CAR        |7213       |\n",
      "|CAR        |8271       |\n",
      "|CAR        |9111       |\n",
      "|CAR        |9215       |\n",
      "|CAR        |3581       |\n",
      "|CAR        |4407       |\n",
      "|CAR        |1463       |\n",
      "|CAR        |7519       |\n",
      "|CAR        |1892       |\n",
      "|CAR        |4969       |\n",
      "|CAR        |4185       |\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time taken for the query: 0.014000415802001953 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Define the complex SQL query for 'nyc.taxis_10000_50COLUMNS'\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "extra_col_2 , extra_col_1\n",
    "FROM demo.nyc.taxis_1000_50_product\n",
    "where extra_col_2 = 'CAR'\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "df = spark.sql(query)\n",
    "\n",
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Show the result\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Print the time taken for the query\n",
    "print(f\"Time taken for the query: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bf1cf45b-a9c9-43dc-8ff4-52f467263cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.sql(\"SELECT * FROM demo.nyc.taxis_1000_50_product.files\")\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "44a59adc-2d35-46d2-bdac-153e26e3d304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=spark.sql(\"SELECT * FROM demo.nyc.taxis_1000_50_product.partitions\")\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f0a764a6-bb7b-433a-9e0e-e3bf845e6446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.sql(\"CREATE TABLE demo.nyc.taxis_1000_50_product_partitioned USING ICEBERG PARTITIONED BY (extra_col_2)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c10d257f-7f8b-4166-ad19-aed84875683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"DESCRIBE FORMATTED demo.nyc.taxis_1000_50_product\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d761c000-e16f-483a-a354-5abee2a969de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"SELECT DISTINCT extra_col_2 FROM demo.nyc.taxis_1000_50_product_partitioned\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a30758ce-d4ba-44e1-b476-8247f45d1f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed1d7df-2f3c-4821-ace6-d70f53abc945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969dace0-dedc-47aa-9163-25210d2264e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
