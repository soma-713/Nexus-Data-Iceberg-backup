{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "03cfef9b-d416-4d72-901b-10529cefb734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (2.2.1)\n",
      "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.9/site-packages (3.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.9/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ea514196-f134-47c0-a4a0-0dc55898c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"records\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a98f1b74-aa97-435e-a719-d1363e255b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|      nyc|           taxis_100|      false|\n",
      "|      nyc|     taxis_100M_time|      false|\n",
      "|      nyc|taxis_1000_50COLUMNS|      false|\n",
      "|      nyc|            taxis_1B|      false|\n",
      "|      nyc|taxis_100_50COLUM...|      false|\n",
      "|      nyc|taxis_100000_50CO...|      false|\n",
      "|      nyc|            taxis_1L|      false|\n",
      "|      nyc|          taxis_1000|      false|\n",
      "|      nyc|            taxis_10|      false|\n",
      "|      nyc| taxis_10M_50COLUMNS|      false|\n",
      "|      nyc|   taxis_1L_5COLUMNS|      false|\n",
      "|      nyc|         taxis_10000|      false|\n",
      "|      nyc|            taxis_1M|      false|\n",
      "|      nyc|          taxis_1L_5|      false|\n",
      "|      nyc|          taxis_10_M|      false|\n",
      "|      nyc|taxis_1000_50COLU...|      false|\n",
      "|      nyc|  taxis_10_50COLUMNS|      false|\n",
      "|      nyc|           taxis_10K|      false|\n",
      "|      nyc|            taxis_1K|      false|\n",
      "|      nyc|           taxis_10L|      false|\n",
      "+---------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN demo.nyc\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "855ff10f-6045-4439-aaa6-64d6312733e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o39.sql.\n: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: \n\tat org.apache.iceberg.jdbc.JdbcCatalog.listTables(JdbcCatalog.java:304)\n\tat org.apache.iceberg.CachingCatalog.listTables(CachingCatalog.java:135)\n\tat org.apache.iceberg.spark.SparkCatalog.listTables(SparkCatalog.java:417)\n\tat org.apache.spark.sql.execution.datasources.v2.ShowTablesExec.run(ShowTablesExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat jdk.internal.reflect.GeneratedMethodAccessor234.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSHOW TABLES IN demo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o39.sql.\n: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: \n\tat org.apache.iceberg.jdbc.JdbcCatalog.listTables(JdbcCatalog.java:304)\n\tat org.apache.iceberg.CachingCatalog.listTables(CachingCatalog.java:135)\n\tat org.apache.iceberg.spark.SparkCatalog.listTables(SparkCatalog.java:417)\n\tat org.apache.spark.sql.execution.datasources.v2.ShowTablesExec.run(ShowTablesExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat jdk.internal.reflect.GeneratedMethodAccessor234.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN demo\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "884481c8-66d1-4b6c-ae65-006713c54bdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `demo`.`nyc`.`taxis_100_50COLUMNS` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'RefreshTable\n+- 'UnresolvedTableOrView [demo, nyc, taxis_100_50COLUMNS], REFRESH TABLE, true\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mREFRESH TABLE demo.nyc.taxis_100_50COLUMNS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `demo`.`nyc`.`taxis_100_50COLUMNS` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'RefreshTable\n+- 'UnresolvedTableOrView [demo, nyc, taxis_100_50COLUMNS], REFRESH TABLE, true\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"REFRESH TABLE demo.nyc.taxis_100_50COLUMNS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6d293c73-be5e-4d12-8c11-90173cea5ed0",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o167702.load.\n: org.apache.iceberg.exceptions.NotFoundException: Location does not exist: s3://warehouse/nyc/taxis_1000_50COLUMNS/metadata/00046-77d1c2f6-2413-489f-bc2b-08d524a7594a.metadata.json\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:194)\n\tat org.apache.iceberg.aws.s3.S3InputStream.positionStream(S3InputStream.java:177)\n\tat org.apache.iceberg.aws.s3.S3InputStream.read(S3InputStream.java:107)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.ensureLoaded(ByteSourceJsonBootstrapper.java:539)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.detectEncoding(ByteSourceJsonBootstrapper.java:133)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.constructParser(ByteSourceJsonBootstrapper.java:256)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory._createParser(JsonFactory.java:1744)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:1143)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3809)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:280)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:273)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$0(BaseMetastoreTableOperations.java:189)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:208)\n\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:208)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:185)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:176)\n\tat org.apache.iceberg.jdbc.JdbcTableOperations.doRefresh(JdbcTableOperations.java:100)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:97)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:80)\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:843)\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:170)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:355)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:137)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:210)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3, Status Code: 404, Request ID: 1827D1DE977A9BA1, Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleErrorResponse(CombinedResponseHandler.java:125)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleResponse(CombinedResponseHandler.java:82)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:60)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:41)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:50)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:38)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:72)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:55)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:39)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\n\tat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:224)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$0(BaseSyncClientHandler.java:66)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:60)\n\tat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:52)\n\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:60)\n\tat software.amazon.awssdk.services.s3.DefaultS3Client.getObject(DefaultS3Client.java:5174)\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:192)\n\t... 50 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the existing Apache Iceberg table\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miceberg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdemo.nyc.taxis_1000_50COLUMNS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Get total record count\u001b[39;00m\n\u001b[1;32m      9\u001b[0m total_rows \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o167702.load.\n: org.apache.iceberg.exceptions.NotFoundException: Location does not exist: s3://warehouse/nyc/taxis_1000_50COLUMNS/metadata/00046-77d1c2f6-2413-489f-bc2b-08d524a7594a.metadata.json\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:194)\n\tat org.apache.iceberg.aws.s3.S3InputStream.positionStream(S3InputStream.java:177)\n\tat org.apache.iceberg.aws.s3.S3InputStream.read(S3InputStream.java:107)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.ensureLoaded(ByteSourceJsonBootstrapper.java:539)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.detectEncoding(ByteSourceJsonBootstrapper.java:133)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.constructParser(ByteSourceJsonBootstrapper.java:256)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory._createParser(JsonFactory.java:1744)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:1143)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3809)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:280)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:273)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$0(BaseMetastoreTableOperations.java:189)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:208)\n\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:208)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:185)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:176)\n\tat org.apache.iceberg.jdbc.JdbcTableOperations.doRefresh(JdbcTableOperations.java:100)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:97)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:80)\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:843)\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:170)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:355)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:137)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:210)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3, Status Code: 404, Request ID: 1827D1DE977A9BA1, Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleErrorResponse(CombinedResponseHandler.java:125)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleResponse(CombinedResponseHandler.java:82)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:60)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:41)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:50)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:38)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:72)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:55)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:39)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\n\tat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:224)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$0(BaseSyncClientHandler.java:66)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:60)\n\tat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:52)\n\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:60)\n\tat software.amazon.awssdk.services.s3.DefaultS3Client.getObject(DefaultS3Client.java:5174)\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:192)\n\t... 50 more\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import col, when, lit, rand\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load the existing Apache Iceberg table\n",
    "df = spark.read.format(\"iceberg\").load(\"demo.nyc.taxis_1000_50COLUMNS\")\n",
    "\n",
    "# Get total record count\n",
    "total_rows = df.count()\n",
    "print(f\"Total rows in Iceberg table: {total_rows}\")\n",
    "\n",
    "# === Get Update Percentage from User ===\n",
    "update_percentage = float(input(\"Enter update percentage (e.g., 1 for 1%): \").strip()) / 100\n",
    "num_rows = int(total_rows * update_percentage)\n",
    "\n",
    "print(f\"Updating {num_rows} rows (~{update_percentage*100}%)...\")\n",
    "\n",
    "# Sample unique IDs to update\n",
    "sampled_df = df.select(\"extra_col_0\").distinct().orderBy(rand()).limit(num_rows)\n",
    "sampled_ids = [row[\"extra_col_0\"] for row in sampled_df.collect()]\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "# Update records using Iceberg's MERGE INTO\n",
    "query = f\"\"\"\n",
    "MERGE INTO demo.nyc.taxis_1000_50COLUMNS AS target\n",
    "USING (\n",
    "    SELECT extra_col_0 FROM demo.nyc.taxis_1000_50COLUMNS\n",
    "    WHERE extra_col_0 IN ({\",\".join(map(str, sampled_ids))})\n",
    ") AS source\n",
    "ON target.extra_col_0 = source.extra_col_0\n",
    "WHEN MATCHED THEN \n",
    "    UPDATE SET target.extra_col_1 = target.extra_col_1 + 10\n",
    "\"\"\"\n",
    "spark.sql(query)\n",
    "\n",
    "end = time.time() - st\n",
    "print(f\"Updated {num_rows} rows in {end:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "527f1ab6-0719-4221-a0fe-ebc5bf67b4a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o167698.load.\n: org.apache.iceberg.exceptions.NotFoundException: Location does not exist: s3://warehouse/nyc/taxis_1000_50COLUMNS/metadata/00046-77d1c2f6-2413-489f-bc2b-08d524a7594a.metadata.json\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:194)\n\tat org.apache.iceberg.aws.s3.S3InputStream.positionStream(S3InputStream.java:177)\n\tat org.apache.iceberg.aws.s3.S3InputStream.read(S3InputStream.java:107)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.ensureLoaded(ByteSourceJsonBootstrapper.java:539)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.detectEncoding(ByteSourceJsonBootstrapper.java:133)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.constructParser(ByteSourceJsonBootstrapper.java:256)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory._createParser(JsonFactory.java:1744)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:1143)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3809)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:280)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:273)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$0(BaseMetastoreTableOperations.java:189)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:208)\n\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:208)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:185)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:176)\n\tat org.apache.iceberg.jdbc.JdbcTableOperations.doRefresh(JdbcTableOperations.java:100)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:97)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:80)\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:843)\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:170)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:355)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:137)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:210)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3, Status Code: 404, Request ID: 1827D1CA25FE4B98, Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleErrorResponse(CombinedResponseHandler.java:125)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleResponse(CombinedResponseHandler.java:82)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:60)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:41)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:50)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:38)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:72)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:55)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:39)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\n\tat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:224)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$0(BaseSyncClientHandler.java:66)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:60)\n\tat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:52)\n\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:60)\n\tat software.amazon.awssdk.services.s3.DefaultS3Client.getObject(DefaultS3Client.java:5174)\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:192)\n\t... 50 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 13\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize Spark Session\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# spark = SparkSession.builder \\\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     .appName(\"Iceberg Merge\") \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Load the existing Apache Iceberg table\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miceberg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdemo.nyc.taxis_1000_50COLUMNS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Get total record count\u001b[39;00m\n\u001b[1;32m     16\u001b[0m total_rows \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o167698.load.\n: org.apache.iceberg.exceptions.NotFoundException: Location does not exist: s3://warehouse/nyc/taxis_1000_50COLUMNS/metadata/00046-77d1c2f6-2413-489f-bc2b-08d524a7594a.metadata.json\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:194)\n\tat org.apache.iceberg.aws.s3.S3InputStream.positionStream(S3InputStream.java:177)\n\tat org.apache.iceberg.aws.s3.S3InputStream.read(S3InputStream.java:107)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.ensureLoaded(ByteSourceJsonBootstrapper.java:539)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.detectEncoding(ByteSourceJsonBootstrapper.java:133)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.json.ByteSourceJsonBootstrapper.constructParser(ByteSourceJsonBootstrapper.java:256)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory._createParser(JsonFactory.java:1744)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:1143)\n\tat org.apache.iceberg.shaded.com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3809)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:280)\n\tat org.apache.iceberg.TableMetadataParser.read(TableMetadataParser.java:273)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$0(BaseMetastoreTableOperations.java:189)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(BaseMetastoreTableOperations.java:208)\n\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:208)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:185)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(BaseMetastoreTableOperations.java:176)\n\tat org.apache.iceberg.jdbc.JdbcTableOperations.doRefresh(JdbcTableOperations.java:100)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:97)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:80)\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:843)\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:170)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.getTable(CatalogV2Util.scala:355)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.loadV2Source(DataSourceV2Utils.scala:137)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:210)\n\tat scala.Option.flatMap(Option.scala:271)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3, Status Code: 404, Request ID: 1827D1CA25FE4B98, Extended Request ID: dd9025bab4ad464b049177c95eb6ebf374d3b3fd1af9251148b658df7ac2e3e8)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleErrorResponse(CombinedResponseHandler.java:125)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handleResponse(CombinedResponseHandler.java:82)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:60)\n\tat software.amazon.awssdk.core.internal.http.CombinedResponseHandler.handle(CombinedResponseHandler.java:41)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:50)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:38)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:72)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:55)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:39)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:50)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:32)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\n\tat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:224)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$0(BaseSyncClientHandler.java:66)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:60)\n\tat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:52)\n\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:60)\n\tat software.amazon.awssdk.services.s3.DefaultS3Client.getObject(DefaultS3Client.java:5174)\n\tat org.apache.iceberg.aws.s3.S3InputStream.openStream(S3InputStream.java:192)\n\t... 50 more\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Iceberg Merge\") \\\n",
    "#     .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "#     .config(\"spark.sql.catalog.demo.type\", \"hadoop\") \\\n",
    "#     .config(\"spark.sql.catalog.demo.warehouse\", \"s3://your-bucket/path-to-iceberg/\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# Load the existing Apache Iceberg table\n",
    "df = spark.read.format(\"iceberg\").load(\"demo.nyc.taxis_1000_50COLUMNS\")\n",
    "\n",
    "# Get total record count\n",
    "total_rows = df.count()\n",
    "print(f\"Total rows in Iceberg table: {total_rows}\")\n",
    "\n",
    "# === Get Update Percentage from User ===\n",
    "update_percentage = float(input(\"Enter update percentage (e.g., 1 for 1%): \").strip()) / 100\n",
    "num_rows = int(total_rows * update_percentage)\n",
    "print(f\"Updating {num_rows} rows (~{update_percentage*100}%)...\")\n",
    "\n",
    "# ✅ Use a deterministic sampling approach (e.g., modulo filtering)\n",
    "sampled_df = df.filter(\"extra_col_0 % 10 = 0\").limit(num_rows)\n",
    "\n",
    "# Record start time\n",
    "st = time.time()\n",
    "\n",
    "# Create a temporary table for sampled records\n",
    "sampled_df.createOrReplaceTempView(\"sampled_ids\")\n",
    "\n",
    "# ✅ Perform deterministic Iceberg MERGE INTO\n",
    "query = \"\"\"\n",
    "MERGE INTO demo.nyc.taxis_1000_50COLUMNS AS target\n",
    "USING sampled_ids AS source\n",
    "ON target.extra_col_0 = source.extra_col_0\n",
    "WHEN MATCHED THEN \n",
    "    UPDATE SET target.extra_col_1 = target.extra_col_1 + 10\n",
    "\"\"\"\n",
    "spark.sql(query)\n",
    "\n",
    "# Print execution time\n",
    "end = time.time() - st\n",
    "print(f\"Updated {num_rows} rows in {end:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "93c67ab3-8908-410a-8a5b-84b285463479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql(\"DROP TABLE IF EXISTS demo.nyc.taxis_1M_50COLUMNS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc655ec2-a28e-4a98-aa7b-377e56094c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# iceberg_table_dir = \"../warehouse/nyc/taxis_1M_50COLUMNS_Update_5\"\n",
    "# metadata_dir = f\"{iceberg_table_dir}/metadata\"\n",
    "# data_dir = f\"{iceberg_table_dir}/data\"\n",
    "# input_data_dir = f\"../input_data\"\n",
    "# analysis_info = []\n",
    "# records_before_op = 0\n",
    "\n",
    "# def append_to_file(file_path, msg):\n",
    "#     open_mode = \"a\"\n",
    "#     if not os.path.exists(file_path):\n",
    "#         open_mode = \"w\"\n",
    "\n",
    "#     # Open the CSV file in write mode\n",
    "#     with open(file_path, open_mode) as file:\n",
    "#         writer = csv.writer(file)\n",
    "        \n",
    "#         if open_mode==\"w\":\n",
    "#             #writing header of the columns\n",
    "#             writer.writerows([list(msg.keys())])    \n",
    "\n",
    "#         row_values = [list(msg.values())]\n",
    "#         # Write the data to the CSV file\n",
    "#         writer.writerows(row_values)\n",
    "\n",
    "# def get_size():\n",
    "#     # List the metadata files\n",
    "#     manifest_pattern = re.compile(r\".*-m\\d+\\.avro$\")\n",
    "#     metadata_files = os.listdir(metadata_dir)\n",
    "    \n",
    "#     # Initialize variables to store the sizes of different types of metadata files\n",
    "#     snap_avro_size = 0\n",
    "#     metadata_json_size = 0\n",
    "#     m_avro_size = 0\n",
    "\n",
    "#     data_dir_size = 0\n",
    "#     # get data dir size\n",
    "#     data_dir_files = os.listdir(data_dir)\n",
    "#     # print(data_dir_files)\n",
    "#     for filename in data_dir_files:\n",
    "#         file_path = os.path.join(data_dir, filename)\n",
    "#         data_dir_size += os.path.getsize(file_path) / 1024  # Convert size to KB\n",
    "    \n",
    "#     # Iterate through the metadata files and calculate their sizes\n",
    "#     for file in metadata_files:\n",
    "#         file_path = os.path.join(metadata_dir, file)\n",
    "#         file_size_kb = os.path.getsize(file_path) / 1024  # Convert size to KB\n",
    "        \n",
    "#         if file.startswith(\"snap-\") and file.endswith(\".avro\"):\n",
    "#             snap_avro_size += file_size_kb\n",
    "#         elif file.endswith(\".metadata.json\"):\n",
    "#             metadata_json_size += file_size_kb\n",
    "#         elif manifest_pattern.match(file):\n",
    "#             m_avro_size += file_size_kb\n",
    "    \n",
    "#     # Print the time taken and the sizes of the metadata files\n",
    "#     # print(f\"Time taken to read Parquet files: {time_taken:.2f} seconds\")\n",
    "#     # print(f\"Size of snap-*.avro files: {snap_avro_size:.2f} KB\")\n",
    "#     # print(f\"Size of *.metadata.json files: {metadata_json_size:.2f} KB\")\n",
    "#     # print(f\"Size of *m{0-9}{1,}.avro files: {m_avro_size:.2f} KB\")\n",
    "\n",
    "#     return {\"data_dir_size\": data_dir_size,\"metadata_size\": metadata_json_size,\"snapshot_size\": snap_avro_size,\"manifest_size\": m_avro_size}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d48d848d-a9c1-43f3-b2b2-f7b62d94b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import (\n",
    "#     DoubleType, FloatType, LongType, StructType, StructField, \n",
    "#     StringType, IntegerType, DateType\n",
    "# )\n",
    "\n",
    "# # Define the schema with 50 columns based on the required data types\n",
    "# schema = StructType([\n",
    "#     # StructField(\"vendor_id\", LongType(), True),  # INT\n",
    "#     # StructField(\"trip_id\", LongType(), True),  # INT\n",
    "#     # StructField(\"trip_distance\", FloatType(), True),  # FLOAT\n",
    "#     # StructField(\"fare_amount\", DoubleType(), True),  # DOUBLE\n",
    "#     # StructField(\"store_and_fwd_flag\", StringType(), True)  # STRING\n",
    "# # ] + [\n",
    "#     # Assigning VARCHAR, INT, STRING, and DATE data types in a cyclic pattern\n",
    "#     StructField(f\"extra_col_{i}\", StringType(), True) if i % 4 == 0 else  # VARCHAR\n",
    "#     StructField(f\"extra_col_{i}\", IntegerType(), True) if i % 4 == 1 else  # INT\n",
    "#     StructField(f\"extra_col_{i}\", StringType(), True) if i % 4 == 2 else  # STRING\n",
    "#     StructField(f\"extra_col_{i}\", DateType(), True)  # DATE\n",
    "#     for i in range(50)\n",
    "# ])\n",
    "\n",
    "# # Create an empty DataFrame with the schema\n",
    "# df = spark.createDataFrame([], schema)\n",
    "\n",
    "# # Create the Iceberg table\n",
    "# df.writeTo(\"demo.nyc.taxis_1M_50COLUMNS_Update_5\").create()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "47d06967-7e69-4b3b-8bef-8298cfabfb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|extra_col_0|extra_col_1|extra_col_2|extra_col_3|extra_col_4|extra_col_5|extra_col_6|extra_col_7|extra_col_8|extra_col_9|extra_col_10|extra_col_11|extra_col_12|extra_col_13|extra_col_14|extra_col_15|extra_col_16|extra_col_17|extra_col_18|extra_col_19|extra_col_20|extra_col_21|extra_col_22|extra_col_23|extra_col_24|extra_col_25|extra_col_26|extra_col_27|extra_col_28|extra_col_29|extra_col_30|extra_col_31|extra_col_32|extra_col_33|extra_col_34|extra_col_35|extra_col_36|extra_col_37|extra_col_38|extra_col_39|extra_col_40|extra_col_41|extra_col_42|extra_col_43|extra_col_44|extra_col_45|extra_col_46|extra_col_47|extra_col_48|extra_col_49|\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df = spark.table(\"demo.nyc.taxis_1M_50COLUMNS_Update_5\")\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f785ab1d-91fe-4047-bf50-664a4629b791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter input file type csv or parquet? :  parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started with file=records_1000000_part_1_1740397925.6984496.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 33.17 sec.\n",
      "\n",
      "Total insertion time: 33.17 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter update percentage (e.g., 1 for 1%):  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 100000 rows (~10.0%)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/26 13:44:29 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 100000 rows in 154.69 sec\n"
     ]
    }
   ],
   "source": [
    "# import time, csv\n",
    "# from pyspark.sql.functions import col, when, lit\n",
    "# from pyspark.sql import functions as F\n",
    "# import os\n",
    "\n",
    "# input_data_dir = \"../input_data\"\n",
    "# output_dir = \"../output\"\n",
    "# analysis_info = []\n",
    "# records_before_op = 0\n",
    "# total_insertion_time = 0\n",
    "\n",
    "# file_type = input(\"Enter input file type csv or parquet? : \").lower().strip()\n",
    "# input_data_dir = os.path.join(input_data_dir, file_type)\n",
    "# input_files = os.listdir(input_data_dir)\n",
    "\n",
    "# analysis_file = os.path.join(output_dir, f\"analysis_info_{file_type}.csv\")\n",
    "# if os.path.exists(analysis_file):\n",
    "#     os.remove(analysis_file)\n",
    "\n",
    "# df = spark.table(\"demo.nyc.taxis_1M_50COLUMNS_Update_5\")\n",
    "# records_before_op = df.count()\n",
    "\n",
    "# for file in input_files:\n",
    "#     print(f\"Started with file={file}\")\n",
    "#     file_path = os.path.join(input_data_dir, file)\n",
    "\n",
    "#     st = time.time()\n",
    "#     if file_type == \"parquet\":\n",
    "#         df = spark.read.parquet(file_path)\n",
    "#     else:\n",
    "#         df = spark.read.csv(file_path, header=True)\n",
    "#         df = df.select(\n",
    "#             F.col(\"extra_col_0\").cast(\"long\"),\n",
    "#             F.col(\"extra_col_1\").cast(\"int\"),\n",
    "#             F.col(\"extra_col_2\").cast(\"string\"),\n",
    "#             F.col(\"extra_col_3\").cast(\"date\"),\n",
    "#             *[F.col(f\"extra_col_{i}\").cast(\"string\" if i % 4 == 0 or i % 4 == 2 else \"int\" if i % 4 == 1 else \"date\") for i in range(4, 45)]\n",
    "#         )\n",
    "\n",
    "#     rows = df.count()\n",
    "    \n",
    "#     df.writeTo(\"demo.nyc.taxis_1M_50COLUMNS_Update_5\").append()\n",
    "#     end = time.time() - st\n",
    "#     total_insertion_time += end\n",
    "\n",
    "#     details = {\"time_taken\": f\"{end:.2f} sec\", \"Operation\": f\"Inserted {rows} records\", \"records_after_op\": records_before_op + rows}\n",
    "#     records_before_op += rows\n",
    "\n",
    "#     print(f\"Inserted {rows} records in {end:.2f} sec.\")\n",
    "\n",
    "# # **PRINT INSERTION TIME BEFORE UPDATE**\n",
    "# print(f\"\\nTotal insertion time: {total_insertion_time:.2f} sec\\n\")\n",
    "\n",
    "# # === Get Update Percentage from User ===\n",
    "# update_percentage = float(input(\"Enter update percentage (e.g., 1 for 1%): \").strip()) / 100\n",
    "\n",
    "# df = spark.table(\"demo.nyc.taxis_1M_50COLUMNS_Update_5\")\n",
    "# total_rows = df.count()\n",
    "\n",
    "# num_rows = int(total_rows * update_percentage)\n",
    "# print(f\"Updating {num_rows} rows (~{update_percentage*100}%)...\")\n",
    "\n",
    "# # Sample random rows ensuring unique IDs\n",
    "# sampled_df = df.select(\"extra_col_0\").distinct().orderBy(F.rand()).limit(num_rows)\n",
    "# sampled_ids = [row[\"extra_col_0\"] for row in sampled_df.collect()]\n",
    "\n",
    "# st = time.time()\n",
    "\n",
    "# # Updating `extra_col_1` (integer)\n",
    "# updated_df = df.withColumn(\n",
    "#     \"extra_col_1\",\n",
    "#     when(col(\"extra_col_0\").isin(sampled_ids), col(\"extra_col_1\") + 10)\n",
    "#     .otherwise(col(\"extra_col_1\"))\n",
    "# )\n",
    "\n",
    "# # Updating `extra_col_3` (date) - Commented out\n",
    "# # updated_df = updated_df.withColumn(\n",
    "# #     \"extra_col_3\",\n",
    "# #     when(col(\"extra_col_0\").isin(sampled_ids), F.date_add(col(\"extra_col_3\"), 5))\n",
    "# #     .otherwise(col(\"extra_col_3\"))\n",
    "# # )\n",
    "\n",
    "# # Overwrite updated data\n",
    "# updated_df.writeTo(\"demo.nyc.taxis_1M_50COLUMNS_Update_5\").overwritePartitions()\n",
    "\n",
    "# end = time.time() - st\n",
    "\n",
    "# print(f\"Updated {num_rows} rows in {end:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c5c81709-4ca4-414c-9b3c-9d6e218d8e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in Iceberg table: 100000\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter update percentage (e.g., 1 for 1%):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 1000 rows (~1.0%)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 1000 rows in 3.37 sec\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "59f4bddd-5163-411d-a04b-4267ea56a7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in Iceberg table: 100000\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter update percentage (e.g., 1 for 1%):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 1000 rows (~1.0%)...\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[INVALID_NON_DETERMINISTIC_EXPRESSIONS] The operator expects a deterministic expression, but the actual expression is \"(extra_col_0 = extra_col_0)\", \"exists(extra_col_0)\".; line 2 pos 0;\nReplaceData\n:  +- Filter (outer(extra_col_0#15251) = extra_col_0#15361)\n:     +- SubqueryAlias source\n:        +- SubqueryAlias sampled_ids\n:           +- View (`sampled_ids`, [extra_col_0#15361])\n:              +- GlobalLimit 1000\n:                 +- LocalLimit 1000\n:                    +- Project [extra_col_0#15361]\n:                       +- Sort [_nondeterministic#15250 ASC NULLS FIRST], true\n:                          +- Project [extra_col_0#15361, rand(-736351859637484279) AS _nondeterministic#15250]\n:                             +- Deduplicate [extra_col_0#15361]\n:                                +- Project [extra_col_0#15361]\n:                                   +- RelationV2[extra_col_0#15361, extra_col_1#15362, extra_col_2#15363, extra_col_3#15364, extra_col_4#15365, extra_col_5#15366, extra_col_6#15367, extra_col_7#15368, extra_col_8#15369, extra_col_9#15370, extra_col_10#15371, extra_col_11#15372, extra_col_12#15373, extra_col_13#15374, extra_col_14#15375, extra_col_15#15376, extra_col_16#15377, extra_col_17#15378, extra_col_18#15379, extra_col_19#15380, extra_col_20#15381, extra_col_21#15382, extra_col_22#15383, extra_col_23#15384, ... 26 more fields] demo.nyc.taxis_100000_50COLUMNS demo.nyc.taxis_100000_50COLUMNS\n+- MergeRows[extra_col_0#15309, extra_col_1#15310, extra_col_2#15311, extra_col_3#15312, extra_col_4#15313, extra_col_5#15314, extra_col_6#15315, extra_col_7#15316, extra_col_8#15317, extra_col_9#15318, extra_col_10#15319, extra_col_11#15320, extra_col_12#15321, extra_col_13#15322, extra_col_14#15323, extra_col_15#15324, extra_col_16#15325, extra_col_17#15326, extra_col_18#15327, extra_col_19#15328, extra_col_20#15329, extra_col_21#15330, extra_col_22#15331, extra_col_23#15332, ... 27 more fields]\n   +- Join LeftOuter, (extra_col_0#15251 = extra_col_0#15086), leftHint=(strategy=no_broadcast_and_replication)\n      :- Project [extra_col_0#15251, extra_col_1#15252, extra_col_2#15253, extra_col_3#15254, extra_col_4#15255, extra_col_5#15256, extra_col_6#15257, extra_col_7#15258, extra_col_8#15259, extra_col_9#15260, extra_col_10#15261, extra_col_11#15262, extra_col_12#15263, extra_col_13#15264, extra_col_14#15265, extra_col_15#15266, extra_col_16#15267, extra_col_17#15268, extra_col_18#15269, extra_col_19#15270, extra_col_20#15271, extra_col_21#15272, extra_col_22#15273, extra_col_23#15274, ... 29 more fields]\n      :  +- RelationV2[extra_col_0#15251, extra_col_1#15252, extra_col_2#15253, extra_col_3#15254, extra_col_4#15255, extra_col_5#15256, extra_col_6#15257, extra_col_7#15258, extra_col_8#15259, extra_col_9#15260, extra_col_10#15261, extra_col_11#15262, extra_col_12#15263, extra_col_13#15264, extra_col_14#15265, extra_col_15#15266, extra_col_16#15267, extra_col_17#15268, extra_col_18#15269, extra_col_19#15270, extra_col_20#15271, extra_col_21#15272, extra_col_22#15273, extra_col_23#15274, ... 27 more fields] demo.nyc.taxis_100000_50COLUMNS demo.nyc.taxis_100000_50COLUMNS\n      +- Project [extra_col_0#15086, true AS __row_from_source#15308]\n         +- SubqueryAlias source\n            +- SubqueryAlias sampled_ids\n               +- View (`sampled_ids`, [extra_col_0#15086])\n                  +- GlobalLimit 1000\n                     +- LocalLimit 1000\n                        +- Project [extra_col_0#15086]\n                           +- Sort [_nondeterministic#15250 ASC NULLS FIRST], true\n                              +- Project [extra_col_0#15086, rand(-736351859637484279) AS _nondeterministic#15250]\n                                 +- Deduplicate [extra_col_0#15086]\n                                    +- Project [extra_col_0#15086]\n                                       +- RelationV2[extra_col_0#15086, extra_col_1#15087, extra_col_2#15088, extra_col_3#15089, extra_col_4#15090, extra_col_5#15091, extra_col_6#15092, extra_col_7#15093, extra_col_8#15094, extra_col_9#15095, extra_col_10#15096, extra_col_11#15097, extra_col_12#15098, extra_col_13#15099, extra_col_14#15100, extra_col_15#15101, extra_col_16#15102, extra_col_17#15103, extra_col_18#15104, extra_col_19#15105, extra_col_20#15106, extra_col_21#15107, extra_col_22#15108, extra_col_23#15109, ... 26 more fields] demo.nyc.taxis_100000_50COLUMNS demo.nyc.taxis_100000_50COLUMNS\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 32\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Perform Iceberg MERGE INTO\u001b[39;00m\n\u001b[1;32m     25\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124mMERGE INTO demo.nyc.taxis_100000_50COLUMNS AS target\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124mUSING sampled_ids AS source\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124m    UPDATE SET target.extra_col_1 = target.extra_col_1 + 10\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m st\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [INVALID_NON_DETERMINISTIC_EXPRESSIONS] The operator expects a deterministic expression, but the actual expression is \"(extra_col_0 = extra_col_0)\", \"exists(extra_col_0)\".; line 2 pos 0;\nReplaceData\n:  +- Filter (outer(extra_col_0#15251) = extra_col_0#15361)\n:     +- SubqueryAlias source\n:        +- SubqueryAlias sampled_ids\n:           +- View (`sampled_ids`, [extra_col_0#15361])\n:              +- GlobalLimit 1000\n:                 +- LocalLimit 1000\n:                    +- Project [extra_col_0#15361]\n:                       +- Sort [_nondeterministic#15250 ASC NULLS FIRST], true\n:                          +- Project [extra_col_0#15361, rand(-736351859637484279) AS _nondeterministic#15250]\n:                             +- Deduplicate [extra_col_0#15361]\n:                                +- Project [extra_col_0#15361]\n:                                   +- RelationV2[extra_col_0#15361, extra_col_1#15362, extra_col_2#15363, extra_col_3#15364, extra_col_4#15365, extra_col_5#15366, extra_col_6#15367, extra_col_7#15368, extra_col_8#15369, extra_col_9#15370, extra_col_10#15371, extra_col_11#15372, extra_col_12#15373, extra_col_13#15374, extra_col_14#15375, extra_col_15#15376, extra_col_16#15377, extra_col_17#15378, extra_col_18#15379, extra_col_19#15380, extra_col_20#15381, extra_col_21#15382, extra_col_22#15383, extra_col_23#15384, ... 26 more fields] demo.nyc.taxis_100000_50COLUMNS demo.nyc.taxis_100000_50COLUMNS\n+- MergeRows[extra_col_0#15309, extra_col_1#15310, extra_col_2#15311, extra_col_3#15312, extra_col_4#15313, extra_col_5#15314, extra_col_6#15315, extra_col_7#15316, extra_col_8#15317, extra_col_9#15318, extra_col_10#15319, extra_col_11#15320, extra_col_12#15321, extra_col_13#15322, extra_col_14#15323, extra_col_15#15324, extra_col_16#15325, extra_col_17#15326, extra_col_18#15327, extra_col_19#15328, extra_col_20#15329, extra_col_21#15330, extra_col_22#15331, extra_col_23#15332, ... 27 more fields]\n   +- Join LeftOuter, (extra_col_0#15251 = extra_col_0#15086), leftHint=(strategy=no_broadcast_and_replication)\n      :- Project [extra_col_0#15251, extra_col_1#15252, extra_col_2#15253, extra_col_3#15254, extra_col_4#15255, extra_col_5#15256, extra_col_6#15257, extra_col_7#15258, extra_col_8#15259, extra_col_9#15260, extra_col_10#15261, extra_col_11#15262, extra_col_12#15263, extra_col_13#15264, extra_col_14#15265, extra_col_15#15266, extra_col_16#15267, extra_col_17#15268, extra_col_18#15269, extra_col_19#15270, extra_col_20#15271, extra_col_21#15272, extra_col_22#15273, extra_col_23#15274, ... 29 more fields]\n      :  +- RelationV2[extra_col_0#15251, extra_col_1#15252, extra_col_2#15253, extra_col_3#15254, extra_col_4#15255, extra_col_5#15256, extra_col_6#15257, extra_col_7#15258, extra_col_8#15259, extra_col_9#15260, extra_col_10#15261, extra_col_11#15262, extra_col_12#15263, extra_col_13#15264, extra_col_14#15265, extra_col_15#15266, extra_col_16#15267, extra_col_17#15268, extra_col_18#15269, extra_col_19#15270, extra_col_20#15271, extra_col_21#15272, extra_col_22#15273, extra_col_23#15274, ... 27 more fields] demo.nyc.taxis_100000_50COLUMNS demo.nyc.taxis_100000_50COLUMNS\n      +- Project [extra_col_0#15086, true AS __row_from_source#15308]\n         +- SubqueryAlias source\n            +- SubqueryAlias sampled_ids\n               +- View (`sampled_ids`, [extra_col_0#15086])\n                  +- GlobalLimit 1000\n                     +- LocalLimit 1000\n                        +- Project [extra_col_0#15086]\n                           +- Sort [_nondeterministic#15250 ASC NULLS FIRST], true\n                              +- Project [extra_col_0#15086, rand(-736351859637484279) AS _nondeterministic#15250]\n                                 +- Deduplicate [extra_col_0#15086]\n                                    +- Project [extra_col_0#15086]\n                                       +- RelationV2[extra_col_0#15086, extra_col_1#15087, extra_col_2#15088, extra_col_3#15089, extra_col_4#15090, extra_col_5#15091, extra_col_6#15092, extra_col_7#15093, extra_col_8#15094, extra_col_9#15095, extra_col_10#15096, extra_col_11#15097, extra_col_12#15098, extra_col_13#15099, extra_col_14#15100, extra_col_15#15101, extra_col_16#15102, extra_col_17#15103, extra_col_18#15104, extra_col_19#15105, extra_col_20#15106, extra_col_21#15107, extra_col_22#15108, extra_col_23#15109, ... 26 more fields] demo.nyc.taxis_100000_50COLUMNS demo.nyc.taxis_100000_50COLUMNS\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# from pyspark.sql.functions import col, rand\n",
    "\n",
    "# # Load the existing Apache Iceberg table\n",
    "# df = spark.read.format(\"iceberg\").load(\"demo.nyc.taxis_100000_50COLUMNS\")\n",
    "\n",
    "# # Get total record count\n",
    "# total_rows = df.count()\n",
    "# print(f\"Total rows in Iceberg table: {total_rows}\")\n",
    "\n",
    "# # === Get Update Percentage from User ===\n",
    "# update_percentage = float(input(\"Enter update percentage (e.g., 1 for 1%): \").strip()) / 100\n",
    "# num_rows = int(total_rows * update_percentage)\n",
    "# print(f\"Updating {num_rows} rows (~{update_percentage*100}%)...\")\n",
    "\n",
    "# # Sample unique IDs to update\n",
    "# sampled_df = df.select(\"extra_col_0\").distinct().orderBy(rand()).limit(num_rows)\n",
    "\n",
    "# st = time.time()\n",
    "\n",
    "# # Create a temporary table for sampled records\n",
    "# sampled_df.createOrReplaceTempView(\"sampled_ids\")\n",
    "\n",
    "# # Perform Iceberg MERGE INTO\n",
    "# query = \"\"\"\n",
    "# MERGE INTO demo.nyc.taxis_100000_50COLUMNS AS target\n",
    "# USING sampled_ids AS source\n",
    "# ON target.extra_col_0 = source.extra_col_0\n",
    "# WHEN MATCHED THEN \n",
    "#     UPDATE SET target.extra_col_1 = target.extra_col_1 + 10\n",
    "# \"\"\"\n",
    "# spark.sql(query)\n",
    "\n",
    "# end = time.time() - st\n",
    "# print(f\"Updated {num_rows} rows in {end:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0e415d6c-94e2-4ab3-83cb-8a8d3e9f89af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in Iceberg table: 100000\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter update percentage (e.g., 1 for 1%):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 1000 rows (~1.0%)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `IIJMJEKEKI` cannot be resolved. Did you mean one of the following? [`extra_col_0`, `extra_col_1`, `extra_col_2`, `extra_col_3`, `extra_col_4`].; line 5 pos 26;\n'MergeIntoTable ('target.extra_col_0 = 'source.extra_col_0), [updateaction(None, assignment('target.extra_col_1, ('target.extra_col_1 + 10)))]\n:- SubqueryAlias target\n:  +- SubqueryAlias demo.nyc.taxis_100000_50COLUMNS\n:     +- RelationV2[extra_col_0#14981, extra_col_1#14982, extra_col_2#14983, extra_col_3#14984, extra_col_4#14985, extra_col_5#14986, extra_col_6#14987, extra_col_7#14988, extra_col_8#14989, extra_col_9#14990, extra_col_10#14991, extra_col_11#14992, extra_col_12#14993, extra_col_13#14994, extra_col_14#14995, extra_col_15#14996, extra_col_16#14997, extra_col_17#14998, extra_col_18#14999, extra_col_19#15000, extra_col_20#15001, extra_col_21#15002, extra_col_22#15003, extra_col_23#15004, ... 26 more fields] demo.nyc.taxis_100000_50COLUMNS demo.nyc.taxis_100000_50COLUMNS\n+- 'SubqueryAlias source\n   +- 'Project ['extra_col_0]\n      +- 'Filter extra_col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n         +- SubqueryAlias demo.nyc.taxis_100000_50COLUMNS\n            +- RelationV2[extra_col_0#15031, extra_col_1#15032, extra_col_2#15033, extra_col_3#15034, extra_col_4#15035, extra_col_5#15036, extra_col_6#15037, extra_col_7#15038, extra_col_8#15039, extra_col_9#15040, extra_col_10#15041, extra_col_11#15042, extra_col_12#15043, extra_col_13#15044, extra_col_14#15045, extra_col_15#15046, extra_col_16#15047, extra_col_17#15048, extra_col_18#15049, extra_col_19#15050, extra_col_20#15051, extra_col_21#15052, extra_col_22#15053, extra_col_23#15054, ... 26 more fields] demo.nyc.taxis_100000_50COLUMNS demo.nyc.taxis_100000_50COLUMNS\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 35\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Update records using Iceberg's MERGE INTO\u001b[39;00m\n\u001b[1;32m     25\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124mMERGE INTO demo.nyc.taxis_100000_50COLUMNS AS target\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124mUSING (\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124m    UPDATE SET target.extra_col_1 = target.extra_col_1 + 10\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m st\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `IIJMJEKEKI` cannot be resolved. Did you mean one of the following? [`extra_col_0`, `extra_col_1`, `extra_col_2`, `extra_col_3`, `extra_col_4`].; line 5 pos 26;\n'MergeIntoTable ('target.extra_col_0 = 'source.extra_col_0), [updateaction(None, assignment('target.extra_col_1, ('target.extra_col_1 + 10)))]\n:- SubqueryAlias target\n:  +- SubqueryAlias demo.nyc.taxis_100000_50COLUMNS\n:     +- RelationV2[extra_col_0#14981, extra_col_1#14982, extra_col_2#14983, extra_col_3#14984, extra_col_4#14985, extra_col_5#14986, extra_col_6#14987, extra_col_7#14988, extra_col_8#14989, extra_col_9#14990, extra_col_10#14991, extra_col_11#14992, extra_col_12#14993, extra_col_13#14994, extra_col_14#14995, extra_col_15#14996, extra_col_16#14997, extra_col_17#14998, extra_col_18#14999, extra_col_19#15000, extra_col_20#15001, extra_col_21#15002, extra_col_22#15003, extra_col_23#15004, ... 26 more fields] demo.nyc.taxis_100000_50COLUMNS demo.nyc.taxis_100000_50COLUMNS\n+- 'SubqueryAlias source\n   +- 'Project ['extra_col_0]\n      +- 'Filter extra_col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n         +- SubqueryAlias demo.nyc.taxis_100000_50COLUMNS\n            +- RelationV2[extra_col_0#15031, extra_col_1#15032, extra_col_2#15033, extra_col_3#15034, extra_col_4#15035, extra_col_5#15036, extra_col_6#15037, extra_col_7#15038, extra_col_8#15039, extra_col_9#15040, extra_col_10#15041, extra_col_11#15042, extra_col_12#15043, extra_col_13#15044, extra_col_14#15045, extra_col_15#15046, extra_col_16#15047, extra_col_17#15048, extra_col_18#15049, extra_col_19#15050, extra_col_20#15051, extra_col_21#15052, extra_col_22#15053, extra_col_23#15054, ... 26 more fields] demo.nyc.taxis_100000_50COLUMNS demo.nyc.taxis_100000_50COLUMNS\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "85565bc1-fe91-4854-838e-6f92eb48a8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|      nyc|           taxis_100|      false|\n",
      "|      nyc|     taxis_100M_time|      false|\n",
      "|      nyc|taxis_1000_50COLUMNS|      false|\n",
      "|      nyc|            taxis_1B|      false|\n",
      "|      nyc|taxis_100_50COLUM...|      false|\n",
      "|      nyc|            taxis_1L|      false|\n",
      "|      nyc|          taxis_1000|      false|\n",
      "|      nyc|            taxis_10|      false|\n",
      "|      nyc| taxis_10M_50COLUMNS|      false|\n",
      "|      nyc|   taxis_1L_5COLUMNS|      false|\n",
      "|      nyc|         taxis_10000|      false|\n",
      "|      nyc|            taxis_1M|      false|\n",
      "|      nyc|          taxis_1L_5|      false|\n",
      "|      nyc|          taxis_10_M|      false|\n",
      "|      nyc|taxis_1000_50COLU...|      false|\n",
      "|      nyc|  taxis_10_50COLUMNS|      false|\n",
      "|      nyc|           taxis_10K|      false|\n",
      "|      nyc|taxis_100000_50CO...|      false|\n",
      "|      nyc|            taxis_1K|      false|\n",
      "|      nyc|           taxis_10L|      false|\n",
      "+---------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acddc1d2-00ea-473a-b22f-92c6afc924ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
