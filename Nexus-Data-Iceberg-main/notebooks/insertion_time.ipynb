{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1544bc6e-fb7c-47c3-9961-7099c4258610",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (2.2.1)\n",
      "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.9/site-packages (3.2.2)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.9/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "608420a0-392e-4d76-9a36-a1e788ebb7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"records\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b443206d-6ebe-46fd-bd8a-2934dd468585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS demo.nyc.taxis_5_100M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9595ebac-0106-4941-9239-b1f7e1a513cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "iceberg_table_dir = \"../warehouse/nyc/taxis_5_100M\"\n",
    "metadata_dir = f\"{iceberg_table_dir}/metadata\"\n",
    "data_dir = f\"{iceberg_table_dir}/data\"\n",
    "input_data_dir = f\"../input_data\"\n",
    "analysis_info = []\n",
    "records_before_op = 0\n",
    "\n",
    "def append_to_file(file_path, msg):\n",
    "    open_mode = \"a\"\n",
    "    if not os.path.exists(file_path):\n",
    "        open_mode = \"w\"\n",
    "\n",
    "    # Open the CSV file in write mode\n",
    "    with open(file_path, open_mode) as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        if open_mode==\"w\":\n",
    "            #writing header of the columns\n",
    "            writer.writerows([list(msg.keys())])    \n",
    "\n",
    "        row_values = [list(msg.values())]\n",
    "        # Write the data to the CSV file\n",
    "        writer.writerows(row_values)\n",
    "\n",
    "def get_size():\n",
    "    # List the metadata files\n",
    "    manifest_pattern = re.compile(r\".*-m\\d+\\.avro$\")\n",
    "    metadata_files = os.listdir(metadata_dir)\n",
    "    \n",
    "    # Initialize variables to store the sizes of different types of metadata files\n",
    "    snap_avro_size = 0\n",
    "    metadata_json_size = 0\n",
    "    m_avro_size = 0\n",
    "\n",
    "    data_dir_size = 0\n",
    "    # get data dir size\n",
    "    data_dir_files = os.listdir(data_dir)\n",
    "    # print(data_dir_files)\n",
    "    for filename in data_dir_files:\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        data_dir_size += os.path.getsize(file_path) / 1024  # Convert size to KB\n",
    "    \n",
    "    # Iterate through the metadata files and calculate their sizes\n",
    "    for file in metadata_files:\n",
    "        file_path = os.path.join(metadata_dir, file)\n",
    "        file_size_kb = os.path.getsize(file_path) / 1024  # Convert size to KB\n",
    "        \n",
    "        if file.startswith(\"snap-\") and file.endswith(\".avro\"):\n",
    "            snap_avro_size += file_size_kb\n",
    "        elif file.endswith(\".metadata.json\"):\n",
    "            metadata_json_size += file_size_kb\n",
    "        elif manifest_pattern.match(file):\n",
    "            m_avro_size += file_size_kb\n",
    "    \n",
    "    # Print the time taken and the sizes of the metadata files\n",
    "    # print(f\"Time taken to read Parquet files: {time_taken:.2f} seconds\")\n",
    "    # print(f\"Size of snap-*.avro files: {snap_avro_size:.2f} KB\")\n",
    "    # print(f\"Size of *.metadata.json files: {metadata_json_size:.2f} KB\")\n",
    "    # print(f\"Size of *m{0-9}{1,}.avro files: {m_avro_size:.2f} KB\")\n",
    "\n",
    "    return {\"data_dir_size\": data_dir_size,\"metadata_size\": metadata_json_size,\"snapshot_size\": snap_avro_size,\"manifest_size\": m_avro_size}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7587c21-bddb-4b48-80e6-32f2d827d5c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, FloatType, LongType, StructType,StructField, StringType\n",
    "schema = StructType([\n",
    "  StructField(\"vendor_id\", LongType(), True),\n",
    "  StructField(\"trip_id\", LongType(), True),\n",
    "  StructField(\"trip_distance\", FloatType(), True),\n",
    "  StructField(\"fare_amount\", DoubleType(), True),\n",
    "  StructField(\"store_and_fwd_flag\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame([], schema)\n",
    "df.writeTo(\"demo.nyc.taxis_5_100M\").create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e629758c-b748-4a02-bd27-662baad3ff03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------------+-----------+------------------+\n",
      "|vendor_id|trip_id|trip_distance|fare_amount|store_and_fwd_flag|\n",
      "+---------+-------+-------------+-----------+------------------+\n",
      "+---------+-------+-------------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"demo.nyc.taxis_5_100M\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c390ba-cc7b-401a-897a-a71e2dfca2a1",
   "metadata": {},
   "source": [
    "## Perform operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e93147e-754e-4a23-92dc-3cf4de808594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter input file type csv or parquet? :  parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started with file=records_1000000_part_100_1740479562.9411988.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 0.5, 'metadata_size': 1.0, 'snapshot_size': 1.0, 'manifest_size': 0.5, 'time_taken': '2.49 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 1000000, 'insertion_start_time': '2025-02-25 10:34:46', 'insertion_end_time': '2025-02-25 10:34:49', 'total_insertion_time': '2.49 sec'}\n",
      "Inserted 1000000 records..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started with file=records_1000000_part_10_1740479017.2732255.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 6.0, 'metadata_size': 6.5, 'snapshot_size': 6.5, 'manifest_size': 11.0, 'time_taken': '1.77 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 2000000, 'insertion_start_time': '2025-02-25 10:35:11', 'insertion_end_time': '2025-02-25 10:35:12', 'total_insertion_time': '1.77 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_11_1740479024.0898237.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 6.5, 'metadata_size': 7.0, 'snapshot_size': 7.0, 'manifest_size': 11.5, 'time_taken': '1.71 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 3000000, 'insertion_start_time': '2025-02-25 10:35:12', 'insertion_end_time': '2025-02-25 10:35:14', 'total_insertion_time': '1.71 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_12_1740479030.8139675.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 7.0, 'metadata_size': 7.5, 'snapshot_size': 7.5, 'manifest_size': 12.0, 'time_taken': '1.98 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 4000000, 'insertion_start_time': '2025-02-25 10:35:14', 'insertion_end_time': '2025-02-25 10:35:16', 'total_insertion_time': '1.98 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_13_1740479037.5945997.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 7.5, 'metadata_size': 8.0, 'snapshot_size': 8.0, 'manifest_size': 12.5, 'time_taken': '1.86 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 5000000, 'insertion_start_time': '2025-02-25 10:35:16', 'insertion_end_time': '2025-02-25 10:35:18', 'total_insertion_time': '1.86 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_14_1740479044.2930903.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 8.0, 'metadata_size': 8.5, 'snapshot_size': 8.5, 'manifest_size': 13.0, 'time_taken': '1.70 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 6000000, 'insertion_start_time': '2025-02-25 10:35:18', 'insertion_end_time': '2025-02-25 10:35:20', 'total_insertion_time': '1.70 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_15_1740479050.5188167.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 8.5, 'metadata_size': 9.0, 'snapshot_size': 9.0, 'manifest_size': 13.5, 'time_taken': '1.95 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 7000000, 'insertion_start_time': '2025-02-25 10:35:20', 'insertion_end_time': '2025-02-25 10:35:22', 'total_insertion_time': '1.95 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_16_1740479056.7795022.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 9.0, 'metadata_size': 9.5, 'snapshot_size': 9.5, 'manifest_size': 14.0, 'time_taken': '1.74 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 8000000, 'insertion_start_time': '2025-02-25 10:35:22', 'insertion_end_time': '2025-02-25 10:35:24', 'total_insertion_time': '1.74 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_17_1740479063.0324295.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 9.5, 'metadata_size': 10.0, 'snapshot_size': 10.0, 'manifest_size': 14.5, 'time_taken': '1.81 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 9000000, 'insertion_start_time': '2025-02-25 10:35:24', 'insertion_end_time': '2025-02-25 10:35:26', 'total_insertion_time': '1.81 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_18_1740479069.140782.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 10.0, 'metadata_size': 10.5, 'snapshot_size': 10.5, 'manifest_size': 15.0, 'time_taken': '1.82 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 10000000, 'insertion_start_time': '2025-02-25 10:35:26', 'insertion_end_time': '2025-02-25 10:35:28', 'total_insertion_time': '1.82 sec'}\n",
      "Inserted 1000000 records..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started with file=records_1000000_part_19_1740479075.2074947.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 60.5, 'metadata_size': 16.0, 'snapshot_size': 16.0, 'manifest_size': 30.0, 'time_taken': '1.98 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 11000000, 'insertion_start_time': '2025-02-25 10:37:02', 'insertion_end_time': '2025-02-25 10:37:04', 'total_insertion_time': '1.98 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_1_1740478958.676483.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 61.0, 'metadata_size': 16.5, 'snapshot_size': 16.5, 'manifest_size': 30.5, 'time_taken': '1.77 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 12000000, 'insertion_start_time': '2025-02-25 10:37:04', 'insertion_end_time': '2025-02-25 10:37:06', 'total_insertion_time': '1.77 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_20_1740479081.4518008.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 61.5, 'metadata_size': 17.0, 'snapshot_size': 17.0, 'manifest_size': 31.0, 'time_taken': '1.91 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 13000000, 'insertion_start_time': '2025-02-25 10:37:06', 'insertion_end_time': '2025-02-25 10:37:08', 'total_insertion_time': '1.91 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_21_1740479087.641574.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 62.0, 'metadata_size': 17.5, 'snapshot_size': 17.5, 'manifest_size': 31.5, 'time_taken': '1.74 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 14000000, 'insertion_start_time': '2025-02-25 10:37:09', 'insertion_end_time': '2025-02-25 10:37:10', 'total_insertion_time': '1.74 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_22_1740479093.7348895.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 62.5, 'metadata_size': 18.0, 'snapshot_size': 18.0, 'manifest_size': 32.0, 'time_taken': '1.73 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 15000000, 'insertion_start_time': '2025-02-25 10:37:11', 'insertion_end_time': '2025-02-25 10:37:12', 'total_insertion_time': '1.73 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_23_1740479099.6942494.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 63.0, 'metadata_size': 18.5, 'snapshot_size': 18.5, 'manifest_size': 32.5, 'time_taken': '1.67 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 16000000, 'insertion_start_time': '2025-02-25 10:37:13', 'insertion_end_time': '2025-02-25 10:37:15', 'total_insertion_time': '1.67 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_24_1740479105.6955447.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 63.5, 'metadata_size': 19.0, 'snapshot_size': 19.0, 'manifest_size': 33.0, 'time_taken': '1.75 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 17000000, 'insertion_start_time': '2025-02-25 10:37:15', 'insertion_end_time': '2025-02-25 10:37:17', 'total_insertion_time': '1.75 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_25_1740479111.7442427.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 64.0, 'metadata_size': 19.5, 'snapshot_size': 19.5, 'manifest_size': 33.5, 'time_taken': '1.81 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 18000000, 'insertion_start_time': '2025-02-25 10:37:17', 'insertion_end_time': '2025-02-25 10:37:19', 'total_insertion_time': '1.81 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_26_1740479117.7690225.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 64.5, 'metadata_size': 20.0, 'snapshot_size': 20.0, 'manifest_size': 34.0, 'time_taken': '1.77 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 19000000, 'insertion_start_time': '2025-02-25 10:37:19', 'insertion_end_time': '2025-02-25 10:37:21', 'total_insertion_time': '1.77 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_27_1740479123.752309.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 65.0, 'metadata_size': 20.5, 'snapshot_size': 20.5, 'manifest_size': 34.5, 'time_taken': '1.75 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 20000000, 'insertion_start_time': '2025-02-25 10:37:21', 'insertion_end_time': '2025-02-25 10:37:23', 'total_insertion_time': '1.75 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_28_1740479129.7383063.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 65.5, 'metadata_size': 21.0, 'snapshot_size': 21.0, 'manifest_size': 35.0, 'time_taken': '1.69 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 21000000, 'insertion_start_time': '2025-02-25 10:37:24', 'insertion_end_time': '2025-02-25 10:37:25', 'total_insertion_time': '1.69 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_29_1740479135.6430395.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 66.0, 'metadata_size': 21.5, 'snapshot_size': 21.5, 'manifest_size': 35.5, 'time_taken': '1.90 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 22000000, 'insertion_start_time': '2025-02-25 10:37:26', 'insertion_end_time': '2025-02-25 10:37:28', 'total_insertion_time': '1.90 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_2_1740478965.0064826.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 66.5, 'metadata_size': 22.0, 'snapshot_size': 22.0, 'manifest_size': 36.0, 'time_taken': '2.72 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 23000000, 'insertion_start_time': '2025-02-25 10:37:28', 'insertion_end_time': '2025-02-25 10:37:31', 'total_insertion_time': '2.72 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_30_1740479141.7463803.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 67.0, 'metadata_size': 22.5, 'snapshot_size': 22.5, 'manifest_size': 36.5, 'time_taken': '1.77 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 24000000, 'insertion_start_time': '2025-02-25 10:37:31', 'insertion_end_time': '2025-02-25 10:37:33', 'total_insertion_time': '1.77 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_31_1740479147.6707687.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 67.5, 'metadata_size': 23.0, 'snapshot_size': 23.0, 'manifest_size': 37.0, 'time_taken': '1.75 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 25000000, 'insertion_start_time': '2025-02-25 10:37:33', 'insertion_end_time': '2025-02-25 10:37:35', 'total_insertion_time': '1.75 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_32_1740479153.630442.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 68.0, 'metadata_size': 23.5, 'snapshot_size': 23.5, 'manifest_size': 37.5, 'time_taken': '1.75 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 26000000, 'insertion_start_time': '2025-02-25 10:37:36', 'insertion_end_time': '2025-02-25 10:37:37', 'total_insertion_time': '1.75 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_33_1740479159.7014382.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 68.5, 'metadata_size': 24.0, 'snapshot_size': 24.0, 'manifest_size': 38.0, 'time_taken': '1.91 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 27000000, 'insertion_start_time': '2025-02-25 10:37:38', 'insertion_end_time': '2025-02-25 10:37:40', 'total_insertion_time': '1.91 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_34_1740479165.6889257.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 69.0, 'metadata_size': 24.5, 'snapshot_size': 24.5, 'manifest_size': 38.5, 'time_taken': '1.81 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 28000000, 'insertion_start_time': '2025-02-25 10:37:40', 'insertion_end_time': '2025-02-25 10:37:42', 'total_insertion_time': '1.81 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_35_1740479171.6570647.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 69.5, 'metadata_size': 25.0, 'snapshot_size': 25.0, 'manifest_size': 39.0, 'time_taken': '1.77 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 29000000, 'insertion_start_time': '2025-02-25 10:37:42', 'insertion_end_time': '2025-02-25 10:37:44', 'total_insertion_time': '1.77 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_36_1740479177.6566036.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 70.0, 'metadata_size': 25.5, 'snapshot_size': 25.5, 'manifest_size': 39.5, 'time_taken': '3.21 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 30000000, 'insertion_start_time': '2025-02-25 10:37:45', 'insertion_end_time': '2025-02-25 10:37:48', 'total_insertion_time': '3.21 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_37_1740479183.6467981.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 70.5, 'metadata_size': 26.0, 'snapshot_size': 26.0, 'manifest_size': 40.0, 'time_taken': '1.83 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 31000000, 'insertion_start_time': '2025-02-25 10:37:48', 'insertion_end_time': '2025-02-25 10:37:50', 'total_insertion_time': '1.83 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_38_1740479189.635899.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 71.0, 'metadata_size': 26.5, 'snapshot_size': 26.5, 'manifest_size': 40.5, 'time_taken': '1.75 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 32000000, 'insertion_start_time': '2025-02-25 10:37:51', 'insertion_end_time': '2025-02-25 10:37:52', 'total_insertion_time': '1.75 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_39_1740479195.6901538.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 71.5, 'metadata_size': 27.0, 'snapshot_size': 27.0, 'manifest_size': 41.0, 'time_taken': '1.67 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 33000000, 'insertion_start_time': '2025-02-25 10:37:53', 'insertion_end_time': '2025-02-25 10:37:55', 'total_insertion_time': '1.67 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_3_1740478971.4056692.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 72.0, 'metadata_size': 27.5, 'snapshot_size': 27.5, 'manifest_size': 41.5, 'time_taken': '2.12 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 34000000, 'insertion_start_time': '2025-02-25 10:37:55', 'insertion_end_time': '2025-02-25 10:37:57', 'total_insertion_time': '2.12 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_40_1740479201.7212226.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 72.5, 'metadata_size': 28.0, 'snapshot_size': 28.0, 'manifest_size': 42.0, 'time_taken': '1.55 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 35000000, 'insertion_start_time': '2025-02-25 10:37:58', 'insertion_end_time': '2025-02-25 10:37:59', 'total_insertion_time': '1.55 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_41_1740479207.6827612.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 73.0, 'metadata_size': 28.5, 'snapshot_size': 28.5, 'manifest_size': 42.5, 'time_taken': '1.71 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 36000000, 'insertion_start_time': '2025-02-25 10:38:00', 'insertion_end_time': '2025-02-25 10:38:02', 'total_insertion_time': '1.71 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_42_1740479213.6259525.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 73.5, 'metadata_size': 29.0, 'snapshot_size': 29.0, 'manifest_size': 43.0, 'time_taken': '1.65 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 37000000, 'insertion_start_time': '2025-02-25 10:38:02', 'insertion_end_time': '2025-02-25 10:38:04', 'total_insertion_time': '1.65 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_43_1740479219.5911868.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 74.0, 'metadata_size': 29.5, 'snapshot_size': 29.5, 'manifest_size': 43.5, 'time_taken': '1.59 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 38000000, 'insertion_start_time': '2025-02-25 10:38:04', 'insertion_end_time': '2025-02-25 10:38:06', 'total_insertion_time': '1.59 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_44_1740479225.5836053.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 74.5, 'metadata_size': 30.0, 'snapshot_size': 30.0, 'manifest_size': 44.0, 'time_taken': '1.83 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 39000000, 'insertion_start_time': '2025-02-25 10:38:07', 'insertion_end_time': '2025-02-25 10:38:08', 'total_insertion_time': '1.83 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_45_1740479231.5547447.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 75.0, 'metadata_size': 30.5, 'snapshot_size': 30.5, 'manifest_size': 44.5, 'time_taken': '1.97 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 40000000, 'insertion_start_time': '2025-02-25 10:38:09', 'insertion_end_time': '2025-02-25 10:38:11', 'total_insertion_time': '1.97 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_46_1740479237.8844259.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 75.5, 'metadata_size': 31.0, 'snapshot_size': 31.0, 'manifest_size': 45.0, 'time_taken': '1.86 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 41000000, 'insertion_start_time': '2025-02-25 10:38:12', 'insertion_end_time': '2025-02-25 10:38:13', 'total_insertion_time': '1.86 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_47_1740479244.9865994.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 76.0, 'metadata_size': 31.5, 'snapshot_size': 31.5, 'manifest_size': 45.5, 'time_taken': '1.76 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 42000000, 'insertion_start_time': '2025-02-25 10:38:14', 'insertion_end_time': '2025-02-25 10:38:16', 'total_insertion_time': '1.76 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_48_1740479251.771145.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 76.5, 'metadata_size': 32.0, 'snapshot_size': 32.0, 'manifest_size': 46.0, 'time_taken': '2.05 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 43000000, 'insertion_start_time': '2025-02-25 10:38:16', 'insertion_end_time': '2025-02-25 10:38:18', 'total_insertion_time': '2.05 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_49_1740479257.7559133.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 77.0, 'metadata_size': 32.5, 'snapshot_size': 32.5, 'manifest_size': 46.5, 'time_taken': '2.14 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 44000000, 'insertion_start_time': '2025-02-25 10:38:19', 'insertion_end_time': '2025-02-25 10:38:21', 'total_insertion_time': '2.14 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_4_1740478977.8232985.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 77.5, 'metadata_size': 33.0, 'snapshot_size': 33.0, 'manifest_size': 47.0, 'time_taken': '1.89 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 45000000, 'insertion_start_time': '2025-02-25 10:38:22', 'insertion_end_time': '2025-02-25 10:38:24', 'total_insertion_time': '1.89 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_50_1740479263.7538955.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 78.0, 'metadata_size': 33.5, 'snapshot_size': 33.5, 'manifest_size': 47.5, 'time_taken': '1.77 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 46000000, 'insertion_start_time': '2025-02-25 10:38:24', 'insertion_end_time': '2025-02-25 10:38:26', 'total_insertion_time': '1.77 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_51_1740479269.7061412.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 78.5, 'metadata_size': 34.0, 'snapshot_size': 34.0, 'manifest_size': 48.0, 'time_taken': '2.00 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 47000000, 'insertion_start_time': '2025-02-25 10:38:27', 'insertion_end_time': '2025-02-25 10:38:29', 'total_insertion_time': '2.00 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_52_1740479275.6108172.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 79.0, 'metadata_size': 34.5, 'snapshot_size': 34.5, 'manifest_size': 48.5, 'time_taken': '1.90 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 48000000, 'insertion_start_time': '2025-02-25 10:38:29', 'insertion_end_time': '2025-02-25 10:38:31', 'total_insertion_time': '1.90 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_53_1740479281.611671.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 79.5, 'metadata_size': 35.0, 'snapshot_size': 35.0, 'manifest_size': 49.0, 'time_taken': '1.86 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 49000000, 'insertion_start_time': '2025-02-25 10:38:32', 'insertion_end_time': '2025-02-25 10:38:34', 'total_insertion_time': '1.86 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_54_1740479287.6296926.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 80.0, 'metadata_size': 35.5, 'snapshot_size': 35.5, 'manifest_size': 49.5, 'time_taken': '1.78 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 50000000, 'insertion_start_time': '2025-02-25 10:38:34', 'insertion_end_time': '2025-02-25 10:38:36', 'total_insertion_time': '1.78 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_55_1740479293.5684905.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 80.5, 'metadata_size': 36.0, 'snapshot_size': 36.0, 'manifest_size': 50.0, 'time_taken': '1.78 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 51000000, 'insertion_start_time': '2025-02-25 10:38:37', 'insertion_end_time': '2025-02-25 10:38:39', 'total_insertion_time': '1.78 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_56_1740479299.7135344.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 81.0, 'metadata_size': 36.5, 'snapshot_size': 36.5, 'manifest_size': 50.5, 'time_taken': '1.78 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 52000000, 'insertion_start_time': '2025-02-25 10:38:39', 'insertion_end_time': '2025-02-25 10:38:41', 'total_insertion_time': '1.78 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_57_1740479305.6371973.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 81.5, 'metadata_size': 37.0, 'snapshot_size': 37.0, 'manifest_size': 51.0, 'time_taken': '1.73 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 53000000, 'insertion_start_time': '2025-02-25 10:38:42', 'insertion_end_time': '2025-02-25 10:38:43', 'total_insertion_time': '1.73 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_58_1740479311.6046054.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 82.0, 'metadata_size': 37.5, 'snapshot_size': 37.5, 'manifest_size': 51.5, 'time_taken': '1.78 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 54000000, 'insertion_start_time': '2025-02-25 10:38:44', 'insertion_end_time': '2025-02-25 10:38:46', 'total_insertion_time': '1.78 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_59_1740479317.5237834.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 82.5, 'metadata_size': 38.0, 'snapshot_size': 38.0, 'manifest_size': 52.0, 'time_taken': '1.87 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 55000000, 'insertion_start_time': '2025-02-25 10:38:47', 'insertion_end_time': '2025-02-25 10:38:48', 'total_insertion_time': '1.87 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_5_1740478983.9570854.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 83.0, 'metadata_size': 38.5, 'snapshot_size': 38.5, 'manifest_size': 52.5, 'time_taken': '2.02 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 56000000, 'insertion_start_time': '2025-02-25 10:38:49', 'insertion_end_time': '2025-02-25 10:38:51', 'total_insertion_time': '2.02 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_60_1740479323.544201.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 83.5, 'metadata_size': 39.0, 'snapshot_size': 39.0, 'manifest_size': 53.0, 'time_taken': '2.14 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 57000000, 'insertion_start_time': '2025-02-25 10:38:52', 'insertion_end_time': '2025-02-25 10:38:54', 'total_insertion_time': '2.14 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_61_1740479329.472506.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 84.0, 'metadata_size': 39.5, 'snapshot_size': 39.5, 'manifest_size': 53.5, 'time_taken': '1.77 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 58000000, 'insertion_start_time': '2025-02-25 10:38:55', 'insertion_end_time': '2025-02-25 10:38:56', 'total_insertion_time': '1.77 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_62_1740479335.399298.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 84.5, 'metadata_size': 40.0, 'snapshot_size': 40.0, 'manifest_size': 54.0, 'time_taken': '1.80 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 59000000, 'insertion_start_time': '2025-02-25 10:38:57', 'insertion_end_time': '2025-02-25 10:38:59', 'total_insertion_time': '1.80 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_63_1740479341.4143083.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 85.0, 'metadata_size': 40.5, 'snapshot_size': 40.5, 'manifest_size': 54.5, 'time_taken': '1.78 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 60000000, 'insertion_start_time': '2025-02-25 10:39:00', 'insertion_end_time': '2025-02-25 10:39:01', 'total_insertion_time': '1.78 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_64_1740479347.381167.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 85.5, 'metadata_size': 41.0, 'snapshot_size': 41.0, 'manifest_size': 55.0, 'time_taken': '1.77 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 61000000, 'insertion_start_time': '2025-02-25 10:39:02', 'insertion_end_time': '2025-02-25 10:39:04', 'total_insertion_time': '1.77 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_65_1740479353.325199.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 86.0, 'metadata_size': 41.5, 'snapshot_size': 41.5, 'manifest_size': 55.5, 'time_taken': '1.89 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 62000000, 'insertion_start_time': '2025-02-25 10:39:05', 'insertion_end_time': '2025-02-25 10:39:06', 'total_insertion_time': '1.89 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_66_1740479359.3133848.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 86.5, 'metadata_size': 42.0, 'snapshot_size': 42.0, 'manifest_size': 56.0, 'time_taken': '1.78 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 63000000, 'insertion_start_time': '2025-02-25 10:39:07', 'insertion_end_time': '2025-02-25 10:39:09', 'total_insertion_time': '1.78 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_67_1740479365.3033757.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 87.0, 'metadata_size': 42.5, 'snapshot_size': 42.5, 'manifest_size': 56.5, 'time_taken': '1.69 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 64000000, 'insertion_start_time': '2025-02-25 10:39:10', 'insertion_end_time': '2025-02-25 10:39:11', 'total_insertion_time': '1.69 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_68_1740479371.2812757.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 87.5, 'metadata_size': 43.0, 'snapshot_size': 43.0, 'manifest_size': 57.0, 'time_taken': '2.03 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 65000000, 'insertion_start_time': '2025-02-25 10:39:12', 'insertion_end_time': '2025-02-25 10:39:14', 'total_insertion_time': '2.03 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_69_1740479377.252577.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 88.0, 'metadata_size': 43.5, 'snapshot_size': 43.5, 'manifest_size': 57.5, 'time_taken': '2.14 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 66000000, 'insertion_start_time': '2025-02-25 10:39:15', 'insertion_end_time': '2025-02-25 10:39:17', 'total_insertion_time': '2.14 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_6_1740478990.108612.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 88.5, 'metadata_size': 44.0, 'snapshot_size': 44.0, 'manifest_size': 58.0, 'time_taken': '2.26 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 67000000, 'insertion_start_time': '2025-02-25 10:39:18', 'insertion_end_time': '2025-02-25 10:39:20', 'total_insertion_time': '2.26 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_70_1740479383.2026403.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 89.0, 'metadata_size': 44.5, 'snapshot_size': 44.5, 'manifest_size': 58.5, 'time_taken': '1.62 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 68000000, 'insertion_start_time': '2025-02-25 10:39:21', 'insertion_end_time': '2025-02-25 10:39:22', 'total_insertion_time': '1.62 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_71_1740479389.1730509.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 89.5, 'metadata_size': 45.0, 'snapshot_size': 45.0, 'manifest_size': 59.0, 'time_taken': '1.88 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 69000000, 'insertion_start_time': '2025-02-25 10:39:23', 'insertion_end_time': '2025-02-25 10:39:25', 'total_insertion_time': '1.88 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_72_1740479395.1066487.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 90.0, 'metadata_size': 45.5, 'snapshot_size': 45.5, 'manifest_size': 59.5, 'time_taken': '1.80 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 70000000, 'insertion_start_time': '2025-02-25 10:39:26', 'insertion_end_time': '2025-02-25 10:39:27', 'total_insertion_time': '1.80 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_73_1740479401.0903814.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 90.5, 'metadata_size': 46.0, 'snapshot_size': 46.0, 'manifest_size': 60.0, 'time_taken': '1.86 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 71000000, 'insertion_start_time': '2025-02-25 10:39:28', 'insertion_end_time': '2025-02-25 10:39:30', 'total_insertion_time': '1.86 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_74_1740479407.1024837.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 91.0, 'metadata_size': 46.5, 'snapshot_size': 46.5, 'manifest_size': 60.5, 'time_taken': '1.97 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 72000000, 'insertion_start_time': '2025-02-25 10:39:31', 'insertion_end_time': '2025-02-25 10:39:33', 'total_insertion_time': '1.97 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_75_1740479413.0137076.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 91.5, 'metadata_size': 47.0, 'snapshot_size': 47.0, 'manifest_size': 61.0, 'time_taken': '1.83 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 73000000, 'insertion_start_time': '2025-02-25 10:39:33', 'insertion_end_time': '2025-02-25 10:39:35', 'total_insertion_time': '1.83 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_76_1740479419.3097925.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 92.0, 'metadata_size': 47.5, 'snapshot_size': 47.5, 'manifest_size': 61.5, 'time_taken': '1.82 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 74000000, 'insertion_start_time': '2025-02-25 10:39:36', 'insertion_end_time': '2025-02-25 10:39:38', 'total_insertion_time': '1.82 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_77_1740479425.2705803.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 92.5, 'metadata_size': 48.0, 'snapshot_size': 48.0, 'manifest_size': 62.0, 'time_taken': '1.75 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 75000000, 'insertion_start_time': '2025-02-25 10:39:38', 'insertion_end_time': '2025-02-25 10:39:40', 'total_insertion_time': '1.75 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_78_1740479431.3048425.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 93.0, 'metadata_size': 48.5, 'snapshot_size': 48.5, 'manifest_size': 62.5, 'time_taken': '1.79 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 76000000, 'insertion_start_time': '2025-02-25 10:39:41', 'insertion_end_time': '2025-02-25 10:39:43', 'total_insertion_time': '1.79 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_79_1740479437.2539167.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 93.5, 'metadata_size': 49.0, 'snapshot_size': 49.0, 'manifest_size': 63.0, 'time_taken': '1.74 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 77000000, 'insertion_start_time': '2025-02-25 10:39:43', 'insertion_end_time': '2025-02-25 10:39:45', 'total_insertion_time': '1.74 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_7_1740478996.495656.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 94.0, 'metadata_size': 49.5, 'snapshot_size': 49.5, 'manifest_size': 63.5, 'time_taken': '4.01 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 78000000, 'insertion_start_time': '2025-02-25 10:39:46', 'insertion_end_time': '2025-02-25 10:39:50', 'total_insertion_time': '4.01 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_80_1740479443.1998882.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 94.5, 'metadata_size': 50.0, 'snapshot_size': 50.0, 'manifest_size': 64.0, 'time_taken': '1.73 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 79000000, 'insertion_start_time': '2025-02-25 10:39:51', 'insertion_end_time': '2025-02-25 10:39:52', 'total_insertion_time': '1.73 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_81_1740479449.116602.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 95.0, 'metadata_size': 50.5, 'snapshot_size': 50.5, 'manifest_size': 64.5, 'time_taken': '2.63 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 80000000, 'insertion_start_time': '2025-02-25 10:39:53', 'insertion_end_time': '2025-02-25 10:39:56', 'total_insertion_time': '2.63 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_82_1740479455.0647976.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 95.5, 'metadata_size': 51.0, 'snapshot_size': 51.0, 'manifest_size': 65.0, 'time_taken': '1.81 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 81000000, 'insertion_start_time': '2025-02-25 10:39:56', 'insertion_end_time': '2025-02-25 10:39:58', 'total_insertion_time': '1.81 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_83_1740479461.0464404.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 96.0, 'metadata_size': 51.5, 'snapshot_size': 51.5, 'manifest_size': 65.5, 'time_taken': '1.95 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 82000000, 'insertion_start_time': '2025-02-25 10:39:59', 'insertion_end_time': '2025-02-25 10:40:01', 'total_insertion_time': '1.95 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_84_1740479466.998438.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 96.5, 'metadata_size': 52.0, 'snapshot_size': 52.0, 'manifest_size': 66.0, 'time_taken': '2.14 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 83000000, 'insertion_start_time': '2025-02-25 10:40:02', 'insertion_end_time': '2025-02-25 10:40:04', 'total_insertion_time': '2.14 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_85_1740479472.9264255.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 97.0, 'metadata_size': 52.5, 'snapshot_size': 52.5, 'manifest_size': 66.5, 'time_taken': '1.76 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 84000000, 'insertion_start_time': '2025-02-25 10:40:05', 'insertion_end_time': '2025-02-25 10:40:07', 'total_insertion_time': '1.76 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_86_1740479478.9221923.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 97.5, 'metadata_size': 53.0, 'snapshot_size': 53.0, 'manifest_size': 67.0, 'time_taken': '1.67 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 85000000, 'insertion_start_time': '2025-02-25 10:40:08', 'insertion_end_time': '2025-02-25 10:40:09', 'total_insertion_time': '1.67 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_87_1740479484.8694992.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 98.0, 'metadata_size': 53.5, 'snapshot_size': 53.5, 'manifest_size': 67.5, 'time_taken': '1.71 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 86000000, 'insertion_start_time': '2025-02-25 10:40:10', 'insertion_end_time': '2025-02-25 10:40:12', 'total_insertion_time': '1.71 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_88_1740479491.0889428.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 98.5, 'metadata_size': 54.0, 'snapshot_size': 54.0, 'manifest_size': 68.0, 'time_taken': '1.70 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 87000000, 'insertion_start_time': '2025-02-25 10:40:13', 'insertion_end_time': '2025-02-25 10:40:14', 'total_insertion_time': '1.70 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_89_1740479497.0018194.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 99.0, 'metadata_size': 54.5, 'snapshot_size': 54.5, 'manifest_size': 68.5, 'time_taken': '1.97 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 88000000, 'insertion_start_time': '2025-02-25 10:40:15', 'insertion_end_time': '2025-02-25 10:40:17', 'total_insertion_time': '1.97 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_8_1740479003.5467188.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 99.5, 'metadata_size': 55.0, 'snapshot_size': 55.0, 'manifest_size': 69.0, 'time_taken': '1.78 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 89000000, 'insertion_start_time': '2025-02-25 10:40:18', 'insertion_end_time': '2025-02-25 10:40:20', 'total_insertion_time': '1.78 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_90_1740479503.047581.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 100.0, 'metadata_size': 55.5, 'snapshot_size': 55.5, 'manifest_size': 69.5, 'time_taken': '1.77 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 90000000, 'insertion_start_time': '2025-02-25 10:40:21', 'insertion_end_time': '2025-02-25 10:40:22', 'total_insertion_time': '1.77 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_91_1740479509.0142138.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 100.5, 'metadata_size': 56.0, 'snapshot_size': 56.0, 'manifest_size': 70.0, 'time_taken': '1.56 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 91000000, 'insertion_start_time': '2025-02-25 10:40:23', 'insertion_end_time': '2025-02-25 10:40:25', 'total_insertion_time': '1.56 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_92_1740479515.0333776.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 101.0, 'metadata_size': 56.5, 'snapshot_size': 56.5, 'manifest_size': 70.5, 'time_taken': '1.70 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 92000000, 'insertion_start_time': '2025-02-25 10:40:26', 'insertion_end_time': '2025-02-25 10:40:27', 'total_insertion_time': '1.70 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_93_1740479521.0087852.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 101.5, 'metadata_size': 57.0, 'snapshot_size': 57.0, 'manifest_size': 71.0, 'time_taken': '1.59 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 93000000, 'insertion_start_time': '2025-02-25 10:40:28', 'insertion_end_time': '2025-02-25 10:40:30', 'total_insertion_time': '1.59 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_94_1740479526.9835107.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 102.0, 'metadata_size': 57.5, 'snapshot_size': 57.5, 'manifest_size': 71.5, 'time_taken': '1.79 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 94000000, 'insertion_start_time': '2025-02-25 10:40:31', 'insertion_end_time': '2025-02-25 10:40:33', 'total_insertion_time': '1.79 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_95_1740479532.9442458.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 102.5, 'metadata_size': 58.0, 'snapshot_size': 58.0, 'manifest_size': 72.0, 'time_taken': '1.77 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 95000000, 'insertion_start_time': '2025-02-25 10:40:33', 'insertion_end_time': '2025-02-25 10:40:35', 'total_insertion_time': '1.77 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_96_1740479538.9636757.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 103.0, 'metadata_size': 58.5, 'snapshot_size': 58.5, 'manifest_size': 72.5, 'time_taken': '1.79 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 96000000, 'insertion_start_time': '2025-02-25 10:40:36', 'insertion_end_time': '2025-02-25 10:40:38', 'total_insertion_time': '1.79 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_97_1740479544.9426436.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 103.5, 'metadata_size': 59.0, 'snapshot_size': 59.0, 'manifest_size': 73.0, 'time_taken': '1.82 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 97000000, 'insertion_start_time': '2025-02-25 10:40:39', 'insertion_end_time': '2025-02-25 10:40:41', 'total_insertion_time': '1.82 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_98_1740479550.9369109.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 104.0, 'metadata_size': 59.5, 'snapshot_size': 59.5, 'manifest_size': 73.5, 'time_taken': '2.05 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 98000000, 'insertion_start_time': '2025-02-25 10:40:42', 'insertion_end_time': '2025-02-25 10:40:44', 'total_insertion_time': '2.05 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_99_1740479556.937529.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 104.5, 'metadata_size': 60.0, 'snapshot_size': 60.0, 'manifest_size': 74.0, 'time_taken': '1.63 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 99000000, 'insertion_start_time': '2025-02-25 10:40:45', 'insertion_end_time': '2025-02-25 10:40:46', 'total_insertion_time': '1.63 sec'}\n",
      "Inserted 1000000 records..\n",
      "Started with file=records_1000000_part_9_1740479010.487397.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details before appending to CSV: {'data_dir_size': 105.0, 'metadata_size': 60.5, 'snapshot_size': 60.5, 'manifest_size': 74.5, 'time_taken': '1.63 sec', 'Operation': 'Inserted 1000000 records', 'records_after_op': 100000000, 'insertion_start_time': '2025-02-25 10:40:47', 'insertion_end_time': '2025-02-25 10:40:49', 'total_insertion_time': '1.63 sec'}\n",
      "Inserted 1000000 records..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time, csv\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "input_data_dir = f\"../input_data\"\n",
    "output_dir = f\"../output\"\n",
    "analysis_info = []\n",
    "records_before_op = 0\n",
    "    \n",
    "file_type = input(\"Enter input file type csv or parquet? : \")\n",
    "file_type = file_type.lower().strip()\n",
    "input_data_dir = os.path.join(input_data_dir, file_type)\n",
    "input_files = os.listdir(input_data_dir)\n",
    "\n",
    "analysis_file = os.path.join(output_dir, f\"analysis_info_{file_type}.csv\")\n",
    "if os.path.exists(analysis_file):\n",
    "    os.remove(analysis_file)\n",
    "\n",
    "df = spark.table(\"demo.nyc.taxis_5_100M\")\n",
    "records_before_op = df.count()\n",
    "\n",
    "digits = len(str(records_before_op))\n",
    "\n",
    "# Track overall start time\n",
    "for file in input_files:\n",
    "    print(f\"Started with file={file}\")\n",
    "    file_path = os.path.join(input_data_dir, file)\n",
    "\n",
    "    # Record the start time for insertion\n",
    "    insertion_start_time = time.time()\n",
    "    \n",
    "    if file_type == \"parquet\":\n",
    "        # Read the Parquet files into a DataFrame\n",
    "        df = spark.read.parquet(file_path)\n",
    "    else:\n",
    "        df = spark.read.csv(file_path, header=True)\n",
    "        df = df.select(\n",
    "            F.col(\"vendor_id\").cast(\"long\").alias(\"vendor_id\"),\n",
    "            F.col(\"trip_id\").cast(\"long\").alias(\"trip_id\"),\n",
    "            F.col(\"trip_distance\").cast(\"float\").alias(\"trip_distance\"),\n",
    "            F.col(\"fare_amount\").cast(\"double\").alias(\"fare_amount\"),\n",
    "            F.col(\"store_and_fwd_flag\").cast(\"string\").alias(\"store_and_fwd_flag\")\n",
    "        )\n",
    "    \n",
    "    rows = df.count()\n",
    "    \n",
    "    # Write the DataFrame to Parquet format (insert operation)\n",
    "    df.writeTo(\"demo.nyc.taxis_5_100M\").append()\n",
    "    \n",
    "    # Record the end time for insertion\n",
    "    insertion_end_time = time.time()\n",
    "    \n",
    "    # Calculate total insertion time\n",
    "    insertion_time_taken = insertion_end_time - insertion_start_time\n",
    "\n",
    "    # Get details\n",
    "    details = get_size()\n",
    "    details[\"time_taken\"] = f\"{insertion_time_taken:.2f} sec\"\n",
    "    details[\"Operation\"] = f\"Inserted {rows} records\"\n",
    "    details[\"records_after_op\"] = records_before_op + rows\n",
    "\n",
    "    # Add insertion times to the details\n",
    "    details[\"insertion_start_time\"] = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(insertion_start_time))\n",
    "    details[\"insertion_end_time\"] = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(insertion_end_time))\n",
    "    details[\"total_insertion_time\"] = f\"{insertion_time_taken:.2f} sec\"\n",
    "\n",
    "    # Print details before storing to CSV\n",
    "    print(f\"Details before appending to CSV: {details}\")\n",
    "    \n",
    "    records_before_op += rows\n",
    "    del insertion_start_time, insertion_end_time, df\n",
    "\n",
    "    # Store this info in the CSV file\n",
    "    append_to_file(analysis_file, details)\n",
    "    analysis_info.append(details)\n",
    "    \n",
    "    print(f\"Inserted {rows} records..\")\n",
    "    \n",
    "    ##### Updated records #####\n",
    "    current_digit = len(str(records_before_op))\n",
    "\n",
    "    #### Don't perform update operation if the digit has not increased\n",
    "    if current_digit <= digits:\n",
    "        continue\n",
    "    else:\n",
    "        digits = current_digit\n",
    "        df = spark.table(\"demo.nyc.taxis_5_100M\")\n",
    "    \n",
    "    # Perform the update operation for the first 10 distinct vendor_ids\n",
    "    for vendor_id in df.select('vendor_id').distinct().collect()[:10]:\n",
    "        vendor_id = vendor_id[0]\n",
    "        df = spark.table(\"demo.nyc.taxis_5_100M\")\n",
    "\n",
    "        st = time.time()\n",
    "        # Perform an update operation: Set fare_amount to fare_amount+40.0 for vendor_id\n",
    "        updated_df = df.withColumn(\"fare_amount\", \n",
    "                                  when(col(\"vendor_id\") == vendor_id, col(\"fare_amount\")+40)\n",
    "                                  .otherwise(col(\"fare_amount\")))\n",
    "        \n",
    "        # Overwrite the updated DataFrame back to the table\n",
    "        updated_df.writeTo(\"demo.nyc.taxis_5_100M\").overwritePartitions()\n",
    "        \n",
    "        end = time.time() - st\n",
    "        rows = updated_df.filter(updated_df['vendor_id']==vendor_id).count()\n",
    "\n",
    "        # Get details\n",
    "        details = get_size()\n",
    "        details[\"time_taken\"] = f\"{end:.2f} sec\"\n",
    "        details[\"Operation\"] = f\"Updated {rows} records\"\n",
    "        details[\"records_after_op\"] = records_before_op\n",
    "\n",
    "        append_to_file(analysis_file, details)\n",
    "\n",
    "        del df, st, end\n",
    "        analysis_info.append(details)\n",
    "\n",
    "# Function to get the size of the current table (e.g., row count or any other relevant metric)\n",
    "def get_size():\n",
    "    # Example: Return the size of the table and other useful metrics.\n",
    "    # Update this function based on your needs. This is just an example.\n",
    "    table_size = spark.table(\"demo.nyc.taxis_5_100M\").count()  # Placeholder for table size retrieval\n",
    "    return {\"table_size\": table_size}\n",
    "\n",
    "# Function to append details to the CSV file\n",
    "def append_to_file(file, details):\n",
    "    with open(file, mode='a', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=details.keys())\n",
    "        if f.tell() == 0:  # If file is empty, write headers\n",
    "            writer.writeheader()\n",
    "        writer.writerow(details)\n",
    "        print(f\"Appended details to {file}: {details}\")  # Log the details being appended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20170d99-d549-4f2d-a314-fc91c1edf469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
