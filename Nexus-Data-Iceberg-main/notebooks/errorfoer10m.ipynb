{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cb8ce48-2c46-449a-9e95-23ba0f51a24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/26 20:33:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize the SparkSession (step 1)\n",
    "spark = SparkSession.builder.appName(\"records\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4567926b-fc45-46f2-85bb-59dcde1e75f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/26 20:34:24 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"records\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43f83571-57a9-4421-9f0a-878f585a18bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS demo.nyc.taxis_10M_50COLUMNS_where\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6a3024b-5c28-4df3-8288-b79e384064d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "iceberg_table_dir = \"../warehouse/nyc/taxis_10M_50COLUMNS_where\"\n",
    "metadata_dir = f\"{iceberg_table_dir}/metadata\"\n",
    "data_dir = f\"{iceberg_table_dir}/data\"\n",
    "input_data_dir = f\"../input_data\"\n",
    "analysis_info = []\n",
    "records_before_op = 0\n",
    "\n",
    "def append_to_file(file_path, msg):\n",
    "    open_mode = \"a\"\n",
    "    if not os.path.exists(file_path):\n",
    "        open_mode = \"w\"\n",
    "\n",
    "    # Open the CSV file in write mode\n",
    "    with open(file_path, open_mode) as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        if open_mode==\"w\":\n",
    "            #writing header of the columns\n",
    "            writer.writerows([list(msg.keys())])    \n",
    "\n",
    "        row_values = [list(msg.values())]\n",
    "        # Write the data to the CSV file\n",
    "        writer.writerows(row_values)\n",
    "\n",
    "def get_size():\n",
    "    # List the metadata files\n",
    "    manifest_pattern = re.compile(r\".*-m\\d+\\.avro$\")\n",
    "    metadata_files = os.listdir(metadata_dir)\n",
    "    \n",
    "    # Initialize variables to store the sizes of different types of metadata files\n",
    "    snap_avro_size = 0\n",
    "    metadata_json_size = 0\n",
    "    m_avro_size = 0\n",
    "\n",
    "    data_dir_size = 0\n",
    "    # get data dir size\n",
    "    data_dir_files = os.listdir(data_dir)\n",
    "    # print(data_dir_files)\n",
    "    for filename in data_dir_files:\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        data_dir_size += os.path.getsize(file_path) / 1024  # Convert size to KB\n",
    "    \n",
    "    # Iterate through the metadata files and calculate their sizes\n",
    "    for file in metadata_files:\n",
    "        file_path = os.path.join(metadata_dir, file)\n",
    "        file_size_kb = os.path.getsize(file_path) / 1024  # Convert size to KB\n",
    "        \n",
    "        if file.startswith(\"snap-\") and file.endswith(\".avro\"):\n",
    "            snap_avro_size += file_size_kb\n",
    "        elif file.endswith(\".metadata.json\"):\n",
    "            metadata_json_size += file_size_kb\n",
    "        elif manifest_pattern.match(file):\n",
    "            m_avro_size += file_size_kb\n",
    "    \n",
    "    # Print the time taken and the sizes of the metadata files\n",
    "    # print(f\"Time taken to read Parquet files: {time_taken:.2f} seconds\")\n",
    "    # print(f\"Size of snap-*.avro files: {snap_avro_size:.2f} KB\")\n",
    "    # print(f\"Size of *.metadata.json files: {metadata_json_size:.2f} KB\")\n",
    "    # print(f\"Size of *m{0-9}{1,}.avro files: {m_avro_size:.2f} KB\")\n",
    "\n",
    "    return {\"data_dir_size\": data_dir_size,\"metadata_size\": metadata_json_size,\"snapshot_size\": snap_avro_size,\"manifest_size\": m_avro_size}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c92c51cb-8b1b-490a-9689-49d208c4a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    DoubleType, FloatType, LongType, StructType, StructField, \n",
    "    StringType, IntegerType, DateType\n",
    ")\n",
    "\n",
    "# Define the schema with 50 columns based on the required data types\n",
    "schema = StructType([\n",
    "    # StructField(\"vendor_id\", LongType(), True),  # INT\n",
    "    # StructField(\"trip_id\", LongType(), True),  # INT\n",
    "    # StructField(\"trip_distance\", FloatType(), True),  # FLOAT\n",
    "    # StructField(\"fare_amount\", DoubleType(), True),  # DOUBLE\n",
    "    # StructField(\"store_and_fwd_flag\", StringType(), True)  # STRING\n",
    "# ] + [\n",
    "    # Assigning VARCHAR, INT, STRING, and DATE data types in a cyclic pattern\n",
    "    StructField(f\"extra_col_{i}\", StringType(), True) if i % 4 == 0 else  # VARCHAR\n",
    "    StructField(f\"extra_col_{i}\", IntegerType(), True) if i % 4 == 1 else  # INT\n",
    "    StructField(f\"extra_col_{i}\", StringType(), True) if i % 4 == 2 else  # STRING\n",
    "    StructField(f\"extra_col_{i}\", DateType(), True)  # DATE\n",
    "    for i in range(50)\n",
    "])\n",
    "\n",
    "# Create an empty DataFrame with the schema\n",
    "df = spark.createDataFrame([], schema)\n",
    "\n",
    "# Create the Iceberg table\n",
    "df.writeTo(\"demo.nyc.taxis_10M_50COLUMNS_where\").create()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3649fdf-ce18-4bd3-92d8-34be64977d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "|extra_col_0|extra_col_1|extra_col_2|extra_col_3|extra_col_4|extra_col_5|extra_col_6|extra_col_7|extra_col_8|extra_col_9|extra_col_10|extra_col_11|extra_col_12|extra_col_13|extra_col_14|extra_col_15|extra_col_16|extra_col_17|extra_col_18|extra_col_19|extra_col_20|extra_col_21|extra_col_22|extra_col_23|extra_col_24|extra_col_25|extra_col_26|extra_col_27|extra_col_28|extra_col_29|extra_col_30|extra_col_31|extra_col_32|extra_col_33|extra_col_34|extra_col_35|extra_col_36|extra_col_37|extra_col_38|extra_col_39|extra_col_40|extra_col_41|extra_col_42|extra_col_43|extra_col_44|extra_col_45|extra_col_46|extra_col_47|extra_col_48|extra_col_49|\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(\"demo.nyc.taxis_10M_50COLUMNS_where\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e7eaaf8-a082-4b9d-b3aa-8f99db37f262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter input file type csv or parquet? :  parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started with file=records_1000000_part_10_1740401457.66906.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 35.03 sec.\n",
      "Started with file=records_1000000_part_1_1740398687.6853974.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 36.23 sec.\n",
      "Started with file=records_1000000_part_2_1740398997.7710938.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 30.20 sec.\n",
      "Started with file=records_1000000_part_3_1740399303.6597402.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 35.27 sec.\n",
      "Started with file=records_1000000_part_4_1740399611.4401598.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 31.94 sec.\n",
      "Started with file=records_1000000_part_5_1740399918.8825066.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 30.66 sec.\n",
      "Started with file=records_1000000_part_6_1740400229.5675209.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 37.72 sec.\n",
      "Started with file=records_1000000_part_7_1740400532.7327414.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 40.70 sec.\n",
      "Started with file=records_1000000_part_8_1740400841.6608176.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 45.97 sec.\n",
      "Started with file=records_1000000_part_9_1740401151.339735.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 37.14 sec.\n",
      "\n",
      "Total insertion time: 360.86 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter update percentage (e.g., 1 for 1%):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 100000 rows (~1.0%)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/26 20:42:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 20:42:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 20:42:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 20:42:29 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 20:42:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 20:42:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 20:42:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 20:42:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[INCOMPATIBLE_DATA_FOR_TABLE.CANNOT_FIND_DATA] Cannot write incompatible data for the table `demo`.`nyc`.`taxis_10M_50COLUMNS_where`: Cannot find data for the output column `extra_col_0`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 86\u001b[0m\n\u001b[1;32m     77\u001b[0m updated_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mjoin(broadcast_sampled_ids, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_col_0\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_outer\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_col_1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m     ) \\\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_col_0\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Drop the extra_col_0 after the join since it's no longer needed\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Overwrite the table partitions with the final updated data\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[43mupdated_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdemo.nyc.taxis_10M_50COLUMNS_where\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverwritePartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m st\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:2127\u001b[0m, in \u001b[0;36mDataFrameWriterV2.overwritePartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m3.1\u001b[39m)\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moverwritePartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;124;03m    Overwrite all partition for which the data frame contains at least one row with the contents\u001b[39;00m\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;124;03m    of the data frame in the output table.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2125\u001b[0m \u001b[38;5;124;03m    partitions dynamically depending on the contents of the data frame.\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2127\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverwritePartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [INCOMPATIBLE_DATA_FOR_TABLE.CANNOT_FIND_DATA] Cannot write incompatible data for the table `demo`.`nyc`.`taxis_10M_50COLUMNS_where`: Cannot find data for the output column `extra_col_0`."
     ]
    }
   ],
   "source": [
    "import time, csv\n",
    "from pyspark.sql.functions import col, when, lit, broadcast\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "input_data_dir = \"../input_data\"\n",
    "output_dir = \"../output\"\n",
    "analysis_info = []\n",
    "records_before_op = 0\n",
    "total_insertion_time = 0\n",
    "\n",
    "file_type = input(\"Enter input file type csv or parquet? : \").lower().strip()\n",
    "input_data_dir = os.path.join(input_data_dir, file_type)\n",
    "input_files = os.listdir(input_data_dir)\n",
    "\n",
    "analysis_file = os.path.join(output_dir, f\"analysis_info_{file_type}.csv\")\n",
    "if os.path.exists(analysis_file):\n",
    "    os.remove(analysis_file)\n",
    "\n",
    "df = spark.table(\"demo.nyc.taxis_10M_50COLUMNS_where\")\n",
    "records_before_op = df.count()\n",
    "\n",
    "for file in input_files:\n",
    "    print(f\"Started with file={file}\")\n",
    "    file_path = os.path.join(input_data_dir, file)\n",
    "\n",
    "    st = time.time()\n",
    "    if file_type == \"parquet\":\n",
    "        df = spark.read.parquet(file_path)\n",
    "    else:\n",
    "        df = spark.read.csv(file_path, header=True)\n",
    "        df = df.select(\n",
    "            F.col(\"extra_col_0\").cast(\"long\"),\n",
    "            F.col(\"extra_col_1\").cast(\"int\"),\n",
    "            F.col(\"extra_col_2\").cast(\"string\"),\n",
    "            F.col(\"extra_col_3\").cast(\"date\"),\n",
    "            *[F.col(f\"extra_col_{i}\").cast(\"string\" if i % 4 == 0 or i % 4 == 2 else \"int\" if i % 4 == 1 else \"date\") for i in range(4, 45)]\n",
    "        )\n",
    "\n",
    "    rows = df.count()\n",
    "    \n",
    "    df.writeTo(\"demo.nyc.taxis_10M_50COLUMNS_where\").append()\n",
    "    end = time.time() - st\n",
    "    total_insertion_time += end\n",
    "\n",
    "    details = {\"time_taken\": f\"{end:.2f} sec\", \"Operation\": f\"Inserted {rows} records\", \"records_after_op\": records_before_op + rows}\n",
    "    records_before_op += rows\n",
    "\n",
    "    print(f\"Inserted {rows} records in {end:.2f} sec.\")\n",
    "\n",
    "# **PRINT INSERTION TIME BEFORE UPDATE**\n",
    "print(f\"\\nTotal insertion time: {total_insertion_time:.2f} sec\\n\")\n",
    "\n",
    "# === Get Update Percentage from User ===\n",
    "update_percentage = float(input(\"Enter update percentage (e.g., 1 for 1%): \").strip()) / 100\n",
    "\n",
    "df = spark.table(\"demo.nyc.taxis_10M_50COLUMNS_where\")\n",
    "total_rows = df.count()\n",
    "\n",
    "num_rows = int(total_rows * update_percentage)\n",
    "print(f\"Updating {num_rows} rows (~{update_percentage*100}%)...\")\n",
    "\n",
    "# Sample random rows ensuring unique IDs (this will be done for each batch)\n",
    "sampled_df = df.select(\"extra_col_0\").distinct().orderBy(F.rand()).limit(num_rows)\n",
    "sampled_ids = [row[\"extra_col_0\"] for row in sampled_df.collect()]\n",
    "\n",
    "# Convert sampled_ids to a DataFrame to use with join\n",
    "sampled_ids_df = spark.createDataFrame([(id,) for id in sampled_ids], [\"extra_col_0\"])\n",
    "\n",
    "# Broadcast the sampled DataFrame to avoid broadcasting large task binary\n",
    "broadcast_sampled_ids = broadcast(sampled_ids_df)\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "# === Using JOIN for Update Operation ===\n",
    "# Instead of using `withColumn()` and `isin()` for the update, we join the sampled_ids to the original dataframe\n",
    "updated_df = df.join(broadcast_sampled_ids, on=\"extra_col_0\", how=\"left_outer\") \\\n",
    "    .withColumn(\n",
    "        \"extra_col_1\",\n",
    "        when(col(\"extra_col_0\").isNotNull(), col(\"extra_col_1\") + 10)\n",
    "        .otherwise(col(\"extra_col_1\"))\n",
    "    ) \\\n",
    "    .drop(\"extra_col_0\")  # Drop the extra_col_0 after the join since it's no longer needed\n",
    "\n",
    "# Overwrite the table partitions with the final updated data\n",
    "updated_df.writeTo(\"demo.nyc.taxis_10M_50COLUMNS_where\").overwritePartitions()\n",
    "\n",
    "end = time.time() - st\n",
    "\n",
    "print(f\"Updated {num_rows} rows in {end:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28ce5d44-5385-4434-89a6-aa6efa68d501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter input file type csv or parquet? :  parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started with file=records_1000000_part_10_1740401457.66906.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 38.04 sec.\n",
      "Started with file=records_1000000_part_1_1740398687.6853974.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 31.85 sec.\n",
      "Started with file=records_1000000_part_2_1740398997.7710938.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 35.17 sec.\n",
      "Started with file=records_1000000_part_3_1740399303.6597402.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 30.31 sec.\n",
      "Started with file=records_1000000_part_4_1740399611.4401598.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 36.82 sec.\n",
      "Started with file=records_1000000_part_5_1740399918.8825066.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 28.93 sec.\n",
      "Started with file=records_1000000_part_6_1740400229.5675209.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 34.69 sec.\n",
      "Started with file=records_1000000_part_7_1740400532.7327414.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 32.40 sec.\n",
      "Started with file=records_1000000_part_8_1740400841.6608176.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 33.85 sec.\n",
      "Started with file=records_1000000_part_9_1740401151.339735.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 30.38 sec.\n",
      "\n",
      "Total insertion time: 332.44 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter update percentage (e.g., 1 for 1%):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 100000 rows (~1.0%)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/26 20:51:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 20:51:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 20:51:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 20:51:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 20:51:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 20:51:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 20:51:24 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 20:51:25 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[INSERT_COLUMN_ARITY_MISMATCH.TOO_MANY_DATA_COLUMNS] Cannot write to `demo`.`nyc`.`taxis_10M_50COLUMNS_where`, the reason is too many data columns:\nTable columns: `extra_col_0`, `extra_col_1`, `extra_col_2`, `extra_col_3`, `extra_col_4`, `extra_col_5`, `extra_col_6`, `extra_col_7`, `extra_col_8`, `extra_col_9`, `extra_col_10`, `extra_col_11`, `extra_col_12`, `extra_col_13`, `extra_col_14`, `extra_col_15`, `extra_col_16`, `extra_col_17`, `extra_col_18`, `extra_col_19`, `extra_col_20`, `extra_col_21`, `extra_col_22`, `extra_col_23`, `extra_col_24`, `extra_col_25`, `extra_col_26`, `extra_col_27`, `extra_col_28`, `extra_col_29`, `extra_col_30`, `extra_col_31`, `extra_col_32`, `extra_col_33`, `extra_col_34`, `extra_col_35`, `extra_col_36`, `extra_col_37`, `extra_col_38`, `extra_col_39`, `extra_col_40`, `extra_col_41`, `extra_col_42`, `extra_col_43`, `extra_col_44`, `extra_col_45`, `extra_col_46`, `extra_col_47`, `extra_col_48`, `extra_col_49`.\nData columns: `extra_col_0`, `extra_col_1`, `extra_col_1`, `extra_col_2`, `extra_col_3`, `extra_col_4`, `extra_col_5`, `extra_col_6`, `extra_col_7`, `extra_col_8`, `extra_col_9`, `extra_col_10`, `extra_col_11`, `extra_col_12`, `extra_col_13`, `extra_col_14`, `extra_col_15`, `extra_col_16`, `extra_col_17`, `extra_col_18`, `extra_col_19`, `extra_col_20`, `extra_col_21`, `extra_col_22`, `extra_col_23`, `extra_col_24`, `extra_col_25`, `extra_col_26`, `extra_col_27`, `extra_col_28`, `extra_col_29`, `extra_col_30`, `extra_col_31`, `extra_col_32`, `extra_col_33`, `extra_col_34`, `extra_col_35`, `extra_col_36`, `extra_col_37`, `extra_col_38`, `extra_col_39`, `extra_col_40`, `extra_col_41`, `extra_col_42`, `extra_col_43`, `extra_col_44`, `extra_col_45`, `extra_col_46`, `extra_col_47`, `extra_col_48`, `extra_col_49`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 96\u001b[0m\n\u001b[1;32m     89\u001b[0m updated_df \u001b[38;5;241m=\u001b[39m updated_df\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_col_0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_col_1\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Ensure other columns are selected\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;241m*\u001b[39m[col(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_col_0\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# Include all other original columns\u001b[39;00m\n\u001b[1;32m     93\u001b[0m )\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Overwrite the table partitions with the final updated data\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m \u001b[43mupdated_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdemo.nyc.taxis_10M_50COLUMNS_where\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverwritePartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m st\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m rows in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:2127\u001b[0m, in \u001b[0;36mDataFrameWriterV2.overwritePartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m3.1\u001b[39m)\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moverwritePartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;124;03m    Overwrite all partition for which the data frame contains at least one row with the contents\u001b[39;00m\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;124;03m    of the data frame in the output table.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2125\u001b[0m \u001b[38;5;124;03m    partitions dynamically depending on the contents of the data frame.\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2127\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverwritePartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [INSERT_COLUMN_ARITY_MISMATCH.TOO_MANY_DATA_COLUMNS] Cannot write to `demo`.`nyc`.`taxis_10M_50COLUMNS_where`, the reason is too many data columns:\nTable columns: `extra_col_0`, `extra_col_1`, `extra_col_2`, `extra_col_3`, `extra_col_4`, `extra_col_5`, `extra_col_6`, `extra_col_7`, `extra_col_8`, `extra_col_9`, `extra_col_10`, `extra_col_11`, `extra_col_12`, `extra_col_13`, `extra_col_14`, `extra_col_15`, `extra_col_16`, `extra_col_17`, `extra_col_18`, `extra_col_19`, `extra_col_20`, `extra_col_21`, `extra_col_22`, `extra_col_23`, `extra_col_24`, `extra_col_25`, `extra_col_26`, `extra_col_27`, `extra_col_28`, `extra_col_29`, `extra_col_30`, `extra_col_31`, `extra_col_32`, `extra_col_33`, `extra_col_34`, `extra_col_35`, `extra_col_36`, `extra_col_37`, `extra_col_38`, `extra_col_39`, `extra_col_40`, `extra_col_41`, `extra_col_42`, `extra_col_43`, `extra_col_44`, `extra_col_45`, `extra_col_46`, `extra_col_47`, `extra_col_48`, `extra_col_49`.\nData columns: `extra_col_0`, `extra_col_1`, `extra_col_1`, `extra_col_2`, `extra_col_3`, `extra_col_4`, `extra_col_5`, `extra_col_6`, `extra_col_7`, `extra_col_8`, `extra_col_9`, `extra_col_10`, `extra_col_11`, `extra_col_12`, `extra_col_13`, `extra_col_14`, `extra_col_15`, `extra_col_16`, `extra_col_17`, `extra_col_18`, `extra_col_19`, `extra_col_20`, `extra_col_21`, `extra_col_22`, `extra_col_23`, `extra_col_24`, `extra_col_25`, `extra_col_26`, `extra_col_27`, `extra_col_28`, `extra_col_29`, `extra_col_30`, `extra_col_31`, `extra_col_32`, `extra_col_33`, `extra_col_34`, `extra_col_35`, `extra_col_36`, `extra_col_37`, `extra_col_38`, `extra_col_39`, `extra_col_40`, `extra_col_41`, `extra_col_42`, `extra_col_43`, `extra_col_44`, `extra_col_45`, `extra_col_46`, `extra_col_47`, `extra_col_48`, `extra_col_49`."
     ]
    }
   ],
   "source": [
    "import time, csv\n",
    "from pyspark.sql.functions import col, when, lit, broadcast\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "input_data_dir = \"../input_data\"\n",
    "output_dir = \"../output\"\n",
    "analysis_info = []\n",
    "records_before_op = 0\n",
    "total_insertion_time = 0\n",
    "\n",
    "file_type = input(\"Enter input file type csv or parquet? : \").lower().strip()\n",
    "input_data_dir = os.path.join(input_data_dir, file_type)\n",
    "input_files = os.listdir(input_data_dir)\n",
    "\n",
    "analysis_file = os.path.join(output_dir, f\"analysis_info_{file_type}.csv\")\n",
    "if os.path.exists(analysis_file):\n",
    "    os.remove(analysis_file)\n",
    "\n",
    "df = spark.table(\"demo.nyc.taxis_10M_50COLUMNS_where\")\n",
    "records_before_op = df.count()\n",
    "\n",
    "for file in input_files:\n",
    "    print(f\"Started with file={file}\")\n",
    "    file_path = os.path.join(input_data_dir, file)\n",
    "\n",
    "    st = time.time()\n",
    "    if file_type == \"parquet\":\n",
    "        df = spark.read.parquet(file_path)\n",
    "    else:\n",
    "        df = spark.read.csv(file_path, header=True)\n",
    "        df = df.select(\n",
    "            F.col(\"extra_col_0\").cast(\"long\"),\n",
    "            F.col(\"extra_col_1\").cast(\"int\"),\n",
    "            F.col(\"extra_col_2\").cast(\"string\"),\n",
    "            F.col(\"extra_col_3\").cast(\"date\"),\n",
    "            *[F.col(f\"extra_col_{i}\").cast(\"string\" if i % 4 == 0 or i % 4 == 2 else \"int\" if i % 4 == 1 else \"date\") for i in range(4, 45)]\n",
    "        )\n",
    "\n",
    "    rows = df.count()\n",
    "    \n",
    "    # Writing the records into the existing table\n",
    "    df.writeTo(\"demo.nyc.taxis_10M_50COLUMNS_where\").append()\n",
    "    end = time.time() - st\n",
    "    total_insertion_time += end\n",
    "\n",
    "    details = {\"time_taken\": f\"{end:.2f} sec\", \"Operation\": f\"Inserted {rows} records\", \"records_after_op\": records_before_op + rows}\n",
    "    records_before_op += rows\n",
    "\n",
    "    print(f\"Inserted {rows} records in {end:.2f} sec.\")\n",
    "\n",
    "# **PRINT INSERTION TIME BEFORE UPDATE**\n",
    "print(f\"\\nTotal insertion time: {total_insertion_time:.2f} sec\\n\")\n",
    "\n",
    "# === Get Update Percentage from User ===\n",
    "update_percentage = float(input(\"Enter update percentage (e.g., 1 for 1%): \").strip()) / 100\n",
    "\n",
    "df = spark.table(\"demo.nyc.taxis_10M_50COLUMNS_where\")\n",
    "total_rows = df.count()\n",
    "\n",
    "num_rows = int(total_rows * update_percentage)\n",
    "print(f\"Updating {num_rows} rows (~{update_percentage*100}%)...\")\n",
    "\n",
    "# Sample random rows ensuring unique IDs (this will be done for each batch)\n",
    "sampled_df = df.select(\"extra_col_0\").distinct().orderBy(F.rand()).limit(num_rows)\n",
    "sampled_ids = [row[\"extra_col_0\"] for row in sampled_df.collect()]\n",
    "\n",
    "# Convert sampled_ids to a DataFrame to use with join\n",
    "sampled_ids_df = spark.createDataFrame([(id,) for id in sampled_ids], [\"extra_col_0\"])\n",
    "\n",
    "# Broadcast the sampled DataFrame to avoid broadcasting large task binary\n",
    "broadcast_sampled_ids = broadcast(sampled_ids_df)\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "# === Using JOIN for Update Operation ===\n",
    "# Instead of using `withColumn()` and `isin()` for the update, we join the sampled_ids to the original dataframe\n",
    "updated_df = df.join(broadcast_sampled_ids, on=\"extra_col_0\", how=\"left_outer\") \\\n",
    "    .withColumn(\n",
    "        \"extra_col_1\",\n",
    "        when(col(\"extra_col_0\").isNotNull(), col(\"extra_col_1\") + 10)\n",
    "        .otherwise(col(\"extra_col_1\"))\n",
    "    )\n",
    "\n",
    "# Ensure we keep the necessary column and write it to the table\n",
    "updated_df = updated_df.dropna(subset=[\"extra_col_1\"])  # Drop rows with NaN if needed\n",
    "\n",
    "# **Ensure extra_col_0 is part of the final DataFrame for compatibility with the table schema**\n",
    "updated_df = updated_df.select(\n",
    "    \"extra_col_0\",\n",
    "    \"extra_col_1\",  # Ensure other columns are selected\n",
    "    *[col(c) for c in df.columns if c != \"extra_col_0\"]  # Include all other original columns\n",
    ")\n",
    "\n",
    "# Overwrite the table partitions with the final updated data\n",
    "updated_df.writeTo(\"demo.nyc.taxis_10M_50COLUMNS_where\").overwritePartitions()\n",
    "\n",
    "end = time.time() - st\n",
    "\n",
    "print(f\"Updated {num_rows} rows in {end:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4728d791-0521-4e07-aff6-178a0f798108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter input file type csv or parquet? :  parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started with file=records_1000000_part_10_1740401457.66906.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 32.59 sec.\n",
      "Started with file=records_1000000_part_1_1740398687.6853974.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 33.09 sec.\n",
      "Started with file=records_1000000_part_2_1740398997.7710938.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 30.85 sec.\n",
      "Started with file=records_1000000_part_3_1740399303.6597402.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 35.15 sec.\n",
      "Started with file=records_1000000_part_4_1740399611.4401598.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 33.53 sec.\n",
      "Started with file=records_1000000_part_5_1740399918.8825066.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 37.07 sec.\n",
      "Started with file=records_1000000_part_6_1740400229.5675209.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 37.14 sec.\n",
      "Started with file=records_1000000_part_7_1740400532.7327414.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 31.23 sec.\n",
      "Started with file=records_1000000_part_8_1740400841.6608176.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 34.77 sec.\n",
      "Started with file=records_1000000_part_9_1740401151.339735.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1000000 records in 31.15 sec.\n",
      "\n",
      "Total insertion time: 336.56 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter update percentage (e.g., 1 for 1%):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 100000 rows (~1.0%)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/26 21:00:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 21:00:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 21:00:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 21:00:33 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 21:00:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 21:00:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 21:00:35 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/02/26 21:00:36 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `extra_col_0` cannot be resolved. Did you mean one of the following? [`extra_col_1`, `demo`.`nyc`.`taxis_10M_50COLUMNS_where`.`extra_col_10`, `demo`.`nyc`.`taxis_10M_50COLUMNS_where`.`extra_col_2`, `demo`.`nyc`.`taxis_10M_50COLUMNS_where`.`extra_col_20`, `demo`.`nyc`.`taxis_10M_50COLUMNS_where`.`extra_col_3`].;\n'Project ['extra_col_0, extra_col_1#11318, extra_col_1#11318, extra_col_2#11102, extra_col_3#11103, extra_col_4#11104, extra_col_5#11105, extra_col_6#11106, extra_col_7#11107, extra_col_8#11108, extra_col_9#11109, extra_col_10#11110, extra_col_11#11111, extra_col_12#11112, extra_col_13#11113, extra_col_14#11114, extra_col_15#11115, extra_col_16#11116, extra_col_17#11117, extra_col_18#11118, extra_col_19#11119, extra_col_20#11120, extra_col_21#11121, extra_col_22#11122, ... 27 more fields]\n+- Project [extra_col_1#11318, extra_col_2#11102, extra_col_3#11103, extra_col_4#11104, extra_col_5#11105, extra_col_6#11106, extra_col_7#11107, extra_col_8#11108, extra_col_9#11109, extra_col_10#11110, extra_col_11#11111, extra_col_12#11112, extra_col_13#11113, extra_col_14#11114, extra_col_15#11115, extra_col_16#11116, extra_col_17#11117, extra_col_18#11118, extra_col_19#11119, extra_col_20#11120, extra_col_21#11121, extra_col_22#11122, extra_col_23#11123, extra_col_24#11124, ... 25 more fields]\n   +- Project [extra_col_0#11100, CASE WHEN isnotnull(extra_col_0#11100) THEN (extra_col_1#11101 + 10) ELSE extra_col_1#11101 END AS extra_col_1#11318, extra_col_2#11102, extra_col_3#11103, extra_col_4#11104, extra_col_5#11105, extra_col_6#11106, extra_col_7#11107, extra_col_8#11108, extra_col_9#11109, extra_col_10#11110, extra_col_11#11111, extra_col_12#11112, extra_col_13#11113, extra_col_14#11114, extra_col_15#11115, extra_col_16#11116, extra_col_17#11117, extra_col_18#11118, extra_col_19#11119, extra_col_20#11120, extra_col_21#11121, extra_col_22#11122, extra_col_23#11123, ... 26 more fields]\n      +- Project [extra_col_0#11100, extra_col_1#11101, extra_col_2#11102, extra_col_3#11103, extra_col_4#11104, extra_col_5#11105, extra_col_6#11106, extra_col_7#11107, extra_col_8#11108, extra_col_9#11109, extra_col_10#11110, extra_col_11#11111, extra_col_12#11112, extra_col_13#11113, extra_col_14#11114, extra_col_15#11115, extra_col_16#11116, extra_col_17#11117, extra_col_18#11118, extra_col_19#11119, extra_col_20#11120, extra_col_21#11121, extra_col_22#11122, extra_col_23#11123, ... 26 more fields]\n         +- Join LeftOuter, (extra_col_0#11100 = extra_col_0#11266)\n            :- SubqueryAlias demo.nyc.taxis_10M_50COLUMNS_where\n            :  +- RelationV2[extra_col_0#11100, extra_col_1#11101, extra_col_2#11102, extra_col_3#11103, extra_col_4#11104, extra_col_5#11105, extra_col_6#11106, extra_col_7#11107, extra_col_8#11108, extra_col_9#11109, extra_col_10#11110, extra_col_11#11111, extra_col_12#11112, extra_col_13#11113, extra_col_14#11114, extra_col_15#11115, extra_col_16#11116, extra_col_17#11117, extra_col_18#11118, extra_col_19#11119, extra_col_20#11120, extra_col_21#11121, extra_col_22#11122, extra_col_23#11123, ... 26 more fields] demo.nyc.taxis_10M_50COLUMNS_where demo.nyc.taxis_10M_50COLUMNS_where\n            +- ResolvedHint (strategy=broadcast)\n               +- LogicalRDD [extra_col_0#11266], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 86\u001b[0m\n\u001b[1;32m     77\u001b[0m updated_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mjoin(broadcast_sampled_ids, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_col_0\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_outer\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_col_1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m     ) \\\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_col_0\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Drop the extra_col_0 after the join since it's no longer needed\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# **Correct Column Selection to Avoid Duplicates**\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m updated_df \u001b[38;5;241m=\u001b[39m \u001b[43mupdated_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextra_col_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Include the key column\u001b[39;49;00m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextra_col_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Updated column\u001b[39;49;00m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextra_col_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Include all other original columns except extra_col_0\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Check the schema to verify the columns\u001b[39;00m\n\u001b[1;32m     93\u001b[0m updated_df\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:3227\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3184\u001b[0m \n\u001b[1;32m   3185\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3227\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `extra_col_0` cannot be resolved. Did you mean one of the following? [`extra_col_1`, `demo`.`nyc`.`taxis_10M_50COLUMNS_where`.`extra_col_10`, `demo`.`nyc`.`taxis_10M_50COLUMNS_where`.`extra_col_2`, `demo`.`nyc`.`taxis_10M_50COLUMNS_where`.`extra_col_20`, `demo`.`nyc`.`taxis_10M_50COLUMNS_where`.`extra_col_3`].;\n'Project ['extra_col_0, extra_col_1#11318, extra_col_1#11318, extra_col_2#11102, extra_col_3#11103, extra_col_4#11104, extra_col_5#11105, extra_col_6#11106, extra_col_7#11107, extra_col_8#11108, extra_col_9#11109, extra_col_10#11110, extra_col_11#11111, extra_col_12#11112, extra_col_13#11113, extra_col_14#11114, extra_col_15#11115, extra_col_16#11116, extra_col_17#11117, extra_col_18#11118, extra_col_19#11119, extra_col_20#11120, extra_col_21#11121, extra_col_22#11122, ... 27 more fields]\n+- Project [extra_col_1#11318, extra_col_2#11102, extra_col_3#11103, extra_col_4#11104, extra_col_5#11105, extra_col_6#11106, extra_col_7#11107, extra_col_8#11108, extra_col_9#11109, extra_col_10#11110, extra_col_11#11111, extra_col_12#11112, extra_col_13#11113, extra_col_14#11114, extra_col_15#11115, extra_col_16#11116, extra_col_17#11117, extra_col_18#11118, extra_col_19#11119, extra_col_20#11120, extra_col_21#11121, extra_col_22#11122, extra_col_23#11123, extra_col_24#11124, ... 25 more fields]\n   +- Project [extra_col_0#11100, CASE WHEN isnotnull(extra_col_0#11100) THEN (extra_col_1#11101 + 10) ELSE extra_col_1#11101 END AS extra_col_1#11318, extra_col_2#11102, extra_col_3#11103, extra_col_4#11104, extra_col_5#11105, extra_col_6#11106, extra_col_7#11107, extra_col_8#11108, extra_col_9#11109, extra_col_10#11110, extra_col_11#11111, extra_col_12#11112, extra_col_13#11113, extra_col_14#11114, extra_col_15#11115, extra_col_16#11116, extra_col_17#11117, extra_col_18#11118, extra_col_19#11119, extra_col_20#11120, extra_col_21#11121, extra_col_22#11122, extra_col_23#11123, ... 26 more fields]\n      +- Project [extra_col_0#11100, extra_col_1#11101, extra_col_2#11102, extra_col_3#11103, extra_col_4#11104, extra_col_5#11105, extra_col_6#11106, extra_col_7#11107, extra_col_8#11108, extra_col_9#11109, extra_col_10#11110, extra_col_11#11111, extra_col_12#11112, extra_col_13#11113, extra_col_14#11114, extra_col_15#11115, extra_col_16#11116, extra_col_17#11117, extra_col_18#11118, extra_col_19#11119, extra_col_20#11120, extra_col_21#11121, extra_col_22#11122, extra_col_23#11123, ... 26 more fields]\n         +- Join LeftOuter, (extra_col_0#11100 = extra_col_0#11266)\n            :- SubqueryAlias demo.nyc.taxis_10M_50COLUMNS_where\n            :  +- RelationV2[extra_col_0#11100, extra_col_1#11101, extra_col_2#11102, extra_col_3#11103, extra_col_4#11104, extra_col_5#11105, extra_col_6#11106, extra_col_7#11107, extra_col_8#11108, extra_col_9#11109, extra_col_10#11110, extra_col_11#11111, extra_col_12#11112, extra_col_13#11113, extra_col_14#11114, extra_col_15#11115, extra_col_16#11116, extra_col_17#11117, extra_col_18#11118, extra_col_19#11119, extra_col_20#11120, extra_col_21#11121, extra_col_22#11122, extra_col_23#11123, ... 26 more fields] demo.nyc.taxis_10M_50COLUMNS_where demo.nyc.taxis_10M_50COLUMNS_where\n            +- ResolvedHint (strategy=broadcast)\n               +- LogicalRDD [extra_col_0#11266], false\n"
     ]
    }
   ],
   "source": [
    "import time, csv\n",
    "from pyspark.sql.functions import col, when, lit, broadcast\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "input_data_dir = \"../input_data\"\n",
    "output_dir = \"../output\"\n",
    "analysis_info = []\n",
    "records_before_op = 0\n",
    "total_insertion_time = 0\n",
    "\n",
    "file_type = input(\"Enter input file type csv or parquet? : \").lower().strip()\n",
    "input_data_dir = os.path.join(input_data_dir, file_type)\n",
    "input_files = os.listdir(input_data_dir)\n",
    "\n",
    "analysis_file = os.path.join(output_dir, f\"analysis_info_{file_type}.csv\")\n",
    "if os.path.exists(analysis_file):\n",
    "    os.remove(analysis_file)\n",
    "\n",
    "df = spark.table(\"demo.nyc.taxis_10M_50COLUMNS_where\")\n",
    "records_before_op = df.count()\n",
    "\n",
    "for file in input_files:\n",
    "    print(f\"Started with file={file}\")\n",
    "    file_path = os.path.join(input_data_dir, file)\n",
    "\n",
    "    st = time.time()\n",
    "    if file_type == \"parquet\":\n",
    "        df = spark.read.parquet(file_path)\n",
    "    else:\n",
    "        df = spark.read.csv(file_path, header=True)\n",
    "        df = df.select(\n",
    "            F.col(\"extra_col_0\").cast(\"long\"),\n",
    "            F.col(\"extra_col_1\").cast(\"int\"),\n",
    "            F.col(\"extra_col_2\").cast(\"string\"),\n",
    "            F.col(\"extra_col_3\").cast(\"date\"),\n",
    "            *[F.col(f\"extra_col_{i}\").cast(\"string\" if i % 4 == 0 or i % 4 == 2 else \"int\" if i % 4 == 1 else \"date\") for i in range(4, 45)]\n",
    "        )\n",
    "\n",
    "    rows = df.count()\n",
    "    \n",
    "    df.writeTo(\"demo.nyc.taxis_10M_50COLUMNS_where\").append()\n",
    "    end = time.time() - st\n",
    "    total_insertion_time += end\n",
    "\n",
    "    details = {\"time_taken\": f\"{end:.2f} sec\", \"Operation\": f\"Inserted {rows} records\", \"records_after_op\": records_before_op + rows}\n",
    "    records_before_op += rows\n",
    "\n",
    "    print(f\"Inserted {rows} records in {end:.2f} sec.\")\n",
    "\n",
    "# **PRINT INSERTION TIME BEFORE UPDATE**\n",
    "print(f\"\\nTotal insertion time: {total_insertion_time:.2f} sec\\n\")\n",
    "\n",
    "# === Get Update Percentage from User ===\n",
    "update_percentage = float(input(\"Enter update percentage (e.g., 1 for 1%): \").strip()) / 100\n",
    "\n",
    "df = spark.table(\"demo.nyc.taxis_10M_50COLUMNS_where\")\n",
    "total_rows = df.count()\n",
    "\n",
    "num_rows = int(total_rows * update_percentage)\n",
    "print(f\"Updating {num_rows} rows (~{update_percentage*100}%)...\")\n",
    "\n",
    "# Sample random rows ensuring unique IDs (this will be done for each batch)\n",
    "sampled_df = df.select(\"extra_col_0\").distinct().orderBy(F.rand()).limit(num_rows)\n",
    "sampled_ids = [row[\"extra_col_0\"] for row in sampled_df.collect()]\n",
    "\n",
    "# Convert sampled_ids to a DataFrame to use with join\n",
    "sampled_ids_df = spark.createDataFrame([(id,) for id in sampled_ids], [\"extra_col_0\"])\n",
    "\n",
    "# Broadcast the sampled DataFrame to avoid broadcasting large task binary\n",
    "broadcast_sampled_ids = broadcast(sampled_ids_df)\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "# === Using JOIN for Update Operation ===\n",
    "# Instead of using `withColumn()` and `isin()` for the update, we join the sampled_ids to the original dataframe\n",
    "updated_df = df.join(broadcast_sampled_ids, on=\"extra_col_0\", how=\"left_outer\") \\\n",
    "    .withColumn(\n",
    "        \"extra_col_1\",\n",
    "        when(col(\"extra_col_0\").isNotNull(), col(\"extra_col_1\") + 10)\n",
    "        .otherwise(col(\"extra_col_1\"))\n",
    "    ) \\\n",
    "    .drop(\"extra_col_0\")  # Drop the extra_col_0 after the join since it's no longer needed\n",
    "\n",
    "# **Correct Column Selection to Avoid Duplicates**\n",
    "updated_df = updated_df.select(\n",
    "    \"extra_col_0\",  # Include the key column\n",
    "    \"extra_col_1\",  # Updated column\n",
    "    *[col(c) for c in df.columns if c != \"extra_col_0\"]  # Include all other original columns except extra_col_0\n",
    ")\n",
    "\n",
    "# Check the schema to verify the columns\n",
    "updated_df.printSchema()\n",
    "\n",
    "# Overwrite the table partitions with the final updated data\n",
    "updated_df.writeTo(\"demo.nyc.taxis_10M_50COLUMNS_where\").overwritePartitions()\n",
    "\n",
    "end = time.time() - st\n",
    "\n",
    "print(f\"Updated {num_rows} rows in {end:.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c127c4d-8bd5-492c-87fe-9974ba26f644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
