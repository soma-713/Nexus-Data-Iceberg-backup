{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee35e052-86b4-42f5-83cf-9efbddb22c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (2.2.1)\n",
      "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.9/site-packages (3.2.2)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.9/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.9/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcd5394c-0ce8-4b3d-91de-f9839ff09bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"records\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ab8db6a-f62d-4205-a475-9d25bb0bde90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|         demo|\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW CATALOGS\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9bed794-4f64-4759-9a18-4e5c8c52f7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"DROP TABLE IF EXISTS nyc.taxis_10M_50COLUMNS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0eb2fbb-97ed-4810-9303-bde3d14b628c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " spark.sql(\"DROP TABLE IF EXISTS demo.nyc.taxis_10M_50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bda4a8b-f73f-4b25-bc17-f95cb555aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "iceberg_table_dir = \"../warehouse/nyc/taxis_10M_50\"\n",
    "metadata_dir = f\"{iceberg_table_dir}/metadata\"\n",
    "data_dir = f\"{iceberg_table_dir}/data\"\n",
    "input_data_dir = f\"../input_data\"\n",
    "analysis_info = []\n",
    "records_before_op = 0\n",
    "\n",
    "def append_to_file(file_path, msg):\n",
    "    open_mode = \"a\"\n",
    "    if not os.path.exists(file_path):\n",
    "        open_mode = \"w\"\n",
    "\n",
    "    # Open the CSV file in write mode\n",
    "    with open(file_path, open_mode) as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        if open_mode==\"w\":\n",
    "            #writing header of the columns\n",
    "            writer.writerows([list(msg.keys())])    \n",
    "\n",
    "        row_values = [list(msg.values())]\n",
    "        # Write the data to the CSV file\n",
    "        writer.writerows(row_values)\n",
    "\n",
    "def get_size():\n",
    "    # List the metadata files\n",
    "    manifest_pattern = re.compile(r\".*-m\\d+\\.avro$\")\n",
    "    metadata_files = os.listdir(metadata_dir)\n",
    "    \n",
    "    # Initialize variables to store the sizes of different types of metadata files\n",
    "    snap_avro_size = 0\n",
    "    metadata_json_size = 0\n",
    "    m_avro_size = 0\n",
    "\n",
    "    data_dir_size = 0\n",
    "    # get data dir size\n",
    "    data_dir_files = os.listdir(data_dir)\n",
    "    # print(data_dir_files)\n",
    "    for filename in data_dir_files:\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        data_dir_size += os.path.getsize(file_path) / 1024  # Convert size to KB\n",
    "    \n",
    "    # Iterate through the metadata files and calculate their sizes\n",
    "    for file in metadata_files:\n",
    "        file_path = os.path.join(metadata_dir, file)\n",
    "        file_size_kb = os.path.getsize(file_path) / 1024  # Convert size to KB\n",
    "        \n",
    "        if file.startswith(\"snap-\") and file.endswith(\".avro\"):\n",
    "            snap_avro_size += file_size_kb\n",
    "        elif file.endswith(\".metadata.json\"):\n",
    "            metadata_json_size += file_size_kb\n",
    "        elif manifest_pattern.match(file):\n",
    "            m_avro_size += file_size_kb\n",
    "    \n",
    "    # Print the time taken and the sizes of the metadata files\n",
    "    # print(f\"Time taken to read Parquet files: {time_taken:.2f} seconds\")\n",
    "    # print(f\"Size of snap-*.avro files: {snap_avro_size:.2f} KB\")\n",
    "    # print(f\"Size of *.metadata.json files: {metadata_json_size:.2f} KB\")\n",
    "    # print(f\"Size of *m{0-9}{1,}.avro files: {m_avro_size:.2f} KB\")\n",
    "\n",
    "    return {\"data_dir_size\": data_dir_size,\"metadata_size\": metadata_json_size,\"snapshot_size\": snap_avro_size,\"manifest_size\": m_avro_size}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc52770e-bf50-4028-9d58-916f919225c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    DoubleType, FloatType, LongType, StructType, StructField, \n",
    "    StringType, IntegerType, DateType\n",
    ")\n",
    "\n",
    "# Define the schema with 50 columns based on the required data types\n",
    "schema = StructType([\n",
    "    # StructField(\"vendor_id\", LongType(), True),  # INT\n",
    "    # StructField(\"trip_id\", LongType(), True),  # INT\n",
    "    # StructField(\"trip_distance\", FloatType(), True),  # FLOAT\n",
    "    # StructField(\"fare_amount\", DoubleType(), True),  # DOUBLE\n",
    "    # StructField(\"store_and_fwd_flag\", StringType(), True)  # STRING\n",
    "# ] + [\n",
    "    # Assigning VARCHAR, INT, STRING, and DATE data types in a cyclic pattern\n",
    "    StructField(f\"extra_col_{i}\", StringType(), True) if i % 4 == 0 else  # VARCHAR\n",
    "    StructField(f\"extra_col_{i}\", IntegerType(), True) if i % 4 == 1 else  # INT\n",
    "    StructField(f\"extra_col_{i}\", StringType(), True) if i % 4 == 2 else  # STRING\n",
    "    StructField(f\"extra_col_{i}\", DateType(), True)  # DATE\n",
    "    for i in range(50)\n",
    "])\n",
    "\n",
    "# Create an empty DataFrame with the schema\n",
    "df = spark.createDataFrame([], schema)\n",
    "\n",
    "# Create the Iceberg table\n",
    "df.writeTo(\"demo.nyc.taxis_10M_50\").create()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72f44de1-3b71-4e66-9ebc-6dcdc99ccc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.table(\"demo.nyc.taxis_10M_50COLUMNS\")\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8c952b9-cac6-407d-82c9-afcdf1f52a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter input file type csv or parquet? :  parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started with file=records_1000000_part_10_1740401457.66906.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n",
      "Started with file=records_1000000_part_1_1740398687.6853974.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n",
      "Started with file=records_1000000_part_2_1740398997.7710938.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n",
      "Started with file=records_1000000_part_3_1740399303.6597402.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n",
      "Started with file=records_1000000_part_4_1740399611.4401598.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n",
      "Started with file=records_1000000_part_5_1740399918.8825066.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n",
      "Started with file=records_1000000_part_6_1740400229.5675209.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n",
      "Started with file=records_1000000_part_7_1740400532.7327414.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n",
      "Started with file=records_1000000_part_8_1740400841.6608176.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n",
      "Started with file=records_1000000_part_9_1740401151.339735.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted 1000000 records..\n",
      "\n",
      "Total insertion time: 278.41 sec\n"
     ]
    }
   ],
   "source": [
    "import time, csv\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "\n",
    "input_data_dir = \"../input_data\"\n",
    "output_dir = \"../output\"\n",
    "analysis_info = []\n",
    "records_before_op = 0\n",
    "total_insertion_time = 0\n",
    "    \n",
    "file_type = input(\"Enter input file type csv or parquet? : \")\n",
    "file_type = file_type.lower().strip()\n",
    "input_data_dir = os.path.join(input_data_dir, file_type)\n",
    "input_files = os.listdir(input_data_dir)\n",
    "\n",
    "analysis_file = os.path.join(output_dir, f\"analysis_info_{file_type}.csv\")\n",
    "if os.path.exists(analysis_file):\n",
    "    os.remove(analysis_file)\n",
    "\n",
    "df = spark.table(\"demo.nyc.taxis_10M_50\")\n",
    "records_before_op = df.count()\n",
    "\n",
    "digits = len(str(records_before_op))\n",
    "\n",
    "for file in input_files:\n",
    "    print(f\"Started with file={file}\")\n",
    "    file_path = os.path.join(input_data_dir, file)\n",
    "\n",
    "    st = time.time()\n",
    "    if file_type == \"parquet\":\n",
    "        df = spark.read.parquet(file_path)\n",
    "    else:\n",
    "        df = spark.read.csv(file_path, header=True)\n",
    "        df = df.select(\n",
    "            F.col(\"vendor_id\").cast(\"long\"),\n",
    "            F.col(\"trip_id\").cast(\"long\"),\n",
    "            F.col(\"trip_distance\").cast(\"float\"),\n",
    "            F.col(\"fare_amount\").cast(\"double\"),\n",
    "            F.col(\"store_and_fwd_flag\").cast(\"string\"),\n",
    "            *[F.col(f\"extra_col_{i}\").cast(\"string\" if i % 4 == 0 or i % 4 == 2 else \"int\" if i % 4 == 1 else \"date\") for i in range(45)]\n",
    "        )\n",
    "    \n",
    "    rows = df.count()\n",
    "    \n",
    "    df.writeTo(\"demo.nyc.taxis_10M_50\").append()\n",
    "    end = time.time() - st\n",
    "    total_insertion_time += end\n",
    "\n",
    "    details = get_size()\n",
    "    details[\"time_taken\"] = f\"{end} sec\"\n",
    "    details[\"Operation\"] = f\"Inserted {rows} records\"\n",
    "    details[\"records_after_op\"] = records_before_op + rows\n",
    "\n",
    "    records_before_op += rows\n",
    "    del st, end, df\n",
    "\n",
    "    append_to_file(analysis_file, details)\n",
    "    analysis_info.append(details)\n",
    "    \n",
    "    print(f\"inserted {rows} records..\")\n",
    "    \n",
    "    current_digit = len(str(records_before_op))\n",
    "    \n",
    "    if current_digit <= digits:\n",
    "        continue\n",
    "    else:\n",
    "        digits = current_digit\n",
    "        df = spark.table(\"demo.nyc.taxis_10M_50\")\n",
    "    \n",
    "    # for vendor_id in df.select(\"vendor_id\").distinct().collect()[:10]:\n",
    "    #     vendor_id = vendor_id[0]\n",
    "    #     df = spark.table(\"demo.nyc.taxis_100_50COLUMNS\")\n",
    "\n",
    "    #     st = time.time()\n",
    "    #     updated_df = df.withColumn(\"fare_amount\", \n",
    "    #                               when(col(\"vendor_id\") == vendor_id, col(\"fare_amount\") + 40)\n",
    "    #                               .otherwise(col(\"fare_amount\")))\n",
    "        \n",
    "    #     updated_df.writeTo(\"demo.nyc.taxis_100_50COLUMNS\").overwritePartitions()\n",
    "        \n",
    "    #     end = time.time() - st\n",
    "    #     rows = updated_df.filter(updated_df[\"vendor_id\"] == vendor_id).count()\n",
    "        \n",
    "    #     details = get_size()\n",
    "    #     details[\"time_taken\"] = f\"{end} sec\"\n",
    "    #     details[\"Operation\"] = f\"Updated {rows} records\"\n",
    "    #     details[\"records_after_op\"] = records_before_op\n",
    "\n",
    "    #     append_to_file(analysis_file, details)\n",
    "        \n",
    "    #     del df, st, end\n",
    "    #     analysis_info.append(details)\n",
    "print(f\"\\nTotal insertion time: {total_insertion_time:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a28e02-7647-4b3f-bb71-14e722054298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6673ef4b-89f2-47c3-a4e6-45bf713faa74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178098b6-9075-4cb2-a753-41bb7ef6e51e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
