{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23bfddc5-7d0d-4680-ab9a-cd26092ec436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/28 05:38:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize the SparkSession (step 1)\n",
    "spark = SparkSession.builder.appName(\"records\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4c83e3f-2309-4f8e-bd30-ac04250cf077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|total_records|\n",
      "+-------------+\n",
      "|        10000|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) AS total_records FROM demo.nyc.taxis_10000_50COLUMNS;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95d72b87-7b40-4a5d-ab02-02310f2877be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/28 06:09:30 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|total_records|\n",
      "+-------------+\n",
      "|        10000|\n",
      "+-------------+\n",
      "\n",
      "Execution time for taxis_10000_50COLUMNS: 0.0183 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Function to measure execution time for a query and return the result along with time taken\n",
    "def run_query(query):\n",
    "    start_time = time.time()  # Start the timer\n",
    "    result = spark.sql(query)  # Run the Spark SQL query\n",
    "    end_time = time.time()  # End the timer\n",
    "    \n",
    "    execution_time = end_time - start_time  # Time taken for the query to execute\n",
    "    return result, execution_time\n",
    "\n",
    "# Initialize total execution time\n",
    "total_execution_time = 0\n",
    "\n",
    "# 1. Repartition and Cache taxis_10000_50COLUMNS DataFrame\n",
    "df_10000 = spark.table(\"demo.nyc.taxis_10000_50COLUMNS\")\n",
    "df_10000_repartitioned = df_10000.repartition(\"extra_col_3\", \"extra_col_1\")  # Repartition based on two columns\n",
    "df_10000_repartitioned.cache()  # Cache the DataFrame\n",
    "\n",
    "# Query on the repartitioned DataFrame\n",
    "query_1 = \"SELECT COUNT(*) AS total_records FROM demo.nyc.taxis_10000_50COLUMNS\"\n",
    "result_1, time_1 = run_query(query_1)\n",
    "total_execution_time += time_1\n",
    "result_1.show()\n",
    "\n",
    "# At the end, print the total execution time for this query\n",
    "print(f\"Execution time for taxis_10000_50COLUMNS: {total_execution_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba23ffe0-001e-4110-98f7-7f649883eb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/28 06:11:35 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|total_records|\n",
      "+-------------+\n",
      "|        10000|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|filtered_records|\n",
      "+----------------+\n",
      "|            9913|\n",
      "+----------------+\n",
      "\n",
      "+-----------+---------+\n",
      "|extra_col_3|total_sum|\n",
      "+-----------+---------+\n",
      "| 2015-05-19|    26442|\n",
      "| 2017-08-11|    19705|\n",
      "| 2022-03-28|    27523|\n",
      "| 2025-02-16|    27962|\n",
      "| 2021-12-18|    13786|\n",
      "| 2015-03-09|     2579|\n",
      "| 2016-03-01|    14604|\n",
      "| 2021-06-22|    12391|\n",
      "| 2018-08-10|    18876|\n",
      "| 2023-07-15|    14366|\n",
      "| 2021-08-27|    11011|\n",
      "| 2023-06-22|     8917|\n",
      "| 2019-06-04|    21620|\n",
      "| 2021-11-13|     6562|\n",
      "| 2021-10-11|     9659|\n",
      "| 2020-08-24|    11171|\n",
      "| 2017-09-11|     8514|\n",
      "| 2018-05-28|     3131|\n",
      "| 2021-01-27|     7688|\n",
      "| 2019-05-08|     6894|\n",
      "+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+\n",
      "|total_records_joined|\n",
      "+--------------------+\n",
      "|               37712|\n",
      "+--------------------+\n",
      "\n",
      "Total execution time for all operations: 0.0714 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Function to measure execution time for a query and return the result along with time taken\n",
    "def run_query(query):\n",
    "    start_time = time.time()  # Start the timer\n",
    "    result = spark.sql(query)  # Run the Spark SQL query\n",
    "    end_time = time.time()  # End the timer\n",
    "    \n",
    "    execution_time = end_time - start_time  # Time taken for the query to execute\n",
    "    return result, execution_time\n",
    "\n",
    "# Initialize total execution time\n",
    "total_execution_time = 0\n",
    "\n",
    "# Load the table into a DataFrame\n",
    "df_10000 = spark.table(\"demo.nyc.taxis_10000_50COLUMNS\")\n",
    "\n",
    "# 1. Repartition the DataFrame by multiple columns (for complex partitioning)\n",
    "# Example partitioning by two columns: \"extra_col_3\" (date) and \"extra_col_1\" (int)\n",
    "df_10000_repartitioned = df_10000.repartition(\"extra_col_3\", \"extra_col_1\")  # Repartition based on two columns\n",
    "df_10000_repartitioned.cache()  # Cache the DataFrame for optimized access\n",
    "\n",
    "# 2. Filter data based on a condition (complex filtering)\n",
    "# Example: Filter rows where \"extra_col_5\" (int) > 100\n",
    "df_10000_filtered = df_10000_repartitioned.filter(\"extra_col_5 > 100\")\n",
    "df_10000_filtered.cache()  # Cache after filtering\n",
    "\n",
    "# 3. Group by and aggregate the data\n",
    "# Example: Group by \"extra_col_3\" and calculate the sum of \"extra_col_5\"\n",
    "df_10000_grouped = df_10000_filtered.groupBy(\"extra_col_3\").agg({\"extra_col_5\": \"sum\"})\n",
    "df_10000_grouped.cache()  # Cache after aggregation\n",
    "\n",
    "# 4. Perform a complex join (Join with the same table for demonstration)\n",
    "# Example: Join on \"extra_col_3\" (date) column\n",
    "df_10000_joined = df_10000_repartitioned.alias(\"df1\").join(df_10000_repartitioned.alias(\"df2\"), \"extra_col_3\")\n",
    "df_10000_joined.cache()  # Cache after join\n",
    "\n",
    "# 5. Run a query on the original DataFrame (before repartitioning)\n",
    "query_1 = \"SELECT COUNT(*) AS total_records FROM demo.nyc.taxis_10000_50COLUMNS\"\n",
    "result_1, time_1 = run_query(query_1)\n",
    "total_execution_time += time_1\n",
    "result_1.show()\n",
    "\n",
    "# 6. Run a query on the repartitioned and filtered DataFrame\n",
    "query_2 = \"SELECT COUNT(*) AS filtered_records FROM demo.nyc.taxis_10000_50COLUMNS WHERE extra_col_5 > 100\"\n",
    "result_2, time_2 = run_query(query_2)\n",
    "total_execution_time += time_2\n",
    "result_2.show()\n",
    "\n",
    "# 7. Run a query on the grouped DataFrame (aggregated data)\n",
    "query_3 = \"SELECT extra_col_3, SUM(extra_col_5) AS total_sum FROM demo.nyc.taxis_10000_50COLUMNS GROUP BY extra_col_3\"\n",
    "result_3, time_3 = run_query(query_3)\n",
    "total_execution_time += time_3\n",
    "result_3.show()\n",
    "\n",
    "# 8. Run a query on the joined DataFrame\n",
    "query_4 = \"SELECT COUNT(*) AS total_records_joined FROM demo.nyc.taxis_10000_50COLUMNS df1 JOIN demo.nyc.taxis_10000_50COLUMNS df2 ON df1.extra_col_3 = df2.extra_col_3\"\n",
    "result_4, time_4 = run_query(query_4)\n",
    "total_execution_time += time_4\n",
    "result_4.show()\n",
    "\n",
    "# At the end, print the total execution time for all queries\n",
    "print(f\"Total execution time for all operations: {total_execution_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81c2da-6761-4da3-b892-a3e91b93e1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
