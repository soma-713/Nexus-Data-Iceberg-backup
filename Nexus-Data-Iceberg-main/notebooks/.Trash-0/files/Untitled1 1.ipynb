{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5efe201-d37c-4593-8fe0-5bbb44b13870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/26 12:58:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"records\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04846a5a-fff5-4486-a41f-580284d53a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|      nyc|           taxis_100|      false|\n",
      "|      nyc|     taxis_100M_time|      false|\n",
      "|      nyc|taxis_1000_50COLUMNS|      false|\n",
      "|      nyc|            taxis_1B|      false|\n",
      "|      nyc|            taxis_1L|      false|\n",
      "|      nyc|          taxis_1000|      false|\n",
      "|      nyc|            taxis_10|      false|\n",
      "|      nyc| taxis_10M_50COLUMNS|      false|\n",
      "|      nyc|   taxis_1L_5COLUMNS|      false|\n",
      "|      nyc|         taxis_10000|      false|\n",
      "|      nyc|            taxis_1M|      false|\n",
      "|      nyc|          taxis_1L_5|      false|\n",
      "|      nyc|          taxis_10_M|      false|\n",
      "|      nyc|taxis_1000_50COLU...|      false|\n",
      "|      nyc|  taxis_10_50COLUMNS|      false|\n",
      "|      nyc|           taxis_10K|      false|\n",
      "|      nyc|taxis_100000_50CO...|      false|\n",
      "|      nyc|            taxis_1K|      false|\n",
      "|      nyc|           taxis_10L|      false|\n",
      "|      nyc|        taxis_5_100M|      false|\n",
      "+---------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Switch to the 'nyc' database\n",
    "spark.sql(\"USE nyc\")\n",
    "\n",
    "# List all tables in the 'nyc' database\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f40be685-e268-4f0e-bb40-574a066829c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records before update: 100\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter percentage of records to update:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 1 records.\n",
      "✅ Successfully updated 1 records!\n",
      "⏳ Total time taken for update: 7.18 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Set the configuration to avoid truncation warning\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)  # Adjust as needed\n",
    "\n",
    "# Define table name\n",
    "table_name = \"demo.nyc.taxis_100_50COLUMNS_Update\"  # Adjust table name accordingly\n",
    "\n",
    "# Get total records before update\n",
    "start_time = time.time()\n",
    "df = spark.table(table_name)\n",
    "total_records_before = df.count()\n",
    "print(f\"Total records before update: {total_records_before}\")\n",
    "\n",
    "# Ask user input for update percentage\n",
    "update_percentage = float(input(\"Enter percentage of records to update: \"))\n",
    "\n",
    "# Calculate records to update\n",
    "num_records_to_update = max(1, int((update_percentage / 100) * total_records_before))\n",
    "print(f\"Updating {num_records_to_update} records.\")\n",
    "\n",
    "# Step 1: Get table schema (cached once)\n",
    "schema_df = spark.sql(f\"DESCRIBE {table_name}\").collect()\n",
    "schema_dict = {row[\"col_name\"]: row[\"data_type\"] for row in schema_df if not row[\"col_name\"].startswith(\"#\")}\n",
    "columns = list(schema_dict.keys())\n",
    "\n",
    "# Step 2: Generate random updates while respecting column types\n",
    "def generate_random_value(data_type):\n",
    "    if \"int\" in data_type.lower():\n",
    "        return random.randint(1, 10000)  # Generate random integer for INT columns\n",
    "    elif \"string\" in data_type.lower():\n",
    "        return f\"Random_{random.randint(100, 999)}\"  # Generate random string for STRING columns\n",
    "    elif \"date\" in data_type.lower():\n",
    "        return (datetime.today() - timedelta(days=random.randint(0, 365))).strftime('%Y-%m-%d')  # Generate random date in YYYY-MM-DD format\n",
    "    else:\n",
    "        return None  # Default case for unsupported data types\n",
    "\n",
    "# Step 3: Generate a list of sampled records to update\n",
    "# Sample random rows ensuring unique IDs\n",
    "sampled_df = df.select(\"extra_col_0\").distinct().orderBy(F.rand()).limit(num_records_to_update)\n",
    "sampled_ids = [row[\"extra_col_0\"] for row in sampled_df.collect()]\n",
    "\n",
    "# Step 4: Update columns using the DataFrame API\n",
    "update_df = df\n",
    "\n",
    "for col_name in columns:\n",
    "    data_type = schema_dict[col_name]\n",
    "    if col_name == \"extra_col_0\":  # Do not update the primary key\n",
    "        continue\n",
    "    random_value = generate_random_value(data_type)\n",
    "\n",
    "    # Apply the update using `withColumn`\n",
    "    if random_value is not None:\n",
    "        update_df = update_df.withColumn(\n",
    "            col_name,\n",
    "            F.when(F.col(\"extra_col_0\").isin(sampled_ids), F.lit(random_value).cast(schema_dict[col_name])).otherwise(F.col(col_name))\n",
    "        )\n",
    "\n",
    "# Step 5: Write back the updated DataFrame\n",
    "update_df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "# Step 6: Print final output\n",
    "end_time = time.time()\n",
    "print(f\"✅ Successfully updated {num_records_to_update} records!\")\n",
    "print(f\"⏳ Total time taken for update: {round(end_time - start_time, 2)} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20f2e6f0-c29e-438b-b4b4-a3e76a166193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records before update: 100\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter percentage of records to update:  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 10 records.\n",
      "✅ Successfully updated 10 records!\n",
      "⏳ Total time taken for update: 6.74 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Set the configuration to avoid truncation warning\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)  # Adjust as needed\n",
    "\n",
    "# Define table name\n",
    "table_name = \"demo.nyc.taxis_100_50COLUMNS_Update\"  # Adjust table name accordingly\n",
    "\n",
    "# Get total records before update\n",
    "start_time = time.time()\n",
    "df = spark.table(table_name)\n",
    "total_records_before = df.count()\n",
    "print(f\"Total records before update: {total_records_before}\")\n",
    "\n",
    "# Ask user input for update percentage\n",
    "update_percentage = float(input(\"Enter percentage of records to update: \"))\n",
    "\n",
    "# Calculate records to update\n",
    "num_records_to_update = max(1, int((update_percentage / 100) * total_records_before))\n",
    "print(f\"Updating {num_records_to_update} records.\")\n",
    "\n",
    "# Step 1: Get table schema (cached once)\n",
    "schema_df = spark.sql(f\"DESCRIBE {table_name}\").collect()\n",
    "schema_dict = {row[\"col_name\"]: row[\"data_type\"] for row in schema_df if not row[\"col_name\"].startswith(\"#\")}\n",
    "columns = list(schema_dict.keys())\n",
    "\n",
    "# Step 2: Generate random updates while respecting column types\n",
    "def generate_random_value(data_type):\n",
    "    if \"int\" in data_type.lower():\n",
    "        return random.randint(1, 10000)  # Generate random integer for INT columns\n",
    "    elif \"string\" in data_type.lower():\n",
    "        return f\"Random_{random.randint(100, 999)}\"  # Generate random string for STRING columns\n",
    "    elif \"date\" in data_type.lower():\n",
    "        return (datetime.today() - timedelta(days=random.randint(0, 365))).strftime('%Y-%m-%d')  # Generate random date in YYYY-MM-DD format\n",
    "    else:\n",
    "        return None  # Default case for unsupported data types\n",
    "\n",
    "# Step 3: Generate sampled IDs (efficiently without using collect)\n",
    "sampled_df = df.select(\"extra_col_0\").distinct().orderBy(F.rand()).limit(num_records_to_update)\n",
    "sampled_ids_broadcast = sampled_df.rdd.map(lambda row: row[\"extra_col_0\"]).collect()\n",
    "sampled_ids_broadcast = set(sampled_ids_broadcast)  # Convert to set for faster lookup\n",
    "\n",
    "# Broadcast the sampled IDs to all workers\n",
    "broadcast_sampled_ids = spark.sparkContext.broadcast(sampled_ids_broadcast)\n",
    "\n",
    "# Step 4: Combine updates into one transformation to avoid multiple `withColumn()` calls\n",
    "# We will apply all updates in a single `select` operation\n",
    "update_exprs = []\n",
    "\n",
    "for col_name in columns:\n",
    "    if col_name == \"extra_col_0\":  # Do not update the primary key\n",
    "        update_exprs.append(F.col(col_name))  # Keep the original value of the primary key\n",
    "        continue\n",
    "    \n",
    "    data_type = schema_dict[col_name]\n",
    "    random_value = generate_random_value(data_type)\n",
    "    \n",
    "    if random_value is not None:\n",
    "        # Create the update expression for the column\n",
    "        update_exprs.append(\n",
    "            F.when(F.col(\"extra_col_0\").isin(broadcast_sampled_ids.value), F.lit(random_value).cast(schema_dict[col_name])).otherwise(F.col(col_name)).alias(col_name)\n",
    "        )\n",
    "    else:\n",
    "        update_exprs.append(F.col(col_name))  # No update needed, retain original column value\n",
    "\n",
    "# Apply the update using select with the generated expressions\n",
    "update_df = df.select(*update_exprs)\n",
    "\n",
    "# Step 5: Write back the updated DataFrame\n",
    "update_df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "# Step 6: Print final output\n",
    "end_time = time.time()\n",
    "print(f\"✅ Successfully updated {num_records_to_update} records!\")\n",
    "print(f\"⏳ Total time taken for update: {round(end_time - start_time, 2)} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9386665-a02d-470b-8a22-9e07523165e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
